{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing Vertex AI pipelines with the KFP v2 SDK\n",
    "\n",
    "![Vertex pipeline](../images/pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import kfp\n",
    "import kfp.v2.dsl as dsl\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, Metrics, ClassificationMetrics)\n",
    "from typing import NamedTuple, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure lab settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'jk-wst1'\n",
    "REGION = 'us-central1'\n",
    "PREFIX = 'jkwst1'\n",
    "\n",
    "STAGING_BUCKET = f'gs://{PREFIX}-bucket'\n",
    "VERTEX_SA = f'training-sa@{PROJECT}.iam.gserviceaccount.com'\n",
    "PIPELINES_SA = f'pipelines-sa@{PROJECT}.iam.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy training data schema to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://schema.pbtxt [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  3.1 KiB/  3.1 KiB]                                                \n",
      "Operation completed over 1 objects/3.1 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "SCHEMA_LOCATION = f'{STAGING_BUCKET}/schema'\n",
    "\n",
    "!gsutil cp schema.pbtxt {SCHEMA_LOCATION}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://jkwst1-bucket/schema/schema.pbtxt\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls {SCHEMA_LOCATION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a training container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_FOLDER = 'trainer'\n",
    "if tf.io.gfile.exists(SCRIPT_FOLDER):\n",
    "    tf.io.gfile.rmtree(SCRIPT_FOLDER)\n",
    "tf.io.gfile.mkdir(SCRIPT_FOLDER)\n",
    "file_path = os.path.join(SCRIPT_FOLDER, 'train.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {file_path}\n",
    "\n",
    "\n",
    "# Copyright 2021 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow_io import bigquery as tfio_bq\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('epochs', 3, 'Nubmer of epochs')\n",
    "flags.DEFINE_integer('units', 32, 'Number units in a hidden layer')\n",
    "flags.DEFINE_integer('per_replica_batch_size', 128, 'Per replica batch size')\n",
    "flags.DEFINE_float('dropout_ratio', 0.5, 'Dropout ratio')\n",
    "flags.DEFINE_string('training_table', None, 'Training table name')\n",
    "flags.DEFINE_string('validation_table', None, 'Validationa table name')\n",
    "flags.DEFINE_string('schema_file', None, 'Location of the data schema file')\n",
    "flags.mark_flag_as_required('training_table')\n",
    "flags.mark_flag_as_required('validation_table')\n",
    "flags.mark_flag_as_required('schema_file')\n",
    "\n",
    "LOCAL_MODEL_DIR = '/tmp/saved_model'\n",
    "LOCAL_TB_DIR = '/tmp/logs'\n",
    "LOCAL_CHECKPOINT_DIR = '/tmp/checkpoints'\n",
    "TARGET_TAG = 'target'\n",
    "\n",
    "\n",
    "def schema_to_features(schema):\n",
    "    \"\"\"Converts a schema_pb2 protobuf to feature dictionary.\"\"\"\n",
    "    \n",
    "    features = {}\n",
    "    for feature in schema.feature:\n",
    "        if feature.type == 2:\n",
    "            if feature.int_domain.is_categorical:\n",
    "                features[feature.name] = ('categorical', tf.int64)\n",
    "            else:\n",
    "                features[feature.name] = ('numeric', tf.int64)\n",
    "        elif feature.type == 1:\n",
    "            features[feature.name] = ('categorical', tf.string)\n",
    "        elif feature.type == 3:\n",
    "            features[feature.name] = ('numeric', tf.double)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def get_target_feature(schema):\n",
    "    \"\"\"Returns the name of the target feature in schema.\"\"\"\n",
    "    \n",
    "    target_feature = None\n",
    "    for feature in schema.feature:\n",
    "        if feature.HasField('annotation'):\n",
    "            if TARGET_TAG in feature.annotation.tag:\n",
    "                target_feature = feature.name\n",
    "    return target_feature\n",
    "\n",
    "\n",
    "def set_job_dirs():\n",
    "    \"\"\"Sets job directories based on env variables set by Vertex AI.\"\"\"\n",
    "    \n",
    "    model_dir = os.getenv('AIP_MODEL_DIR', LOCAL_MODEL_DIR)\n",
    "    tb_dir = os.getenv('AIP_TENSORBOARD_LOG_DIR', LOCAL_TB_DIR)\n",
    "    checkpoint_dir = os.getenv('AIP_CHECKPOINT_DIR', LOCAL_CHECKPOINT_DIR)\n",
    "    \n",
    "    return model_dir, tb_dir, checkpoint_dir\n",
    "\n",
    "\n",
    "def get_bq_dataset(table_name, features, target_feature, batch_size=32):\n",
    "    \"\"\"Creates a tf.data dataset for direct access to BQ table.\"\"\"\n",
    "    \n",
    "    def _transform_row(row_dict):\n",
    "        trimmed_dict = {column:\n",
    "                       (tf.strings.strip(tensor) if tensor.dtype == 'string' else tensor) \n",
    "                       for (column,tensor) in row_dict.items()\n",
    "                       }\n",
    "        target = trimmed_dict.pop(target_feature)\n",
    "        return (trimmed_dict, target)\n",
    "    \n",
    "    selected_fields = {key: {'output_type': value[1]} \n",
    "                       for key, value in features.items()}\n",
    "    project_id, dataset_id, table_id = table_name.split('.')\n",
    "    client = tfio_bq.BigQueryClient()\n",
    "    parent = f'projects/{project_id}'\n",
    "\n",
    "    read_session = client.read_session(\n",
    "        parent=parent,\n",
    "        project_id=project_id,\n",
    "        table_id=table_id,\n",
    "        dataset_id=dataset_id,\n",
    "        selected_fields=selected_fields,\n",
    "    )\n",
    "\n",
    "    dataset = read_session.parallel_read_rows().map(_transform_row).batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_category_encoding_layer(name, dataset, dtype):\n",
    "    \"\"\"Creates a CategoryEncoding layer for a given feature.\"\"\"\n",
    "\n",
    "    if dtype == tf.string:\n",
    "      index = preprocessing.StringLookup()\n",
    "    else:\n",
    "      index = preprocessing.IntegerLookup()\n",
    "\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    index.adapt(feature_ds)\n",
    "    encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
    "\n",
    "    return lambda feature: encoder(index(feature))\n",
    "\n",
    "\n",
    "def get_normalization_layer(name, dataset):\n",
    "    \"\"\"\"Creates a Normalization layer for a given feature.\"\"\"\n",
    "    normalizer = preprocessing.Normalization()\n",
    "\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    return normalizer\n",
    "\n",
    "\n",
    "def create_model(dataset, input_features, units, dropout_ratio):\n",
    "    \"\"\"Creates a binary classifier for Chicago Taxi tip prediction task.\"\"\"\n",
    "    \n",
    "    all_inputs = []\n",
    "    encoded_features = []\n",
    "    for feature_name, feature_info in input_features.items():\n",
    "        col = tf.keras.Input(shape=(1,), name=feature_name, dtype=feature_info[1])\n",
    "        if feature_info[0] == 'categorical':\n",
    "            \n",
    "            encoding_layer = get_category_encoding_layer(feature_name, \n",
    "                                                         dataset,\n",
    "                                                         feature_info[1])\n",
    "        else:\n",
    "            encoding_layer = get_normalization_layer(feature_name,\n",
    "                                                     dataset) \n",
    "        encoded_col = encoding_layer(col)\n",
    "        all_inputs.append(col)\n",
    "        encoded_features.append(encoded_col)\n",
    "        \n",
    "    all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(units, activation=\"relu\")(all_features)\n",
    "    x = tf.keras.layers.Dropout(dropout_ratio)(x)\n",
    "    output = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.Model(all_inputs, output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    del argv\n",
    "    \n",
    "    # Set distribution strategy\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "    global_batch_size = (strategy.num_replicas_in_sync *\n",
    "                         FLAGS.per_replica_batch_size)\n",
    "    \n",
    "    # Extract features from schema_pb2\n",
    "    schema = tfdv.load_schema_text(FLAGS.schema_file)\n",
    "    features = schema_to_features(schema)\n",
    "    target_feature = get_target_feature(schema)\n",
    "\n",
    "    if not target_feature:\n",
    "        raise RuntimeError('Schema does not have a target feature')\n",
    "    \n",
    "    # Prepare datasets\n",
    "    validation_ds = get_bq_dataset(FLAGS.validation_table, \n",
    "                                   features,\n",
    "                                   target_feature,\n",
    "                                   batch_size=global_batch_size)\n",
    "    training_ds = get_bq_dataset(FLAGS.training_table,\n",
    "                                 features,\n",
    "                                 target_feature,\n",
    "                                 batch_size=global_batch_size)\n",
    "    \n",
    "    # Prepare the model\n",
    "    logging.info('Creating the model ...')\n",
    "    input_features = {key: value for key, value in features.items() if key != target_feature}\n",
    "    \n",
    "    with strategy.scope():\n",
    "        model = create_model(training_ds, input_features, FLAGS.units, FLAGS.dropout_ratio)\n",
    "        model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Configure Keras callbacks\n",
    "    model_dir, tb_dir, checkpoint_dir = set_job_dirs()\n",
    "    callbacks = [tf.keras.callbacks.experimental.BackupAndRestore(backup_dir=checkpoint_dir)]\n",
    "    callbacks.append(tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=tb_dir, update_freq='batch'))\n",
    "    \n",
    "    logging.info('Starting training ...')\n",
    "    model.fit(training_ds, \n",
    "              epochs=FLAGS.epochs, \n",
    "              validation_data=validation_ds,\n",
    "              callbacks=callbacks)\n",
    "    \n",
    "    # Save trained model\n",
    "    logging.info('Training completed. Saving the trained model to: {}'.format(model_dir))\n",
    "    model.save(model_dir)  \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "    app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = 'gcr.io/deeplearning-platform-release/tf2-cpu.2-4'\n",
    "TRAIN_IMAGE = f'gcr.io/{PROJECT}/taxi_classifier_trainer_v2'\n",
    "\n",
    "dockerfile = f'''\n",
    "FROM {BASE_IMAGE}\n",
    "\n",
    "WORKDIR /trainer\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n",
    "'''\n",
    "\n",
    "with open(os.path.join(SCRIPT_FOLDER, 'Dockerfile'), 'w') as f:\n",
    "    f.write(dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 8.0 KiB before compression.\n",
      "Uploading tarball of [trainer] to [gs://jk-wst1_cloudbuild/source/1624911236.228198-f3d3e081fb5b46629b000eb35a8f2be2.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/jk-wst1/locations/global/builds/16647e18-fafb-4b76-b46d-8d730eea469b].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/16647e18-fafb-4b76-b46d-8d730eea469b?project=630263135640].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"16647e18-fafb-4b76-b46d-8d730eea469b\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://jk-wst1_cloudbuild/source/1624911236.228198-f3d3e081fb5b46629b000eb35a8f2be2.tgz#1624911236558629\n",
      "Copying gs://jk-wst1_cloudbuild/source/1624911236.228198-f3d3e081fb5b46629b000eb35a8f2be2.tgz#1624911236558629...\n",
      "/ [1 files][  2.9 KiB/  2.9 KiB]                                                \n",
      "Operation completed over 1 objects/2.9 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  10.75kB\n",
      "Step 1/4 : FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-4\n",
      "latest: Pulling from deeplearning-platform-release/tf2-cpu.2-4\n",
      "01bf7da0a88c: Pulling fs layer\n",
      "f3b4a5f15c7a: Pulling fs layer\n",
      "57ffbe87baa1: Pulling fs layer\n",
      "424e7c9d5d89: Pulling fs layer\n",
      "9b397537aef0: Pulling fs layer\n",
      "2bd5028f4b85: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "b2ed56b85d3a: Pulling fs layer\n",
      "8bfb788e9874: Pulling fs layer\n",
      "0618fb353339: Pulling fs layer\n",
      "42045a665612: Pulling fs layer\n",
      "031d8d7b75f7: Pulling fs layer\n",
      "5780cc9addac: Pulling fs layer\n",
      "8fbe78107b3d: Pulling fs layer\n",
      "eee173fc570a: Pulling fs layer\n",
      "9334ecc802d5: Pulling fs layer\n",
      "424e7c9d5d89: Waiting\n",
      "9b397537aef0: Waiting\n",
      "2bd5028f4b85: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "b2ed56b85d3a: Waiting\n",
      "8bfb788e9874: Waiting\n",
      "0618fb353339: Waiting\n",
      "42045a665612: Waiting\n",
      "031d8d7b75f7: Waiting\n",
      "5780cc9addac: Waiting\n",
      "8fbe78107b3d: Waiting\n",
      "eee173fc570a: Waiting\n",
      "c631c38965fd: Pulling fs layer\n",
      "803ecb627365: Pulling fs layer\n",
      "4466b5c2ac37: Pulling fs layer\n",
      "9334ecc802d5: Waiting\n",
      "c631c38965fd: Waiting\n",
      "803ecb627365: Waiting\n",
      "16a6777d4439: Pulling fs layer\n",
      "7a84944cecd7: Pulling fs layer\n",
      "4466b5c2ac37: Waiting\n",
      "16a6777d4439: Waiting\n",
      "7a84944cecd7: Waiting\n",
      "57ffbe87baa1: Verifying Checksum\n",
      "57ffbe87baa1: Download complete\n",
      "f3b4a5f15c7a: Verifying Checksum\n",
      "f3b4a5f15c7a: Download complete\n",
      "424e7c9d5d89: Verifying Checksum\n",
      "424e7c9d5d89: Download complete\n",
      "01bf7da0a88c: Verifying Checksum\n",
      "01bf7da0a88c: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "b2ed56b85d3a: Verifying Checksum\n",
      "b2ed56b85d3a: Download complete\n",
      "2bd5028f4b85: Verifying Checksum\n",
      "2bd5028f4b85: Download complete\n",
      "0618fb353339: Verifying Checksum\n",
      "0618fb353339: Download complete\n",
      "42045a665612: Verifying Checksum\n",
      "42045a665612: Download complete\n",
      "031d8d7b75f7: Verifying Checksum\n",
      "031d8d7b75f7: Download complete\n",
      "5780cc9addac: Verifying Checksum\n",
      "5780cc9addac: Download complete\n",
      "8bfb788e9874: Verifying Checksum\n",
      "8bfb788e9874: Download complete\n",
      "8fbe78107b3d: Verifying Checksum\n",
      "8fbe78107b3d: Download complete\n",
      "eee173fc570a: Verifying Checksum\n",
      "eee173fc570a: Download complete\n",
      "9334ecc802d5: Verifying Checksum\n",
      "9334ecc802d5: Download complete\n",
      "c631c38965fd: Verifying Checksum\n",
      "c631c38965fd: Download complete\n",
      "4466b5c2ac37: Verifying Checksum\n",
      "4466b5c2ac37: Download complete\n",
      "16a6777d4439: Verifying Checksum\n",
      "16a6777d4439: Download complete\n",
      "9b397537aef0: Verifying Checksum\n",
      "9b397537aef0: Download complete\n",
      "7a84944cecd7: Verifying Checksum\n",
      "7a84944cecd7: Download complete\n",
      "01bf7da0a88c: Pull complete\n",
      "f3b4a5f15c7a: Pull complete\n",
      "57ffbe87baa1: Pull complete\n",
      "424e7c9d5d89: Pull complete\n",
      "803ecb627365: Verifying Checksum\n",
      "803ecb627365: Download complete\n",
      "9b397537aef0: Pull complete\n",
      "2bd5028f4b85: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "b2ed56b85d3a: Pull complete\n",
      "8bfb788e9874: Pull complete\n",
      "0618fb353339: Pull complete\n",
      "42045a665612: Pull complete\n",
      "031d8d7b75f7: Pull complete\n",
      "5780cc9addac: Pull complete\n",
      "8fbe78107b3d: Pull complete\n",
      "eee173fc570a: Pull complete\n",
      "9334ecc802d5: Pull complete\n",
      "c631c38965fd: Pull complete\n",
      "803ecb627365: Pull complete\n",
      "4466b5c2ac37: Pull complete\n",
      "16a6777d4439: Pull complete\n",
      "7a84944cecd7: Pull complete\n",
      "Digest: sha256:56489bdb00b3f5c342d8cbcdeea68fd63b982858199c4ed7edac15614954b110\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/tf2-cpu.2-4:latest\n",
      " ---> fbf8f454dc05\n",
      "Step 2/4 : WORKDIR /trainer\n",
      " ---> Running in 16be0e27b480\n",
      "Removing intermediate container 16be0e27b480\n",
      " ---> e0680f08b68e\n",
      "Step 3/4 : COPY train.py .\n",
      " ---> aa391c933e39\n",
      "Step 4/4 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 10787f786c28\n",
      "Removing intermediate container 10787f786c28\n",
      " ---> ba0a08f12e14\n",
      "Successfully built ba0a08f12e14\n",
      "Successfully tagged gcr.io/jk-wst1/taxi_classifier_trainer_v2:latest\n",
      "PUSH\n",
      "Pushing gcr.io/jk-wst1/taxi_classifier_trainer_v2\n",
      "The push refers to repository [gcr.io/jk-wst1/taxi_classifier_trainer_v2]\n",
      "67d0f895230b: Preparing\n",
      "4771aa7847de: Preparing\n",
      "d39fae2e5e4e: Preparing\n",
      "33302bd81efa: Preparing\n",
      "0c5f899e9bc7: Preparing\n",
      "5e593cdd4a12: Preparing\n",
      "06a5bf49b163: Preparing\n",
      "b34dae69fc5d: Preparing\n",
      "0ffb7465dde9: Preparing\n",
      "e2563d1ada9a: Preparing\n",
      "42b027d1e826: Preparing\n",
      "636a7c2e7d03: Preparing\n",
      "1ba1158adf89: Preparing\n",
      "96e46d1341e8: Preparing\n",
      "954f6dc3f7f5: Preparing\n",
      "8760a171b659: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "a0710233fd2d: Preparing\n",
      "05449afa4be9: Preparing\n",
      "5b9e34b5cf74: Preparing\n",
      "8cafc6d2db45: Preparing\n",
      "a5d4bacb0351: Preparing\n",
      "5153e1acaabc: Preparing\n",
      "1ba1158adf89: Waiting\n",
      "96e46d1341e8: Waiting\n",
      "954f6dc3f7f5: Waiting\n",
      "8760a171b659: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "a0710233fd2d: Waiting\n",
      "05449afa4be9: Waiting\n",
      "5b9e34b5cf74: Waiting\n",
      "8cafc6d2db45: Waiting\n",
      "a5d4bacb0351: Waiting\n",
      "5153e1acaabc: Waiting\n",
      "06a5bf49b163: Waiting\n",
      "b34dae69fc5d: Waiting\n",
      "0ffb7465dde9: Waiting\n",
      "e2563d1ada9a: Waiting\n",
      "42b027d1e826: Waiting\n",
      "636a7c2e7d03: Waiting\n",
      "5e593cdd4a12: Waiting\n",
      "33302bd81efa: Layer already exists\n",
      "d39fae2e5e4e: Layer already exists\n",
      "0c5f899e9bc7: Layer already exists\n",
      "5e593cdd4a12: Layer already exists\n",
      "06a5bf49b163: Layer already exists\n",
      "b34dae69fc5d: Layer already exists\n",
      "e2563d1ada9a: Layer already exists\n",
      "0ffb7465dde9: Layer already exists\n",
      "42b027d1e826: Layer already exists\n",
      "636a7c2e7d03: Layer already exists\n",
      "1ba1158adf89: Layer already exists\n",
      "96e46d1341e8: Layer already exists\n",
      "954f6dc3f7f5: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "8760a171b659: Layer already exists\n",
      "a0710233fd2d: Layer already exists\n",
      "5b9e34b5cf74: Layer already exists\n",
      "05449afa4be9: Layer already exists\n",
      "8cafc6d2db45: Layer already exists\n",
      "a5d4bacb0351: Layer already exists\n",
      "5153e1acaabc: Layer already exists\n",
      "4771aa7847de: Pushed\n",
      "67d0f895230b: Pushed\n",
      "latest: digest: sha256:7506434c148de4096f052a823f3e8a7edd0d7ba163e97b91f485f3075c2b9863 size: 5126\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                 IMAGES                                               STATUS\n",
      "16647e18-fafb-4b76-b46d-8d730eea469b  2021-06-28T20:13:56+00:00  2M37S     gs://jk-wst1_cloudbuild/source/1624911236.228198-f3d3e081fb5b46629b000eb35a8f2be2.tgz  gcr.io/jk-wst1/taxi_classifier_trainer_v2 (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag {TRAIN_IMAGE} {SCRIPT_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom KFP components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='gcr.io/ml-pipeline/google-cloud-pipeline-components:0.1.1')\n",
    "def ingest_data_op(\n",
    "    project: str,\n",
    "    source_table_name: str,\n",
    "    bq_location: str,\n",
    "    dataset_name: str,\n",
    "    train_split_name: str,\n",
    "    valid_split_name: str,\n",
    "    test_split_name: str,\n",
    "    dataset: Output[Dataset]\n",
    "):\n",
    "    \"\"\"Prepares training, validation, and testing data splits\n",
    "    from Chicago taxi public dataset.\"\"\"\n",
    "    \n",
    "    import logging\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import exceptions\n",
    "    \n",
    "    METADATA_TRAIN_SPLIT_KEY = 'train_split'\n",
    "    METADATA_VALID_SPLIT_KEY = 'valid_split'\n",
    "    METADATA_TEST_SPLIT_KEY = 'test_split'\n",
    "    \n",
    "    sql_script_template = '''\n",
    "    CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@TRAIN_SPLIT`\n",
    "    AS\n",
    "    SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "    FROM `@SOURCE_TABLE`\n",
    "    WHERE data_split='TRAIN';\n",
    "\n",
    "    CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@VALIDATE_SPLIT`\n",
    "    AS\n",
    "    SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "    FROM `@SOURCE_TABLE`\n",
    "    WHERE data_split='VALIDATE';\n",
    "\n",
    "    CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@TEST_SPLIT`\n",
    "    AS\n",
    "    SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "    FROM `@SOURCE_TABLE`\n",
    "    WHERE data_split='TEST';\n",
    "    '''\n",
    "    \n",
    "    client = bigquery.Client(project=project)\n",
    "    ds = bigquery.Dataset(f'{project}.{dataset_name}')\n",
    "    ds.location = bq_location\n",
    "    try:\n",
    "        ds = client.create_dataset(ds, timeout=30)\n",
    "        logging.info(f'Created dataset: {project}.{dataset_name}')\n",
    "    except exceptions.Conflict:\n",
    "        logging.info(f'Dataset {project}.{dataset_name} already exists')\n",
    "        \n",
    "    sql_script = sql_script_template.replace(\n",
    "        '@PROJECT', project).replace(\n",
    "        '@DATASET', dataset_name).replace(\n",
    "        '@TRAIN_SPLIT', train_split_name).replace(\n",
    "        '@VALIDATE_SPLIT', valid_split_name).replace(\n",
    "        '@TEST_SPLIT', test_split_name).replace(\n",
    "        '@SOURCE_TABLE', source_table_name)\n",
    "\n",
    "    job = client.query(sql_script)\n",
    "    job.result()\n",
    "    \n",
    "    dataset.metadata[METADATA_TRAIN_SPLIT_KEY] = f'{project}.{dataset_name}.{train_split_name}'\n",
    "    dataset.metadata[METADATA_VALID_SPLIT_KEY] = f'{project}.{dataset_name}.{valid_split_name}'\n",
    "    dataset.metadata[METADATA_TEST_SPLIT_KEY] = f'{project}.{dataset_name}.{test_split_name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='tensorflow/tfx:latest',\n",
    "               packages_to_install=['google-cloud-bigquery[bqstorage,pandas]'])\n",
    "def generate_stats_op(\n",
    "    project: str,\n",
    "    sample_percentage: int,\n",
    "    dataset: Input[Dataset],\n",
    "    stats: Output[Artifact],\n",
    "   \n",
    "):\n",
    "    \"\"\"Generates statistics for the data splits \n",
    "    referenced in the input Dataset artifact.\"\"\"\n",
    "    \n",
    "    import os\n",
    "    import tensorflow_data_validation as tfdv\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    METADATA_TRAIN_SPLIT_KEY = 'train_split'\n",
    "    METADATA_VALID_SPLIT_KEY = 'valid_split'\n",
    "    METADATA_TEST_SPLIT_KEY = 'test_split'\n",
    "    \n",
    "    STATS_FILE_NAME = 'stats.pbtxt'\n",
    "    \n",
    "    sql_script_template = '''\n",
    "    SELECT * \n",
    "    FROM @TABLE\n",
    "    TABLESAMPLE SYSTEM (@SAMPLE_PERC PERCENT)\n",
    "    '''\n",
    "    \n",
    "    client = bigquery.Client(project=project)\n",
    "    for key in [METADATA_TRAIN_SPLIT_KEY, METADATA_VALID_SPLIT_KEY, METADATA_TEST_SPLIT_KEY]:\n",
    "        if key in dataset.metadata.keys():\n",
    "            sql_script = sql_script_template.replace(\n",
    "                '@TABLE', dataset.metadata[key]).replace(\n",
    "                '@SAMPLE_PERC', str(sample_percentage))\n",
    "            \n",
    "            df = client.query(sql_script).result().to_dataframe()\n",
    "    \n",
    "            stats_proto = tfdv.generate_statistics_from_dataframe(\n",
    "                dataframe=df,\n",
    "                stats_options=tfdv.StatsOptions(\n",
    "                    num_top_values=50\n",
    "                )\n",
    "            )\n",
    "    \n",
    "            file_path = os.path.join(stats.path, key)\n",
    "            os.makedirs(file_path)\n",
    "            tfdv.write_stats_text(stats_proto, \n",
    "                                  os.path.join(file_path, STATS_FILE_NAME))\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='tensorflow/tfx:latest')\n",
    "def validate_stats_op(\n",
    "    project: str,\n",
    "    stats: Input[Artifact],\n",
    "    schema: Input[Artifact],\n",
    "    anomalies: Output[Artifact],  \n",
    ")-> NamedTuple(\n",
    "    'ValidOutputs',\n",
    "    [\n",
    "        ('anomalies_detected', str)\n",
    "    ]):\n",
    "    \"\"\"Validates statistices referenced in the input stats Artifact.\"\"\"\n",
    "    \n",
    "    STATS_FILE_NAME = 'stats.pbtxt'\n",
    "    ANOMALIES_FILE_NAME = 'anomalies.pbtxt'\n",
    "    TRUE = 'true'\n",
    "    FALSE = 'false'\n",
    "    \n",
    "    import os\n",
    "    import logging\n",
    "    import tensorflow_data_validation as tfdv\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    schema_proto = tfdv.load_schema_text(\n",
    "        input_path=schema.path\n",
    "    ) \n",
    "    \n",
    "    anomalies_detected = FALSE\n",
    "    for folder in os.listdir(stats.path):\n",
    "        stats_proto = tfdv.load_stats_text(\n",
    "            input_path=os.path.join(stats.path, folder, STATS_FILE_NAME)\n",
    "        )\n",
    "        \n",
    "        anomalies_proto = tfdv.validate_statistics(\n",
    "            statistics=stats_proto, \n",
    "            schema=schema_proto\n",
    "        )\n",
    "        \n",
    "        file_path = os.path.join(anomalies.path, folder)\n",
    "        os.makedirs(file_path)\n",
    "        file_path = os.path.join(file_path, ANOMALIES_FILE_NAME)\n",
    "        tfdv.write_anomalies_text(anomalies_proto, file_path)\n",
    "                                 \n",
    "        if anomalies_proto.anomaly_info:\n",
    "            anomalies_detected = TRUE\n",
    "            logging.info('Anomamlies detected: {}'.format(file_path))\n",
    "    \n",
    "    output = namedtuple('ValidOutputs', ['anomalies_detected'])\n",
    "    \n",
    "    return output(anomalies_detected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile time settings\n",
    "PIPELINE_NAME = f'{PREFIX}-continuous-training-pipeline'\n",
    "\n",
    "BQ_DATASET_NAME = f'{PREFIX}_dataset_pipeline'\n",
    "BQ_LOCATION = REGION\n",
    "TRAINING_TABLE_NAME = 'training'\n",
    "VALIDATION_TABLE_NAME = 'validation'\n",
    "TESTING_TABLE_NAME = 'testing'\n",
    "SCHEMA = f'{SCHEMA_LOCATION}/schema.pbtxt'\n",
    "\n",
    "TRAINING_CONTAINER_IMAGE = TRAIN_IMAGE\n",
    "TRAINING_MACHINE_TYPE = 'n1-standard-4'\n",
    "REPLICA_COUNT = 1\n",
    "\n",
    "SERVING_CONTAINER_IMAGE = 'us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-4:latest'\n",
    "SERVING_MACHINE_TYPE = 'n1-standard-4'\n",
    "\n",
    "\n",
    "@dsl.pipeline(name=PIPELINE_NAME)\n",
    "def taxi_tip_predictor_training(\n",
    "    source_table_name: str,\n",
    "    model_display_name: str,\n",
    "    epochs: int,\n",
    "    staging_location: str,\n",
    "    per_replica_batch_size: int,\n",
    "    sample_percentage: int = 100,\n",
    "    year: int = 2020,\n",
    "    sample_size: int = 1000000,\n",
    "):\n",
    "    \n",
    "    import_schema = kfp.dsl.importer(\n",
    "        artifact_uri=SCHEMA,\n",
    "        artifact_class=Artifact,\n",
    "        reimport=False,\n",
    "    )\n",
    "    \n",
    "    prepare_data = ingest_data_op(\n",
    "        project=PROJECT,\n",
    "        source_table_name=source_table_name,\n",
    "        bq_location=BQ_LOCATION,\n",
    "        dataset_name=BQ_DATASET_NAME,\n",
    "        train_split_name=TRAINING_TABLE_NAME,\n",
    "        valid_split_name=VALIDATION_TABLE_NAME,\n",
    "        test_split_name=TESTING_TABLE_NAME,\n",
    "    )\n",
    "    \n",
    "    generate_stats = generate_stats_op(\n",
    "        project=PROJECT,\n",
    "        sample_percentage=sample_percentage,\n",
    "        dataset=prepare_data.outputs['dataset'],\n",
    "    )\n",
    "    \n",
    "    validate_stats = validate_stats_op(\n",
    "        project=PROJECT,\n",
    "        schema=import_schema.output,\n",
    "        stats=generate_stats.outputs['stats'],\n",
    "    )\n",
    "    \n",
    "    with dsl.Condition(validate_stats.outputs['anomalies_detected'] == 'false',\n",
    "                       name = 'Anomalies detected'):\n",
    "    \n",
    "        args = [\n",
    "            '--epochs', str(epochs),\n",
    "            '--per_replica_batch_size', str(per_replica_batch_size),\n",
    "            '--training_table', f'{PROJECT}.{BQ_DATASET_NAME}.{TRAINING_TABLE_NAME}',\n",
    "            '--validation_table',  f'{PROJECT}.{BQ_DATASET_NAME}.{VALIDATION_TABLE_NAME}',\n",
    "            '--schema_file', SCHEMA,\n",
    "        ]\n",
    "        \n",
    "        train = gcc_aip.CustomContainerTrainingJobRunOp(\n",
    "            project=PROJECT,\n",
    "            location=REGION,\n",
    "            display_name=model_display_name,\n",
    "            model_display_name=model_display_name,\n",
    "            container_uri=TRAINING_CONTAINER_IMAGE,\n",
    "            args=args,\n",
    "            replica_count=REPLICA_COUNT,\n",
    "            staging_bucket=staging_location,\n",
    "            model_serving_container_image_uri=SERVING_CONTAINER_IMAGE,\n",
    "        )\n",
    "    \n",
    "        create_endpoint = gcc_aip.EndpointCreateOp(\n",
    "            project=PROJECT,\n",
    "            display_name=model_display_name\n",
    "        )\n",
    "        create_endpoint.after(train)\n",
    "    \n",
    "        deploy_model = gcc_aip.ModelDeployOp(\n",
    "            project=PROJECT,\n",
    "            endpoint=create_endpoint.outputs['endpoint'],\n",
    "            model=train.outputs['model'],\n",
    "            deployed_model_display_name=model_display_name,\n",
    "            machine_type=SERVING_MACHINE_TYPE\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "package_path = 'taxi_tip_predictor_pipeline.json'\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=taxi_tip_predictor_training,\n",
    "    package_path=package_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT,\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/jkwst1-continuous-training-pipeline-20210628205809?project=jk-wst1\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameter_values = {\n",
    "    'model_display_name': f'{PREFIX} Taxi tip predictor',\n",
    "    'source_table_name': f'{PROJECT}.{PREFIX}_dataset.features',\n",
    "    'epochs': 2,\n",
    "    'per_replica_batch_size': 128,\n",
    "    'staging_location': f'{STAGING_BUCKET}/vertex_staging',\n",
    "}\n",
    "\n",
    "response = api_client.create_run_from_job_spec(\n",
    "    package_path,\n",
    "    pipeline_root=f'{STAGING_BUCKET}/pipeline_runs',\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=False,\n",
    "    service_account=PIPELINES_SA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract pipeline run metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline_name</th>\n",
       "      <th>run_name</th>\n",
       "      <th>param.input:per_replica_batch_size</th>\n",
       "      <th>param.input:year</th>\n",
       "      <th>param.input:model_display_name</th>\n",
       "      <th>param.input:sample_percentage</th>\n",
       "      <th>param.input:staging_location</th>\n",
       "      <th>param.input:epochs</th>\n",
       "      <th>param.input:sample_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jk-continuous-training-pipeline</td>\n",
       "      <td>jk-continuous-training-pipeline-20210616005908</td>\n",
       "      <td>128</td>\n",
       "      <td>2020</td>\n",
       "      <td>jk Taxi tip predictor</td>\n",
       "      <td>100</td>\n",
       "      <td>gs://jk-bucket/vertex_staging</td>\n",
       "      <td>2</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jk-continuous-training-pipeline</td>\n",
       "      <td>jk-continuous-training-pipeline-20210615220234</td>\n",
       "      <td>128</td>\n",
       "      <td>2020</td>\n",
       "      <td>jk Taxi tip predictor</td>\n",
       "      <td>100</td>\n",
       "      <td>gs://jk-bucket/vertex_staging</td>\n",
       "      <td>2</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pipeline_name  \\\n",
       "0  jk-continuous-training-pipeline   \n",
       "1  jk-continuous-training-pipeline   \n",
       "\n",
       "                                         run_name  \\\n",
       "0  jk-continuous-training-pipeline-20210616005908   \n",
       "1  jk-continuous-training-pipeline-20210615220234   \n",
       "\n",
       "  param.input:per_replica_batch_size param.input:year  \\\n",
       "0                                128             2020   \n",
       "1                                128             2020   \n",
       "\n",
       "  param.input:model_display_name param.input:sample_percentage  \\\n",
       "0          jk Taxi tip predictor                           100   \n",
       "1          jk Taxi tip predictor                           100   \n",
       "\n",
       "    param.input:staging_location param.input:epochs param.input:sample_size  \n",
       "0  gs://jk-bucket/vertex_staging                  2                 1000000  \n",
       "1  gs://jk-bucket/vertex_staging                  2                 1000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_name = PIPELINE_NAME\n",
    "\n",
    "pipeline_df = aiplatform.get_pipeline_df(pipeline=pipeline_name)\n",
    "pipeline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
