{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e16d91ef",
   "metadata": {},
   "source": [
    "# Training and deploying a tabular model using Vertex custom training job\n",
    "\n",
    "![Training pipeline](../images/custom-tabular.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "615f050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform_v1beta1 import types\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import exceptions\n",
    "\n",
    "from google.cloud.aiplatform.utils import JobClientWithOverride\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from tensorflow_io import bigquery as tfio_bq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e59aa3f",
   "metadata": {},
   "source": [
    "## Configure GCP settings\n",
    "\n",
    "*Before running the notebook make sure to follow the repo's README file to install the pre-requisites.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "684fa974",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'jk-vertex-workshop'\n",
    "REGION = 'us-central1'\n",
    "PREFIX = 'jkvw'\n",
    "\n",
    "STAGING_BUCKET = f'gs://{PREFIX}-bucket'\n",
    "VERTEX_SA = f'{PREFIX}-training-sa@{PROJECT}.iam.gserviceaccount.com'\n",
    "\n",
    "TENSORBOARD = 'projects/910094146258/locations/us-central1/tensorboards/6406405654306619392'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cfcafb",
   "metadata": {},
   "source": [
    "## Preparing training data in BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250468d6",
   "metadata": {},
   "source": [
    "### Explore Chicago Taxi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d44ae474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 1/1 [00:00<00:00, 1108.14query/s]\n",
      "Downloading: 100%|██████████| 3/3 [00:01<00:00,  2.53rows/s]\n"
     ]
    }
   ],
   "source": [
    "%%bigquery data\n",
    "\n",
    "SELECT \n",
    "    *\n",
    "FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "LIMIT 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2534f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>unique_key</th>\n",
       "      <td>97b1a364c3544d2ca99fe6a43ea433986a972bf4</td>\n",
       "      <td>00d8fcc15a4d99f0efc03595919e0eb348847abf</td>\n",
       "      <td>2cf6a01fbd4d79b3f45554099dd2ccaf324c4baa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>taxi_id</th>\n",
       "      <td>01271057890af916c8ee4890c076e44fb6ac327a1868de...</td>\n",
       "      <td>ec73aeaa166a9fabcd546bd176e75ee589e58653db8b0c...</td>\n",
       "      <td>31af69ed89cd6d3c48818cb0ba3541db71149cc4fd3c54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_start_timestamp</th>\n",
       "      <td>2013-09-28 21:30:00+00:00</td>\n",
       "      <td>2013-09-25 14:45:00+00:00</td>\n",
       "      <td>2013-09-25 12:45:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_end_timestamp</th>\n",
       "      <td>2013-09-28 21:45:00+00:00</td>\n",
       "      <td>2013-09-25 15:00:00+00:00</td>\n",
       "      <td>2013-09-25 13:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_seconds</th>\n",
       "      <td>360</td>\n",
       "      <td>1440</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_miles</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_census_tract</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropoff_census_tract</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_community_area</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropoff_community_area</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fare</th>\n",
       "      <td>7.45</td>\n",
       "      <td>31.25</td>\n",
       "      <td>5.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tips</th>\n",
       "      <td>1.49</td>\n",
       "      <td>7.81</td>\n",
       "      <td>1.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tolls</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extras</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_total</th>\n",
       "      <td>8.94</td>\n",
       "      <td>39.06</td>\n",
       "      <td>7.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>payment_type</th>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Credit Card</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <td>Chicago Elite Cab Corp. (Chicago Carriag</td>\n",
       "      <td>Chicago Elite Cab Corp. (Chicago Carriag</td>\n",
       "      <td>Chicago Elite Cab Corp. (Chicago Carriag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_latitude</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_longitude</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_location</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropoff_location</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        0  \\\n",
       "unique_key                       97b1a364c3544d2ca99fe6a43ea433986a972bf4   \n",
       "taxi_id                 01271057890af916c8ee4890c076e44fb6ac327a1868de...   \n",
       "trip_start_timestamp                            2013-09-28 21:30:00+00:00   \n",
       "trip_end_timestamp                              2013-09-28 21:45:00+00:00   \n",
       "trip_seconds                                                          360   \n",
       "trip_miles                                                            0.0   \n",
       "pickup_census_tract                                                   NaN   \n",
       "dropoff_census_tract                                                  NaN   \n",
       "pickup_community_area                                                 NaN   \n",
       "dropoff_community_area                                                NaN   \n",
       "fare                                                                 7.45   \n",
       "tips                                                                 1.49   \n",
       "tolls                                                                 0.0   \n",
       "extras                                                                0.0   \n",
       "trip_total                                                           8.94   \n",
       "payment_type                                                  Credit Card   \n",
       "company                          Chicago Elite Cab Corp. (Chicago Carriag   \n",
       "pickup_latitude                                                       NaN   \n",
       "pickup_longitude                                                      NaN   \n",
       "pickup_location                                                      None   \n",
       "dropoff_latitude                                                      NaN   \n",
       "dropoff_longitude                                                     NaN   \n",
       "dropoff_location                                                     None   \n",
       "\n",
       "                                                                        1  \\\n",
       "unique_key                       00d8fcc15a4d99f0efc03595919e0eb348847abf   \n",
       "taxi_id                 ec73aeaa166a9fabcd546bd176e75ee589e58653db8b0c...   \n",
       "trip_start_timestamp                            2013-09-25 14:45:00+00:00   \n",
       "trip_end_timestamp                              2013-09-25 15:00:00+00:00   \n",
       "trip_seconds                                                         1440   \n",
       "trip_miles                                                            0.0   \n",
       "pickup_census_tract                                                   NaN   \n",
       "dropoff_census_tract                                                  NaN   \n",
       "pickup_community_area                                                 NaN   \n",
       "dropoff_community_area                                                NaN   \n",
       "fare                                                                31.25   \n",
       "tips                                                                 7.81   \n",
       "tolls                                                                 0.0   \n",
       "extras                                                                0.0   \n",
       "trip_total                                                          39.06   \n",
       "payment_type                                                  Credit Card   \n",
       "company                          Chicago Elite Cab Corp. (Chicago Carriag   \n",
       "pickup_latitude                                                       NaN   \n",
       "pickup_longitude                                                      NaN   \n",
       "pickup_location                                                      None   \n",
       "dropoff_latitude                                                      NaN   \n",
       "dropoff_longitude                                                     NaN   \n",
       "dropoff_location                                                     None   \n",
       "\n",
       "                                                                        2  \n",
       "unique_key                       2cf6a01fbd4d79b3f45554099dd2ccaf324c4baa  \n",
       "taxi_id                 31af69ed89cd6d3c48818cb0ba3541db71149cc4fd3c54...  \n",
       "trip_start_timestamp                            2013-09-25 12:45:00+00:00  \n",
       "trip_end_timestamp                              2013-09-25 13:00:00+00:00  \n",
       "trip_seconds                                                          300  \n",
       "trip_miles                                                            0.0  \n",
       "pickup_census_tract                                                   NaN  \n",
       "dropoff_census_tract                                                  NaN  \n",
       "pickup_community_area                                                 NaN  \n",
       "dropoff_community_area                                                NaN  \n",
       "fare                                                                 5.65  \n",
       "tips                                                                 1.41  \n",
       "tolls                                                                 0.0  \n",
       "extras                                                                0.0  \n",
       "trip_total                                                           7.06  \n",
       "payment_type                                                  Credit Card  \n",
       "company                          Chicago Elite Cab Corp. (Chicago Carriag  \n",
       "pickup_latitude                                                       NaN  \n",
       "pickup_longitude                                                      NaN  \n",
       "pickup_location                                                      None  \n",
       "dropoff_latitude                                                      NaN  \n",
       "dropoff_longitude                                                     NaN  \n",
       "dropoff_location                                                     None  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5eaa71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 1/1 [00:00<00:00, 1182.83query/s]\n",
      "Downloading: 100%|██████████| 7/7 [00:01<00:00,  6.32rows/s]\n"
     ]
    }
   ],
   "source": [
    "%%bigquery data\n",
    "\n",
    "SELECT \n",
    "    CAST(EXTRACT(DAYOFWEEK FROM trip_start_timestamp) AS string) AS trip_dayofweek, \n",
    "    FORMAT_DATE('%A',cast(trip_start_timestamp as date)) AS trip_dayname,\n",
    "    COUNT(*) as trip_count,\n",
    "FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "WHERE\n",
    "    EXTRACT(YEAR FROM trip_start_timestamp) = 2015 \n",
    "GROUP BY\n",
    "    trip_dayofweek,\n",
    "    trip_dayname\n",
    "ORDER BY\n",
    "    trip_dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd9a3f69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_dayofweek</th>\n",
       "      <th>trip_dayname</th>\n",
       "      <th>trip_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>4141154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Monday</td>\n",
       "      <td>4105900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>4378805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>4542810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>4918190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Friday</td>\n",
       "      <td>5289830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>5009186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  trip_dayofweek trip_dayname  trip_count\n",
       "0              1       Sunday     4141154\n",
       "1              2       Monday     4105900\n",
       "2              3      Tuesday     4378805\n",
       "3              4    Wednesday     4542810\n",
       "4              5     Thursday     4918190\n",
       "5              6       Friday     5289830\n",
       "6              7     Saturday     5009186"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e51de6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='trip_dayname'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.plot(kind='bar', x='trip_dayname', y='trip_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0829b8a",
   "metadata": {},
   "source": [
    "### Create  data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72569647",
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_DATASET_NAME = f'{PREFIX}_dataset' \n",
    "BQ_TRAIN_SPLIT_NAME = 'training'\n",
    "BQ_VALID_SPLIT_NAME = 'validation'\n",
    "BQ_TEST_SPLIT_NAME = 'testing'\n",
    "BQ_LOCATION = 'US'\n",
    "SAMPLE_SIZE = 500000\n",
    "YEAR = 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ddae14",
   "metadata": {},
   "source": [
    "#### Create a BQ dataset to host the splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8dd17ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset jk-vertex-workshop.jkvw_dataset already exists\n"
     ]
    }
   ],
   "source": [
    "client = bigquery.Client()\n",
    "\n",
    "dataset_id = f'{PROJECT}.{BQ_DATASET_NAME}'\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = BQ_LOCATION\n",
    "\n",
    "try:\n",
    "    dataset = client.create_dataset(dataset, timeout=30)\n",
    "    print('Created dataset: ', dataset_id)\n",
    "except exceptions.Conflict:\n",
    "    print('Dataset {} already exists'.format(dataset_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9e8dc0",
   "metadata": {},
   "source": [
    "#### Create training, validation, and test splits tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32fd8f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_script_template = '''\n",
    "CREATE TEMP TABLE features \n",
    "AS (\n",
    "    WITH\n",
    "      taxitrips AS (\n",
    "      SELECT\n",
    "        FORMAT_DATETIME('%Y-%d-%m', trip_start_timestamp) AS date,\n",
    "        trip_start_timestamp,\n",
    "        trip_seconds,\n",
    "        trip_miles,\n",
    "        payment_type,\n",
    "        pickup_longitude,\n",
    "        pickup_latitude,\n",
    "        dropoff_longitude,\n",
    "        dropoff_latitude,\n",
    "        tips,\n",
    "        fare\n",
    "      FROM\n",
    "        `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "      WHERE 1=1 \n",
    "      AND pickup_longitude IS NOT NULL\n",
    "      AND pickup_latitude IS NOT NULL\n",
    "      AND dropoff_longitude IS NOT NULL\n",
    "      AND dropoff_latitude IS NOT NULL\n",
    "      AND trip_miles > 0\n",
    "      AND trip_seconds > 0\n",
    "      AND fare > 0\n",
    "      AND EXTRACT(YEAR FROM trip_start_timestamp) = @YEAR\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "      trip_start_timestamp,\n",
    "      EXTRACT(MONTH from trip_start_timestamp) as trip_month,\n",
    "      EXTRACT(DAY from trip_start_timestamp) as trip_day,\n",
    "      EXTRACT(DAYOFWEEK from trip_start_timestamp) as trip_day_of_week,\n",
    "      EXTRACT(HOUR from trip_start_timestamp) as trip_hour,\n",
    "      trip_seconds,\n",
    "      trip_miles,\n",
    "      payment_type,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(pickup_longitude, pickup_latitude), 0.1)\n",
    "      ) AS pickup_grid,\n",
    "      ST_AsText(\n",
    "          ST_SnapToGrid(ST_GeogPoint(dropoff_longitude, dropoff_latitude), 0.1)\n",
    "      ) AS dropoff_grid,\n",
    "      ST_Distance(\n",
    "          ST_GeogPoint(pickup_longitude, pickup_latitude), \n",
    "          ST_GeogPoint(dropoff_longitude, dropoff_latitude)\n",
    "      ) AS euclidean,\n",
    "      IF((tips/fare >= 0.2), 1, 0) AS tip_bin,\n",
    "      CASE (ABS(MOD(FARM_FINGERPRINT(date),10))) \n",
    "          WHEN 9 THEN 'TEST'\n",
    "          WHEN 8 THEN 'VALIDATE'\n",
    "          ELSE 'TRAIN' END AS data_split\n",
    "    FROM\n",
    "      taxitrips\n",
    "    LIMIT @LIMIT\n",
    ");\n",
    "\n",
    "CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@TRAIN_SPLIT`\n",
    "AS\n",
    "SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "FROM features\n",
    "WHERE data_split='TRAIN';\n",
    "\n",
    "CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@VALIDATE_SPLIT`\n",
    "AS\n",
    "SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "FROM features\n",
    "WHERE data_split='VALIDATE';\n",
    "\n",
    "CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@TEST_SPLIT`\n",
    "AS\n",
    "SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "FROM features\n",
    "WHERE data_split='TEST';\n",
    "\n",
    "DROP TABLE features;\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab2b34d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.bigquery.table._EmptyRowIterator at 0x7f782f353cd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_script = sql_script_template.replace(\n",
    "    '@PROJECT', PROJECT).replace(\n",
    "    '@DATASET', BQ_DATASET_NAME).replace(\n",
    "    '@TRAIN_SPLIT', BQ_TRAIN_SPLIT_NAME).replace(\n",
    "    '@VALIDATE_SPLIT', BQ_VALID_SPLIT_NAME).replace(\n",
    "    '@TEST_SPLIT', BQ_TEST_SPLIT_NAME).replace(\n",
    "    '@YEAR', str(YEAR)).replace(\n",
    "    '@LIMIT', str(SAMPLE_SIZE))\n",
    "\n",
    "job = client.query(sql_script)\n",
    "job.result()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793898e8",
   "metadata": {},
   "source": [
    "#### Review splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7acb31ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_script = f'''\n",
    "SELECT * \n",
    "FROM {PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}\n",
    "'''\n",
    "\n",
    "data = client.query(sql_script).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ee05508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_month</th>\n",
       "      <th>trip_day</th>\n",
       "      <th>trip_day_of_week</th>\n",
       "      <th>trip_hour</th>\n",
       "      <th>trip_seconds</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>pickup_grid</th>\n",
       "      <th>dropoff_grid</th>\n",
       "      <th>euclidean</th>\n",
       "      <th>tip_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>89</td>\n",
       "      <td>0.07</td>\n",
       "      <td>Cash</td>\n",
       "      <td>POINT(-87.6 41.9)</td>\n",
       "      <td>POINT(-87.6 41.9)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>480</td>\n",
       "      <td>1.20</td>\n",
       "      <td>Cash</td>\n",
       "      <td>POINT(-87.7 41.9)</td>\n",
       "      <td>POINT(-87.7 41.9)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>1500</td>\n",
       "      <td>1.40</td>\n",
       "      <td>Cash</td>\n",
       "      <td>POINT(-87.7 41.8)</td>\n",
       "      <td>POINT(-87.7 41.8)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>518</td>\n",
       "      <td>0.80</td>\n",
       "      <td>Cash</td>\n",
       "      <td>POINT(-87.7 42)</td>\n",
       "      <td>POINT(-87.7 42)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>124</td>\n",
       "      <td>0.39</td>\n",
       "      <td>Cash</td>\n",
       "      <td>POINT(-87.6 41.9)</td>\n",
       "      <td>POINT(-87.6 41.9)</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trip_month  trip_day  trip_day_of_week  trip_hour  trip_seconds  \\\n",
       "0           2         1                 7         13            89   \n",
       "1           2         1                 7         16           480   \n",
       "2           2         1                 7         21          1500   \n",
       "3           2         1                 7         15           518   \n",
       "4           2         1                 7         15           124   \n",
       "\n",
       "   trip_miles payment_type        pickup_grid       dropoff_grid  euclidean  \\\n",
       "0        0.07         Cash  POINT(-87.6 41.9)  POINT(-87.6 41.9)        0.0   \n",
       "1        1.20         Cash  POINT(-87.7 41.9)  POINT(-87.7 41.9)        0.0   \n",
       "2        1.40         Cash  POINT(-87.7 41.8)  POINT(-87.7 41.8)        0.0   \n",
       "3        0.80         Cash    POINT(-87.7 42)    POINT(-87.7 42)        0.0   \n",
       "4        0.39         Cash  POINT(-87.6 41.9)  POINT(-87.6 41.9)        0.0   \n",
       "\n",
       "   tip_bin  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b054fe44",
   "metadata": {},
   "source": [
    "## Submitting Vertex training jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b22301b",
   "metadata": {},
   "source": [
    "### Display the model\n",
    "\n",
    "`tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")`\n",
    "\n",
    "![Model](model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17155570",
   "metadata": {},
   "source": [
    "### Prepare a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a27592f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_FOLDER = 'trainer'\n",
    "if tf.io.gfile.exists(SCRIPT_FOLDER):\n",
    "    tf.io.gfile.rmtree(SCRIPT_FOLDER)\n",
    "tf.io.gfile.mkdir(SCRIPT_FOLDER)\n",
    "file_path = os.path.join(SCRIPT_FOLDER, 'train.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5522029a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {file_path}\n",
    "\n",
    "\n",
    "# Copyright 2021 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "\n",
    "import hypertune\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow_io import bigquery as tfio_bq\n",
    "\n",
    "from tensorboard.plugins.hparams import api as tb_hp\n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('epochs', 3, 'Nubmer of epochs')\n",
    "flags.DEFINE_integer('units', 32, 'Number units in a hidden layer')\n",
    "flags.DEFINE_integer('per_replica_batch_size', 128, 'Per replica batch size')\n",
    "flags.DEFINE_float('dropout_ratio', 0.5, 'Dropout ratio')\n",
    "flags.DEFINE_string('training_table', None, 'Training table name')\n",
    "flags.DEFINE_string('validation_table', None, 'Validationa table name')\n",
    "flags.mark_flag_as_required('training_table')\n",
    "flags.mark_flag_as_required('validation_table')\n",
    "\n",
    "LOCAL_MODEL_DIR = '/tmp/saved_model'\n",
    "LOCAL_TB_DIR = '/tmp/logs'\n",
    "LOCAL_CHECKPOINT_DIR = '/tmp/checkpoints'\n",
    "EVALUATION_FILE_NAME = 'evaluations.json'\n",
    "\n",
    "# Define features\n",
    "FEATURES = {\n",
    "    \"tip_bin\": (\"categorical\", tf.int64),\n",
    "    \"trip_month\": (\"categorical\", tf.int64),\n",
    "    \"trip_day\": (\"categorical\", tf.int64),\n",
    "    \"trip_day_of_week\": (\"categorical\", tf.int64),\n",
    "    \"trip_hour\": (\"categorical\", tf.int64),\n",
    "    \"payment_type\": (\"categorical\", tf.string),\n",
    "    \"pickup_grid\": (\"categorical\", tf.string),\n",
    "    \"dropoff_grid\": (\"categorical\", tf.string),\n",
    "    \"euclidean\": (\"numeric\", tf.double),\n",
    "    \"trip_seconds\": (\"numeric\", tf.int64),\n",
    "    \"trip_miles\": (\"numeric\", tf.double),\n",
    "}\n",
    "TARGET_FEATURE_NAME = 'tip_bin'\n",
    "\n",
    " # Set hparams for Tensorboard and Vertex hp tuner\n",
    "HP_DROPOUT = tb_hp.HParam(\"dropout\")\n",
    "HP_UNITS = tb_hp.HParam(\"units\")\n",
    "HPARAMS = [\n",
    "    HP_UNITS,\n",
    "    HP_DROPOUT,\n",
    "]\n",
    "METRICS = [\n",
    "    tb_hp.Metric(\n",
    "        \"epoch_accuracy\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"epoch accuracy\"),\n",
    "]\n",
    "HPTUNE_METRIC = 'val_accuracy'\n",
    "    \n",
    "\n",
    "def set_job_dirs():\n",
    "    \"\"\"Sets job directories and hyperparameter tuning trial id\n",
    "    based on env variables set by Vertex AI.\"\"\"\n",
    "    \n",
    "    model_dir = os.getenv('AIP_MODEL_DIR', LOCAL_MODEL_DIR)\n",
    "    tb_dir = os.getenv('AIP_TENSORBOARD_LOG_DIR', LOCAL_TB_DIR)\n",
    "    checkpoint_dir = os.getenv('AIP_CHECKPOINT_DIR', LOCAL_CHECKPOINT_DIR)\n",
    "    \n",
    "    path = os.path.normpath(tb_dir)\n",
    "    trial_id = re.match('^[0-9]+$', path.split(os.sep)[-2])\n",
    "    if not trial_id:\n",
    "        trial_id = '0'\n",
    "    else:\n",
    "        trial_id = trial_id[0]\n",
    "    logging.info(trial_id)\n",
    "    \n",
    "    return model_dir, tb_dir, checkpoint_dir, trial_id\n",
    "\n",
    "\n",
    "def get_bq_dataset(table_name, selected_fields, target_feature='tip_bin', batch_size=32):\n",
    "    \n",
    "    def _transform_row(row_dict):\n",
    "        trimmed_dict = {column:\n",
    "                       (tf.strings.strip(tensor) if tensor.dtype == 'string' else tensor) \n",
    "                       for (column,tensor) in row_dict.items()\n",
    "                       }\n",
    "        target = trimmed_dict.pop(target_feature)\n",
    "        return (trimmed_dict, target)\n",
    "\n",
    "    project_id, dataset_id, table_id = table_name.split('.')\n",
    "    \n",
    "    client = tfio_bq.BigQueryClient()\n",
    "    parent = f'projects/{project_id}'\n",
    "\n",
    "    read_session = client.read_session(\n",
    "        parent=parent,\n",
    "        project_id=project_id,\n",
    "        table_id=table_id,\n",
    "        dataset_id=dataset_id,\n",
    "        selected_fields=selected_fields,\n",
    "    )\n",
    "\n",
    "    dataset = read_session.parallel_read_rows().map(_transform_row).batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_category_encoding_layer(name, dataset, dtype):\n",
    "    \"\"\"Creates a CategoryEncoding layer for a given feature.\"\"\"\n",
    "\n",
    "    if dtype == tf.string:\n",
    "      index = preprocessing.StringLookup()\n",
    "    else:\n",
    "      index = preprocessing.IntegerLookup()\n",
    "\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    index.adapt(feature_ds)\n",
    "    encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
    "\n",
    "    return lambda feature: encoder(index(feature))\n",
    "\n",
    "\n",
    "def get_normalization_layer(name, dataset):\n",
    "  \"\"\"\"Creates a Normalization layer for a given feature.\"\"\"\n",
    "  normalizer = preprocessing.Normalization()\n",
    "\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "  normalizer.adapt(feature_ds)\n",
    "\n",
    "  return normalizer\n",
    "\n",
    "\n",
    "def create_model(dataset, input_features, units, dropout_ratio):\n",
    "    \"\"\"Creates a binary classifier for Chicago Taxi tip prediction task.\"\"\"\n",
    "    \n",
    "    all_inputs = []\n",
    "    encoded_features = []\n",
    "    for feature_name, feature_info in input_features.items():\n",
    "        col = tf.keras.Input(shape=(1,), name=feature_name, dtype=feature_info[1])\n",
    "        if feature_info[0] == 'categorical':\n",
    "            \n",
    "            encoding_layer = get_category_encoding_layer(feature_name, \n",
    "                                                         dataset,\n",
    "                                                         feature_info[1])\n",
    "        else:\n",
    "            encoding_layer = get_normalization_layer(feature_name,\n",
    "                                                     dataset) \n",
    "        encoded_col = encoding_layer(col)\n",
    "        all_inputs.append(col)\n",
    "        encoded_features.append(encoded_col)\n",
    "        \n",
    "    all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(units, activation=\"relu\")(all_features)\n",
    "    x = tf.keras.layers.Dropout(dropout_ratio)(x)\n",
    "    output = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.Model(all_inputs, output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "class HptuneCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    A custom Keras callback class that reports a metric to hypertuner\n",
    "    at the end of each epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, metric_tag, metric_value):\n",
    "        super(HptuneCallback, self).__init__()\n",
    "        self.metric_tag = metric_tag\n",
    "        self.metric_value = metric_value\n",
    "        self.hpt = hypertune.HyperTune()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.hpt.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag=self.metric_tag,\n",
    "            metric_value=logs[self.metric_value],\n",
    "            global_step=epoch)\n",
    "        \n",
    "\n",
    "def main(argv):\n",
    "    del argv\n",
    "    \n",
    "    # Set distribution strategy\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "    global_batch_size = (strategy.num_replicas_in_sync *\n",
    "                         FLAGS.per_replica_batch_size)\n",
    "    \n",
    "    # Prepare datasets\n",
    "    selected_fields = {key: {'output_type': value[1]} for key, value in FEATURES.items()}\n",
    "    validation_ds = get_bq_dataset(FLAGS.validation_table, \n",
    "                                   selected_fields, \n",
    "                                   batch_size=global_batch_size)\n",
    "    training_ds = get_bq_dataset(FLAGS.training_table,\n",
    "                                 selected_fields,\n",
    "                                 batch_size=global_batch_size)\n",
    "    \n",
    "    # Configure Tensorboard hparams\n",
    "    model_dir, tb_dir, checkpoint_dir, trial_id = set_job_dirs()\n",
    "    with tf.summary.create_file_writer(tb_dir).as_default():\n",
    "        tb_hp.hparams_config(hparams=HPARAMS, metrics=METRICS)\n",
    "        \n",
    "    hparams = {\n",
    "        HP_UNITS: FLAGS.units,\n",
    "        HP_DROPOUT: FLAGS.dropout_ratio\n",
    "    }\n",
    "    \n",
    "    # Create the model\n",
    "    input_features = {key: value for key, value in FEATURES.items() if key != TARGET_FEATURE_NAME}\n",
    "    logging.info('Creating the model ...')\n",
    "    with strategy.scope():\n",
    "        model = create_model(training_ds, input_features, hparams[HP_UNITS], hparams[HP_DROPOUT])\n",
    "        model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Configure training regimen\n",
    "    callbacks = [tf.keras.callbacks.experimental.BackupAndRestore(backup_dir=checkpoint_dir)]\n",
    "    callbacks.append(tf.keras.callbacks.TensorBoard(log_dir=tb_dir, \n",
    "                                                    update_freq='batch',\n",
    "                                                    profile_batch=0))\n",
    "    callbacks.append(tb_hp.KerasCallback(writer=tb_dir, \n",
    "                                         hparams=hparams,\n",
    "                                         trial_id=trial_id))\n",
    "    callbacks.append(HptuneCallback(HPTUNE_METRIC, HPTUNE_METRIC))\n",
    "    \n",
    "    # Start training\n",
    "    logging.info('Starting training ...')\n",
    "    model.fit(training_ds, \n",
    "              epochs=FLAGS.epochs, \n",
    "              validation_data=validation_ds,\n",
    "              callbacks=callbacks)\n",
    "    \n",
    "    # Save trained model\n",
    "    logging.info('Training completed. Saving the trained model to: {}'.format(model_dir))\n",
    "    model.save(model_dir)  \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "    app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876df113",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aec84664",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8341c4a0",
   "metadata": {},
   "source": [
    "### Configure and submit a custom Vertex job using a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3a48b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.utils.source_utils:Training script copied to:\n",
      "gs://jkvw-bucket/jobs/JOB_20210613_134449/aiplatform-2021-06-13-13:44:50.169-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
      "INFO:google.cloud.aiplatform.jobs:Creating CustomJob\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob created. Resource name: projects/910094146258/locations/us-central1/customJobs/5787341025450131456\n",
      "INFO:google.cloud.aiplatform.jobs:To use this CustomJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:custom_job = aiplatform.CustomJob.get('projects/910094146258/locations/us-central1/customJobs/5787341025450131456')\n",
      "INFO:google.cloud.aiplatform.jobs:View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/5787341025450131456?project=910094146258\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/5787341025450131456 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/5787341025450131456 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/5787341025450131456 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "job_name = job_name = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "base_output_dir = f'{STAGING_BUCKET}/jobs/{job_name}'\n",
    "\n",
    "#container_uri = 'us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-4:latest'\n",
    "container_uri = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-4:latest'\n",
    "requirements = ['tensorflow-datasets==4.3.0']\n",
    "args = [\n",
    "    '--epochs=2', \n",
    "    '--per_replica_batch_size=128',\n",
    "    '--training_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "    '--validation_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "]\n",
    "\n",
    "machine_type = 'n1-standard-4'\n",
    "#accelerator_type = 'NVIDIA_TESLA_T4'\n",
    "#accelerator_count = 1\n",
    "\n",
    "job = vertex_ai.CustomJob.from_local_script(\n",
    "    display_name=job_name,\n",
    "    machine_type=machine_type,\n",
    "#    accelerator_type=accelerator_type,\n",
    "#    accelerator_count=accelerator_count,\n",
    "    script_path='trainer/train.py',\n",
    "    container_uri=container_uri,\n",
    "    requirements=requirements,\n",
    "    args=args,\n",
    "    staging_bucket=base_output_dir\n",
    ")\n",
    "\n",
    "job.run(sync=False, \n",
    "        service_account=VERTEX_SA,\n",
    "        tensorboard=TENSORBOARD,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e04e188",
   "metadata": {},
   "source": [
    "### Configure and submit a Vertex job using a custom container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663cc236",
   "metadata": {},
   "source": [
    "#### Create a docker file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fe32cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = 'gcr.io/deeplearning-platform-release/tf2-cpu.2-4'\n",
    "TRAIN_IMAGE = f'gcr.io/{PROJECT}/{PREFIX}_taxi_trainer'\n",
    "\n",
    "dockerfile = f'''\n",
    "FROM {BASE_IMAGE}\n",
    "\n",
    "WORKDIR /trainer\n",
    "RUN pip install cloudml-hypertune\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n",
    "'''\n",
    "\n",
    "with open(os.path.join(SCRIPT_FOLDER, 'Dockerfile'), 'w') as f:\n",
    "    f.write(dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9746a4cd",
   "metadata": {},
   "source": [
    "#### Build a container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcc55684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 9.0 KiB before compression.\n",
      "Uploading tarball of [trainer] to [gs://jk-vertex-workshop_cloudbuild/source/1623591947.733696-60b788b2fbe7459ab67a1ff4a5174d01.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/jk-vertex-workshop/locations/global/builds/c9cf3a07-0331-436e-803a-be37abb55769].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/c9cf3a07-0331-436e-803a-be37abb55769?project=910094146258].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"c9cf3a07-0331-436e-803a-be37abb55769\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://jk-vertex-workshop_cloudbuild/source/1623591947.733696-60b788b2fbe7459ab67a1ff4a5174d01.tgz#1623591948032840\n",
      "Copying gs://jk-vertex-workshop_cloudbuild/source/1623591947.733696-60b788b2fbe7459ab67a1ff4a5174d01.tgz#1623591948032840...\n",
      "/ [1 files][  3.4 KiB/  3.4 KiB]                                                \n",
      "Operation completed over 1 objects/3.4 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  11.78kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-4\n",
      "latest: Pulling from deeplearning-platform-release/tf2-cpu.2-4\n",
      "01bf7da0a88c: Pulling fs layer\n",
      "f3b4a5f15c7a: Pulling fs layer\n",
      "57ffbe87baa1: Pulling fs layer\n",
      "424e7c9d5d89: Pulling fs layer\n",
      "9b397537aef0: Pulling fs layer\n",
      "2bd5028f4b85: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "b2ed56b85d3a: Pulling fs layer\n",
      "8bfb788e9874: Pulling fs layer\n",
      "0618fb353339: Pulling fs layer\n",
      "42045a665612: Pulling fs layer\n",
      "031d8d7b75f7: Pulling fs layer\n",
      "5780cc9addac: Pulling fs layer\n",
      "8fbe78107b3d: Pulling fs layer\n",
      "eee173fc570a: Pulling fs layer\n",
      "9334ecc802d5: Pulling fs layer\n",
      "c631c38965fd: Pulling fs layer\n",
      "803ecb627365: Pulling fs layer\n",
      "4466b5c2ac37: Pulling fs layer\n",
      "16a6777d4439: Pulling fs layer\n",
      "7a84944cecd7: Pulling fs layer\n",
      "424e7c9d5d89: Waiting\n",
      "9b397537aef0: Waiting\n",
      "2bd5028f4b85: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "b2ed56b85d3a: Waiting\n",
      "8bfb788e9874: Waiting\n",
      "0618fb353339: Waiting\n",
      "42045a665612: Waiting\n",
      "031d8d7b75f7: Waiting\n",
      "5780cc9addac: Waiting\n",
      "8fbe78107b3d: Waiting\n",
      "eee173fc570a: Waiting\n",
      "9334ecc802d5: Waiting\n",
      "c631c38965fd: Waiting\n",
      "803ecb627365: Waiting\n",
      "4466b5c2ac37: Waiting\n",
      "16a6777d4439: Waiting\n",
      "7a84944cecd7: Waiting\n",
      "f3b4a5f15c7a: Verifying Checksum\n",
      "f3b4a5f15c7a: Download complete\n",
      "57ffbe87baa1: Download complete\n",
      "424e7c9d5d89: Verifying Checksum\n",
      "424e7c9d5d89: Download complete\n",
      "01bf7da0a88c: Verifying Checksum\n",
      "01bf7da0a88c: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "b2ed56b85d3a: Verifying Checksum\n",
      "b2ed56b85d3a: Download complete\n",
      "2bd5028f4b85: Verifying Checksum\n",
      "2bd5028f4b85: Download complete\n",
      "0618fb353339: Verifying Checksum\n",
      "0618fb353339: Download complete\n",
      "42045a665612: Download complete\n",
      "031d8d7b75f7: Verifying Checksum\n",
      "031d8d7b75f7: Download complete\n",
      "5780cc9addac: Verifying Checksum\n",
      "5780cc9addac: Download complete\n",
      "8fbe78107b3d: Verifying Checksum\n",
      "8fbe78107b3d: Download complete\n",
      "eee173fc570a: Verifying Checksum\n",
      "eee173fc570a: Download complete\n",
      "9334ecc802d5: Verifying Checksum\n",
      "9334ecc802d5: Download complete\n",
      "c631c38965fd: Verifying Checksum\n",
      "c631c38965fd: Download complete\n",
      "8bfb788e9874: Verifying Checksum\n",
      "8bfb788e9874: Download complete\n",
      "9b397537aef0: Verifying Checksum\n",
      "9b397537aef0: Download complete\n",
      "4466b5c2ac37: Verifying Checksum\n",
      "4466b5c2ac37: Download complete\n",
      "16a6777d4439: Verifying Checksum\n",
      "16a6777d4439: Download complete\n",
      "7a84944cecd7: Verifying Checksum\n",
      "7a84944cecd7: Download complete\n",
      "01bf7da0a88c: Pull complete\n",
      "f3b4a5f15c7a: Pull complete\n",
      "57ffbe87baa1: Pull complete\n",
      "424e7c9d5d89: Pull complete\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/5787341025450131456 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "803ecb627365: Verifying Checksum\n",
      "803ecb627365: Download complete\n",
      "9b397537aef0: Pull complete\n",
      "2bd5028f4b85: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "b2ed56b85d3a: Pull complete\n",
      "8bfb788e9874: Pull complete\n",
      "0618fb353339: Pull complete\n",
      "42045a665612: Pull complete\n",
      "031d8d7b75f7: Pull complete\n",
      "5780cc9addac: Pull complete\n",
      "8fbe78107b3d: Pull complete\n",
      "eee173fc570a: Pull complete\n",
      "9334ecc802d5: Pull complete\n",
      "c631c38965fd: Pull complete\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/5787341025450131456 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "803ecb627365: Pull complete\n",
      "4466b5c2ac37: Pull complete\n",
      "16a6777d4439: Pull complete\n",
      "7a84944cecd7: Pull complete\n",
      "Digest: sha256:56489bdb00b3f5c342d8cbcdeea68fd63b982858199c4ed7edac15614954b110\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/tf2-cpu.2-4:latest\n",
      " ---> fbf8f454dc05\n",
      "Step 2/5 : WORKDIR /trainer\n",
      " ---> Running in ba9a67999816\n",
      "Removing intermediate container ba9a67999816\n",
      " ---> 17f64deaf6c7\n",
      "Step 3/5 : RUN pip install cloudml-hypertune\n",
      " ---> Running in 66a0fb4c15ce\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Building wheels for collected packages: cloudml-hypertune\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3988 sha256=253d2fe650e8ff01f265e4ca78588f3063f2436214248ff260a7d57c906dd14c\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "Successfully built cloudml-hypertune\n",
      "Installing collected packages: cloudml-hypertune\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6\n",
      "\u001b[91mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 66a0fb4c15ce\n",
      " ---> 4c9ce309e0fa\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 1ff66d375be4\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 75a303dfea0b\n",
      "Removing intermediate container 75a303dfea0b\n",
      " ---> 0523ba8e5f66\n",
      "Successfully built 0523ba8e5f66\n",
      "Successfully tagged gcr.io/jk-vertex-workshop/jkvw_taxi_trainer:latest\n",
      "PUSH\n",
      "Pushing gcr.io/jk-vertex-workshop/jkvw_taxi_trainer\n",
      "The push refers to repository [gcr.io/jk-vertex-workshop/jkvw_taxi_trainer]\n",
      "f6f5cc403895: Preparing\n",
      "7243b6d9907f: Preparing\n",
      "d46d7a9c3598: Preparing\n",
      "d39fae2e5e4e: Preparing\n",
      "33302bd81efa: Preparing\n",
      "0c5f899e9bc7: Preparing\n",
      "5e593cdd4a12: Preparing\n",
      "06a5bf49b163: Preparing\n",
      "b34dae69fc5d: Preparing\n",
      "0ffb7465dde9: Preparing\n",
      "e2563d1ada9a: Preparing\n",
      "42b027d1e826: Preparing\n",
      "636a7c2e7d03: Preparing\n",
      "1ba1158adf89: Preparing\n",
      "96e46d1341e8: Preparing\n",
      "954f6dc3f7f5: Preparing\n",
      "8760a171b659: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "a0710233fd2d: Preparing\n",
      "05449afa4be9: Preparing\n",
      "5b9e34b5cf74: Preparing\n",
      "8cafc6d2db45: Preparing\n",
      "a5d4bacb0351: Preparing\n",
      "5153e1acaabc: Preparing\n",
      "0c5f899e9bc7: Waiting\n",
      "5e593cdd4a12: Waiting\n",
      "06a5bf49b163: Waiting\n",
      "b34dae69fc5d: Waiting\n",
      "0ffb7465dde9: Waiting\n",
      "e2563d1ada9a: Waiting\n",
      "42b027d1e826: Waiting\n",
      "636a7c2e7d03: Waiting\n",
      "1ba1158adf89: Waiting\n",
      "96e46d1341e8: Waiting\n",
      "954f6dc3f7f5: Waiting\n",
      "8760a171b659: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "a0710233fd2d: Waiting\n",
      "5153e1acaabc: Waiting\n",
      "5b9e34b5cf74: Waiting\n",
      "8cafc6d2db45: Waiting\n",
      "05449afa4be9: Waiting\n",
      "a5d4bacb0351: Waiting\n",
      "33302bd81efa: Layer already exists\n",
      "d39fae2e5e4e: Layer already exists\n",
      "5e593cdd4a12: Layer already exists\n",
      "0c5f899e9bc7: Layer already exists\n",
      "06a5bf49b163: Layer already exists\n",
      "b34dae69fc5d: Layer already exists\n",
      "e2563d1ada9a: Layer already exists\n",
      "0ffb7465dde9: Layer already exists\n",
      "42b027d1e826: Layer already exists\n",
      "636a7c2e7d03: Layer already exists\n",
      "1ba1158adf89: Layer already exists\n",
      "96e46d1341e8: Layer already exists\n",
      "954f6dc3f7f5: Layer already exists\n",
      "8760a171b659: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "a0710233fd2d: Layer already exists\n",
      "05449afa4be9: Layer already exists\n",
      "5b9e34b5cf74: Layer already exists\n",
      "a5d4bacb0351: Layer already exists\n",
      "8cafc6d2db45: Layer already exists\n",
      "5153e1acaabc: Layer already exists\n",
      "f6f5cc403895: Pushed\n",
      "7243b6d9907f: Pushed\n",
      "d46d7a9c3598: Pushed\n",
      "latest: digest: sha256:2c891ffedd5ecfef317c3c1b3830db02fe48d5098c37dd5610b669cf6d31af9b size: 5335\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                            IMAGES                                                 STATUS\n",
      "c9cf3a07-0331-436e-803a-be37abb55769  2021-06-13T13:45:48+00:00  2M27S     gs://jk-vertex-workshop_cloudbuild/source/1623591947.733696-60b788b2fbe7459ab67a1ff4a5174d01.tgz  gcr.io/jk-vertex-workshop/jkvw_taxi_trainer (+1 more)  SUCCESS\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/5787341025450131456 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag {TRAIN_IMAGE} {SCRIPT_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e648a77",
   "metadata": {},
   "source": [
    "#### Prepare worker pool specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb9d5ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "#            \"accelerator_type\": \"NVIDIA_TESLA_T4\",\n",
    "#            \"accelerator_count\": 1,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE,\n",
    "#            \"command\": [\"python\", \"train.py\"],\n",
    "            \"args\": [\n",
    "                '--epochs=2', \n",
    "                '--per_replica_batch_size=128',\n",
    "                '--training_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "                '--validation_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_VALID_SPLIT_NAME}',\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40506326",
   "metadata": {},
   "source": [
    "#### Submit and monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "700450b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating CustomJob\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob created. Resource name: projects/910094146258/locations/us-central1/customJobs/4978944892337127424\n",
      "INFO:google.cloud.aiplatform.jobs:To use this CustomJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:custom_job = aiplatform.CustomJob.get('projects/910094146258/locations/us-central1/customJobs/4978944892337127424')\n",
      "INFO:google.cloud.aiplatform.jobs:View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/4978944892337127424?project=910094146258\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/4978944892337127424 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/4978944892337127424 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "job_name = \"JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "job = vertex_ai.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=f'{STAGING_BUCKET}/{job_name}'\n",
    ")\n",
    "\n",
    "job.run(sync=False, \n",
    "        service_account=VERTEX_SA,\n",
    "        tensorboard=TENSORBOARD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd780327",
   "metadata": {},
   "source": [
    "### Configure and submit a hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f08662be",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = \"HYPER_JOB_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "hyperparameter_tuning_job_spec = {\n",
    "        \"display_name\": job_name,\n",
    "        \"max_trial_count\": 3,\n",
    "        \"parallel_trial_count\": 3,\n",
    "        \"max_failed_trial_count\": 1,\n",
    "        \"study_spec\": {\n",
    "            \"metrics\": [\n",
    "                {\n",
    "                    \"metric_id\": \"val_accuracy\",\n",
    "                    \"goal\": vertex_ai.gapic.StudySpec.MetricSpec.GoalType.MAXIMIZE,\n",
    "                }\n",
    "            ],\n",
    "            \"parameters\": [\n",
    "                {\n",
    "                    \"parameter_id\": \"units\",\n",
    "                    \"discrete_value_spec\": {\"values\": [32, 64]},\n",
    "                },\n",
    "                {\n",
    "                    \"parameter_id\": \"dropout_ratio\",\n",
    "                    \"double_value_spec\": {\"min_value\": 0.4, \"max_value\": 0.6},\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        \"trial_job_spec\": {\n",
    "            \"base_output_directory\": {\n",
    "                \"output_uri_prefix\": f\"{STAGING_BUCKET}/{job_name}\"\n",
    "            },\n",
    "            \"service_account\": VERTEX_SA,\n",
    "            \"tensorboard\": TENSORBOARD,\n",
    "            \"worker_pool_specs\": [\n",
    "                {\n",
    "                    \"machine_spec\": {\n",
    "                        \"machine_type\": \"n1-standard-4\",\n",
    "                      #  \"accelerator_type\": vertex_ai.gapic.AcceleratorType.NVIDIA_TESLA_T4,\n",
    "                      #  \"accelerator_count\": 1,\n",
    "                    },\n",
    "                    \"replica_count\": 1,\n",
    "                    \"container_spec\": {\n",
    "                        \"image_uri\": TRAIN_IMAGE,\n",
    "                        \"args\": [\n",
    "                            '--epochs=5', \n",
    "                            '--per_replica_batch_size=128',\n",
    "                            '--training_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "                            '--validation_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_VALID_SPLIT_NAME}',\n",
    "                        ],\n",
    "                    },\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c034fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_client = api_client = vertex_ai.initializer.global_config.create_client(\n",
    "        client_class=JobClientWithOverride, location_override=REGION\n",
    ")\n",
    "\n",
    "parent = f'projects/{PROJECT}/locations/{REGION}'\n",
    "version = 'v1beta1'\n",
    "\n",
    "response = job_client.select_version(version).create_hyperparameter_tuning_job(\n",
    "    parent=parent, hyperparameter_tuning_job=hyperparameter_tuning_job_spec\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d45c8a",
   "metadata": {},
   "source": [
    "#### Monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4dd74102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobState.JOB_STATE_QUEUED\n"
     ]
    }
   ],
   "source": [
    "job_spec = job_client.get_hyperparameter_tuning_job(\n",
    "    name = response.name\n",
    ")\n",
    "print(job_spec.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152c3c9f",
   "metadata": {},
   "source": [
    "#### Wait for tuning to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1890821d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/4978944892337127424 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/4978944892337127424 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/4978944892337127424 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/5787341025450131456 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/4978944892337127424 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/5787341025450131456 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/4978944892337127424 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/5787341025450131456 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/4978944892337127424 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/5787341025450131456 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob run completed. Resource name: projects/910094146258/locations/us-central1/customJobs/5787341025450131456\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/4978944892337127424 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob run completed. Resource name: projects/910094146258/locations/us-central1/customJobs/4978944892337127424\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have completed\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    job_response = job_client.get_hyperparameter_tuning_job(\n",
    "        name = response.name)\n",
    "    if job_response.state != vertex_ai.gapic.JobState.JOB_STATE_SUCCEEDED:\n",
    "        print(\"Study trials have not completed:\", job_response.state)\n",
    "        if (job_response.state == vertex_ai.gapic.JobState.JOB_STATE_FAILED or \n",
    "           job_response.state == vertex_ai.gapic.JobState.JOB_STATE_CANCELLED):\n",
    "            break\n",
    "    else:\n",
    "        print(\"Study trials have completed\")\n",
    "        break\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3083305",
   "metadata": {},
   "source": [
    "#### Review the results of the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29380003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: \"3\"\n",
      "state: SUCCEEDED\n",
      "parameters {\n",
      "  parameter_id: \"dropout_ratio\"\n",
      "  value {\n",
      "    number_value: 0.5474600299908973\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameter_id: \"units\"\n",
      "  value {\n",
      "    number_value: 32.0\n",
      "  }\n",
      "}\n",
      "final_measurement {\n",
      "  step_count: 4\n",
      "  metrics {\n",
      "    metric_id: \"val_accuracy\"\n",
      "    value: 0.8812273144721985\n",
      "  }\n",
      "}\n",
      "start_time {\n",
      "  seconds: 1623592300\n",
      "  nanos: 897660329\n",
      "}\n",
      "end_time {\n",
      "  seconds: 1623594046\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0.0\n",
    "best_trial = None\n",
    "for trial in job_response.trials:\n",
    "    accuracy = float(trial.final_measurement.metrics[0].value)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_trial = trial\n",
    "        best_accuracy = accuracy\n",
    "print(best_trial)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4801a58",
   "metadata": {},
   "source": [
    "#### Get the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c873285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://jkvw-bucket/HYPER_JOB_20210613_135131/\n",
      "gs://jkvw-bucket/HYPER_JOB_20210613_135131/1/\n",
      "gs://jkvw-bucket/HYPER_JOB_20210613_135131/2/\n",
      "gs://jkvw-bucket/HYPER_JOB_20210613_135131/3/\n"
     ]
    }
   ],
   "source": [
    "model_dir = f'{STAGING_BUCKET}/{job_name}'\n",
    "\n",
    "!gsutil ls {model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c8a3194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://jkvw-bucket/HYPER_JOB_20210613_135131/3/\n",
      "gs://jkvw-bucket/HYPER_JOB_20210613_135131/3/checkpoints/\n",
      "gs://jkvw-bucket/HYPER_JOB_20210613_135131/3/logs/\n",
      "gs://jkvw-bucket/HYPER_JOB_20210613_135131/3/model/\n"
     ]
    }
   ],
   "source": [
    "best_model_dir = '{}/{}'.format(model_dir, best_trial.id)\n",
    "\n",
    "!gsutil ls {best_model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48792c28",
   "metadata": {},
   "source": [
    "## Deploying a model to Vertex "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96073ce9",
   "metadata": {},
   "source": [
    "### Inspect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37531c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['dropoff_grid'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_dropoff_grid:0\n",
      "  inputs['euclidean'] tensor_info:\n",
      "      dtype: DT_DOUBLE\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_euclidean:0\n",
      "  inputs['payment_type'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_payment_type:0\n",
      "  inputs['pickup_grid'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_pickup_grid:0\n",
      "  inputs['trip_day'] tensor_info:\n",
      "      dtype: DT_INT64\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_trip_day:0\n",
      "  inputs['trip_day_of_week'] tensor_info:\n",
      "      dtype: DT_INT64\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_trip_day_of_week:0\n",
      "  inputs['trip_hour'] tensor_info:\n",
      "      dtype: DT_INT64\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_trip_hour:0\n",
      "  inputs['trip_miles'] tensor_info:\n",
      "      dtype: DT_DOUBLE\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_trip_miles:0\n",
      "  inputs['trip_month'] tensor_info:\n",
      "      dtype: DT_INT64\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_trip_month:0\n",
      "  inputs['trip_seconds'] tensor_info:\n",
      "      dtype: DT_INT64\n",
      "      shape: (-1, 1)\n",
      "      name: serving_default_trip_seconds:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['dense_1'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: StatefulPartitionedCall:0\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "saved_model_path = f'{best_model_dir}/model'\n",
    "#saved_model_path = 'gs://jk-vertex-workshop-bucket/models/taxi'\n",
    "\n",
    "!saved_model_cli show --dir {saved_model_path} --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5f24dd",
   "metadata": {},
   "source": [
    "### Upload the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3710b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/910094146258/locations/us-central1/models/4552585069411172352/operations/7912886392819023872\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/910094146258/locations/us-central1/models/4552585069411172352\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/910094146258/locations/us-central1/models/4552585069411172352')\n"
     ]
    }
   ],
   "source": [
    "display_name = 'Chicago taxi tip classifier'\n",
    "description = 'Chicago taxi tip TensorFlow classifier'\n",
    "serving_image_uri = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-4:latest'\n",
    "\n",
    "model = vertex_ai.Model.upload(\n",
    "    display_name=display_name,\n",
    "    description=description,\n",
    "    artifact_uri=saved_model_path,\n",
    "    serving_container_image_uri=serving_image_uri\n",
    ")\n",
    "\n",
    "model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a29423",
   "metadata": {},
   "source": [
    "### Create an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2963c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/910094146258/locations/us-central1/endpoints/2094508078262124544/operations/274781424798662656\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/910094146258/locations/us-central1/endpoints/2094508078262124544\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/910094146258/locations/us-central1/endpoints/2094508078262124544')\n"
     ]
    }
   ],
   "source": [
    "display_name = 'Taxi tip classifier endpoint'\n",
    "\n",
    "endpoint = vertex_ai.Endpoint.create(display_name=display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef425b07",
   "metadata": {},
   "source": [
    "### Deploy the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff19f4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/910094146258/locations/us-central1/endpoints/2094508078262124544\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/910094146258/locations/us-central1/endpoints/2094508078262124544/operations/4188409500983623680\n",
      "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/910094146258/locations/us-central1/endpoints/2094508078262124544\n"
     ]
    }
   ],
   "source": [
    "deployed_model_display_name = 'taxi-v1'\n",
    "traffic_percentage = 100\n",
    "machine_type = 'n1-standard-4'\n",
    "\n",
    "endpoint = model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        deployed_model_display_name=deployed_model_display_name,\n",
    "        machine_type=machine_type,\n",
    "        traffic_percentage=traffic_percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1912fd",
   "metadata": {},
   "source": [
    "## Invoking the deployed model using Vertex SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0656518c",
   "metadata": {},
   "source": [
    "### Get the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4941b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "for endpoint_info in vertex_ai.Endpoint.list(filter='display_name=\"Taxi tip classifier endpoint\"'):\n",
    "    print(endpoint_info)\n",
    "    \n",
    "endpoint = vertex_ai.Endpoint(endpoint_info.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d36a0f6",
   "metadata": {},
   "source": [
    "### Call the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04a73146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of tip > 20%: [[0.7561871]]\n"
     ]
    }
   ],
   "source": [
    "test_instances = [  \n",
    "    \n",
    "    {\n",
    "        \"dropoff_grid\": [\"POINT(-87.6 41.9)\"],\n",
    "        \"euclidean\": [2064.2696],\n",
    "        \"payment_type\": [\"Credit Card\"],\n",
    "        \"pickup_grid\": [\"POINT(-87.6 41.9)\"],\n",
    "        \"trip_miles\": [1.37],\n",
    "        \"trip_day\": [12],\n",
    "        \"trip_hour\": [16],\n",
    "        \"trip_month\": [2],\n",
    "        \"trip_day_of_week\": [4],\n",
    "        \"trip_seconds\": [555]\n",
    "    }\n",
    "]\n",
    "\n",
    "predictions = endpoint.predict(instances=test_instances)\n",
    "prob = tf.nn.sigmoid(predictions[0])\n",
    "print('Probability of tip > 20%:', prob.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d50626a",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9d5553",
   "metadata": {},
   "source": [
    "### Undeploy models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3eb5312b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[id: \"8217222140416491520\"\n",
       "model: \"projects/910094146258/locations/us-central1/models/4552585069411172352\"\n",
       "display_name: \"taxi-v1\"\n",
       "create_time {\n",
       "  seconds: 1623594464\n",
       "  nanos: 107543000\n",
       "}\n",
       "dedicated_resources {\n",
       "  machine_spec {\n",
       "    machine_type: \"n1-standard-4\"\n",
       "  }\n",
       "  min_replica_count: 1\n",
       "  max_replica_count: 1\n",
       "}\n",
       "]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4b77c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Undeploying Endpoint model: projects/910094146258/locations/us-central1/endpoints/2094508078262124544\n",
      "INFO:google.cloud.aiplatform.models:Undeploy Endpoint model backing LRO: projects/910094146258/locations/us-central1/endpoints/2094508078262124544/operations/7741749606978945024\n",
      "INFO:google.cloud.aiplatform.models:Endpoint model undeployed. Resource name: projects/910094146258/locations/us-central1/endpoints/2094508078262124544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Endpoint object at 0x7f78b0fa2210> \n",
       "resource name: projects/910094146258/locations/us-central1/endpoints/2094508078262124544"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.undeploy_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661782cd",
   "metadata": {},
   "source": [
    "### Delete endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b143a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.base:Deleting Endpoint : projects/910094146258/locations/us-central1/endpoints/2094508078262124544\n",
      "INFO:google.cloud.aiplatform.base:Delete Endpoint  backing LRO: projects/910094146258/locations/us-central1/operations/2481545242210205696\n",
      "INFO:google.cloud.aiplatform.base:Endpoint deleted. . Resource name: projects/910094146258/locations/us-central1/endpoints/2094508078262124544\n"
     ]
    }
   ],
   "source": [
    "endpoint.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d78609",
   "metadata": {},
   "source": [
    "### Delete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5250f0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.base:Deleting Model : projects/910094146258/locations/us-central1/models/4552585069411172352\n",
      "INFO:google.cloud.aiplatform.base:Delete Model  backing LRO: projects/910094146258/locations/us-central1/operations/1202522948036984832\n",
      "INFO:google.cloud.aiplatform.base:Model deleted. . Resource name: projects/910094146258/locations/us-central1/models/4552585069411172352\n"
     ]
    }
   ],
   "source": [
    "model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29517aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m69"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
