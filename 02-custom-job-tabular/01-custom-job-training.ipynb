{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "138e5443",
   "metadata": {},
   "source": [
    "# Training and deploying a tabular model using Vertex custom training job - Part 1\n",
    "\n",
    "![Training pipeline](../images/custom-tabular.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48782526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform_v1beta1 import types\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import exceptions\n",
    "\n",
    "from google.cloud.aiplatform.utils import JobClientWithOverride\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from tensorflow_io import bigquery as tfio_bq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a92339b",
   "metadata": {},
   "source": [
    "## Configure GCP settings\n",
    "\n",
    "*Before running the notebook make sure to follow the repo's README file to install the pre-requisites.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7012528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'jk-vertex-workshop'\n",
    "PREFIX = 'jk1'\n",
    "TENSORBOARD = 'projects/910094146258/locations/us-central1/tensorboards/1647226748082847744'\n",
    "\n",
    "REGION = 'us-central1'\n",
    "BQ_DATASET_NAME = f'{PREFIX}_dataset' \n",
    "BQ_TRAIN_SPLIT_NAME = 'training'\n",
    "BQ_VALID_SPLIT_NAME = 'validation'\n",
    "BQ_TEST_SPLIT_NAME = 'testing'\n",
    "STAGING_BUCKET = f'gs://{PREFIX}-bucket'\n",
    "VERTEX_SA = f'training-sa@{PROJECT}.iam.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef884c65",
   "metadata": {},
   "source": [
    "## Submitting Vertex training jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dbef0d",
   "metadata": {},
   "source": [
    "### Display the model\n",
    "\n",
    "`tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")`\n",
    "\n",
    "![Model](model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89328236",
   "metadata": {},
   "source": [
    "### Prepare a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1b1a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_FOLDER = 'trainer'\n",
    "if tf.io.gfile.exists(SCRIPT_FOLDER):\n",
    "    tf.io.gfile.rmtree(SCRIPT_FOLDER)\n",
    "tf.io.gfile.mkdir(SCRIPT_FOLDER)\n",
    "file_path = os.path.join(SCRIPT_FOLDER, 'train.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82eff9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {file_path}\n",
    "\n",
    "\n",
    "# Copyright 2021 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "\n",
    "import hypertune\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow_io import bigquery as tfio_bq\n",
    "\n",
    "from tensorboard.plugins.hparams import api as tb_hp\n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('epochs', 3, 'Nubmer of epochs')\n",
    "flags.DEFINE_integer('units', 32, 'Number units in a hidden layer')\n",
    "flags.DEFINE_integer('per_replica_batch_size', 128, 'Per replica batch size')\n",
    "flags.DEFINE_float('dropout_ratio', 0.5, 'Dropout ratio')\n",
    "flags.DEFINE_string('training_table', None, 'Training table name')\n",
    "flags.DEFINE_string('validation_table', None, 'Validationa table name')\n",
    "flags.mark_flag_as_required('training_table')\n",
    "flags.mark_flag_as_required('validation_table')\n",
    "\n",
    "LOCAL_MODEL_DIR = '/tmp/saved_model'\n",
    "LOCAL_TB_DIR = '/tmp/logs'\n",
    "LOCAL_CHECKPOINT_DIR = '/tmp/checkpoints'\n",
    "EVALUATION_FILE_NAME = 'evaluations.json'\n",
    "\n",
    "# Define features\n",
    "FEATURES = {\n",
    "    \"tip_bin\": (\"categorical\", tf.int64),\n",
    "    \"trip_month\": (\"categorical\", tf.int64),\n",
    "    \"trip_day\": (\"categorical\", tf.int64),\n",
    "    \"trip_day_of_week\": (\"categorical\", tf.int64),\n",
    "    \"trip_hour\": (\"categorical\", tf.int64),\n",
    "    \"payment_type\": (\"categorical\", tf.string),\n",
    "    \"pickup_grid\": (\"categorical\", tf.string),\n",
    "    \"dropoff_grid\": (\"categorical\", tf.string),\n",
    "    \"euclidean\": (\"numeric\", tf.double),\n",
    "    \"trip_seconds\": (\"numeric\", tf.int64),\n",
    "    \"trip_miles\": (\"numeric\", tf.double),\n",
    "}\n",
    "TARGET_FEATURE_NAME = 'tip_bin'\n",
    "\n",
    " # Set hparams for Tensorboard and Vertex hp tuner\n",
    "HP_DROPOUT = tb_hp.HParam(\"dropout\")\n",
    "HP_UNITS = tb_hp.HParam(\"units\")\n",
    "HPARAMS = [\n",
    "    HP_UNITS,\n",
    "    HP_DROPOUT,\n",
    "]\n",
    "METRICS = [\n",
    "    tb_hp.Metric(\n",
    "        \"epoch_accuracy\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"epoch accuracy\"),\n",
    "]\n",
    "HPTUNE_METRIC = 'val_accuracy'\n",
    "    \n",
    "\n",
    "def set_job_dirs():\n",
    "    \"\"\"Sets job directories and hyperparameter tuning trial id\n",
    "    based on env variables set by Vertex AI.\"\"\"\n",
    "    \n",
    "    model_dir = os.getenv('AIP_MODEL_DIR', LOCAL_MODEL_DIR)\n",
    "    tb_dir = os.getenv('AIP_TENSORBOARD_LOG_DIR', LOCAL_TB_DIR)\n",
    "    checkpoint_dir = os.getenv('AIP_CHECKPOINT_DIR', LOCAL_CHECKPOINT_DIR)\n",
    "    \n",
    "    path = os.path.normpath(tb_dir)\n",
    "    trial_id = re.match('^[0-9]+$', path.split(os.sep)[-2])\n",
    "    if not trial_id:\n",
    "        trial_id = '0'\n",
    "    else:\n",
    "        trial_id = trial_id[0]\n",
    "    logging.info(trial_id)\n",
    "    \n",
    "    return model_dir, tb_dir, checkpoint_dir, trial_id\n",
    "\n",
    "\n",
    "def get_bq_dataset(table_name, selected_fields, target_feature='tip_bin', batch_size=32):\n",
    "    \n",
    "    def _transform_row(row_dict):\n",
    "        trimmed_dict = {column:\n",
    "                       (tf.strings.strip(tensor) if tensor.dtype == 'string' else tensor) \n",
    "                       for (column,tensor) in row_dict.items()\n",
    "                       }\n",
    "        target = trimmed_dict.pop(target_feature)\n",
    "        return (trimmed_dict, target)\n",
    "\n",
    "    project_id, dataset_id, table_id = table_name.split('.')\n",
    "    \n",
    "    client = tfio_bq.BigQueryClient()\n",
    "    parent = f'projects/{project_id}'\n",
    "\n",
    "    read_session = client.read_session(\n",
    "        parent=parent,\n",
    "        project_id=project_id,\n",
    "        table_id=table_id,\n",
    "        dataset_id=dataset_id,\n",
    "        selected_fields=selected_fields,\n",
    "    )\n",
    "\n",
    "    dataset = read_session.parallel_read_rows().map(_transform_row).batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_category_encoding_layer(name, dataset, dtype):\n",
    "    \"\"\"Creates a CategoryEncoding layer for a given feature.\"\"\"\n",
    "\n",
    "    if dtype == tf.string:\n",
    "      index = preprocessing.StringLookup()\n",
    "    else:\n",
    "      index = preprocessing.IntegerLookup()\n",
    "\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    index.adapt(feature_ds)\n",
    "    encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
    "\n",
    "    return lambda feature: encoder(index(feature))\n",
    "\n",
    "\n",
    "def get_normalization_layer(name, dataset):\n",
    "  \"\"\"\"Creates a Normalization layer for a given feature.\"\"\"\n",
    "  normalizer = preprocessing.Normalization()\n",
    "\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "  normalizer.adapt(feature_ds)\n",
    "\n",
    "  return normalizer\n",
    "\n",
    "\n",
    "def create_model(dataset, input_features, units, dropout_ratio):\n",
    "    \"\"\"Creates a binary classifier for Chicago Taxi tip prediction task.\"\"\"\n",
    "    \n",
    "    all_inputs = []\n",
    "    encoded_features = []\n",
    "    for feature_name, feature_info in input_features.items():\n",
    "        col = tf.keras.Input(shape=(1,), name=feature_name, dtype=feature_info[1])\n",
    "        if feature_info[0] == 'categorical':\n",
    "            \n",
    "            encoding_layer = get_category_encoding_layer(feature_name, \n",
    "                                                         dataset,\n",
    "                                                         feature_info[1])\n",
    "        else:\n",
    "            encoding_layer = get_normalization_layer(feature_name,\n",
    "                                                     dataset) \n",
    "        encoded_col = encoding_layer(col)\n",
    "        all_inputs.append(col)\n",
    "        encoded_features.append(encoded_col)\n",
    "        \n",
    "    all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(units, activation=\"relu\")(all_features)\n",
    "    x = tf.keras.layers.Dropout(dropout_ratio)(x)\n",
    "    output = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.Model(all_inputs, output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "class HptuneCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    A custom Keras callback class that reports a metric to hypertuner\n",
    "    at the end of each epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, metric_tag, metric_value):\n",
    "        super(HptuneCallback, self).__init__()\n",
    "        self.metric_tag = metric_tag\n",
    "        self.metric_value = metric_value\n",
    "        self.hpt = hypertune.HyperTune()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.hpt.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag=self.metric_tag,\n",
    "            metric_value=logs[self.metric_value],\n",
    "            global_step=epoch)\n",
    "        \n",
    "\n",
    "def main(argv):\n",
    "    del argv\n",
    "    \n",
    "    # Set distribution strategy\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "    global_batch_size = (strategy.num_replicas_in_sync *\n",
    "                         FLAGS.per_replica_batch_size)\n",
    "    \n",
    "    # Prepare datasets\n",
    "    selected_fields = {key: {'output_type': value[1]} for key, value in FEATURES.items()}\n",
    "    validation_ds = get_bq_dataset(FLAGS.validation_table, \n",
    "                                   selected_fields, \n",
    "                                   batch_size=global_batch_size)\n",
    "    training_ds = get_bq_dataset(FLAGS.training_table,\n",
    "                                 selected_fields,\n",
    "                                 batch_size=global_batch_size)\n",
    "    \n",
    "    # Configure Tensorboard hparams\n",
    "    model_dir, tb_dir, checkpoint_dir, trial_id = set_job_dirs()\n",
    "    with tf.summary.create_file_writer(tb_dir).as_default():\n",
    "        tb_hp.hparams_config(hparams=HPARAMS, metrics=METRICS)\n",
    "        \n",
    "    hparams = {\n",
    "        HP_UNITS: FLAGS.units,\n",
    "        HP_DROPOUT: FLAGS.dropout_ratio\n",
    "    }\n",
    "    \n",
    "    # Create the model\n",
    "    input_features = {key: value for key, value in FEATURES.items() if key != TARGET_FEATURE_NAME}\n",
    "    logging.info('Creating the model ...')\n",
    "    with strategy.scope():\n",
    "        model = create_model(training_ds, input_features, hparams[HP_UNITS], hparams[HP_DROPOUT])\n",
    "        model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Configure training regimen\n",
    "    callbacks = [tf.keras.callbacks.experimental.BackupAndRestore(backup_dir=checkpoint_dir)]\n",
    "    callbacks.append(tf.keras.callbacks.TensorBoard(log_dir=tb_dir, \n",
    "                                                    update_freq='batch',\n",
    "                                                    profile_batch=0))\n",
    "    callbacks.append(tb_hp.KerasCallback(writer=tb_dir, \n",
    "                                         hparams=hparams,\n",
    "                                         trial_id=trial_id))\n",
    "    callbacks.append(HptuneCallback(HPTUNE_METRIC, HPTUNE_METRIC))\n",
    "    \n",
    "    # Start training\n",
    "    logging.info('Starting training ...')\n",
    "    model.fit(training_ds, \n",
    "              epochs=FLAGS.epochs, \n",
    "              validation_data=validation_ds,\n",
    "              callbacks=callbacks)\n",
    "    \n",
    "    # Save trained model\n",
    "    logging.info('Training completed. Saving the trained model to: {}'.format(model_dir))\n",
    "    model.save(model_dir)  \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "    app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1aaeeb7",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bce66ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4439758",
   "metadata": {},
   "source": [
    "### Configure and submit a custom Vertex job using a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ec3d011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.utils.source_utils:Training script copied to:\n",
      "gs://jk1-bucket/jobs/jk1_CUSTOM_SCRIPT/20210622_235057/aiplatform-2021-06-22-23:50:58.156-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
      "INFO:google.cloud.aiplatform.jobs:Creating CustomJob\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob created. Resource name: projects/910094146258/locations/us-central1/customJobs/4225330826566434816\n",
      "INFO:google.cloud.aiplatform.jobs:To use this CustomJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:custom_job = aiplatform.CustomJob.get('projects/910094146258/locations/us-central1/customJobs/4225330826566434816')\n",
      "INFO:google.cloud.aiplatform.jobs:View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/4225330826566434816?project=910094146258\n",
      "INFO:google.cloud.aiplatform.jobs:View Tensorboard:\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+910094146258+locations+us-central1+tensorboards+1647226748082847744+experiments+4225330826566434816\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/4225330826566434816 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "job_name = f'{PREFIX}_CUSTOM_SCRIPT'\n",
    "base_output_dir = '{}/jobs/{}/{}'.format(STAGING_BUCKET, job_name, time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "#container_uri = 'us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-4:latest'\n",
    "container_uri = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-4:latest'\n",
    "requirements = ['tensorflow-datasets==4.3.0']\n",
    "args = [\n",
    "    '--epochs=2', \n",
    "    '--per_replica_batch_size=128',\n",
    "    '--training_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "    '--validation_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "]\n",
    "\n",
    "machine_type = 'n1-standard-4'\n",
    "#accelerator_type = 'NVIDIA_TESLA_T4'\n",
    "#accelerator_count = 1\n",
    "\n",
    "job = vertex_ai.CustomJob.from_local_script(\n",
    "    display_name=job_name,\n",
    "    machine_type=machine_type,\n",
    "#    accelerator_type=accelerator_type,\n",
    "#    accelerator_count=accelerator_count,\n",
    "    script_path='trainer/train.py',\n",
    "    container_uri=container_uri,\n",
    "    requirements=requirements,\n",
    "    args=args,\n",
    "    staging_bucket=base_output_dir\n",
    ")\n",
    "\n",
    "job.run(sync=False, \n",
    "        service_account=VERTEX_SA,\n",
    "        tensorboard=TENSORBOARD,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da6a241",
   "metadata": {},
   "source": [
    "### Configure and submit a Vertex job using a custom container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636807f2",
   "metadata": {},
   "source": [
    "#### Create a docker file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04ec035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = 'gcr.io/deeplearning-platform-release/tf2-cpu.2-4'\n",
    "TRAIN_IMAGE = f'gcr.io/{PROJECT}/{PREFIX}_taxi_trainer'\n",
    "\n",
    "dockerfile = f'''\n",
    "FROM {BASE_IMAGE}\n",
    "\n",
    "WORKDIR /trainer\n",
    "RUN pip install cloudml-hypertune\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n",
    "'''\n",
    "\n",
    "with open(os.path.join(SCRIPT_FOLDER, 'Dockerfile'), 'w') as f:\n",
    "    f.write(dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38f1fa6",
   "metadata": {},
   "source": [
    "#### Build a container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ea3a79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 9.0 KiB before compression.\n",
      "Uploading tarball of [trainer] to [gs://jk-vertex-workshop_cloudbuild/source/1624405871.596184-f548804c5cf947478bc6f00aba038711.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/jk-vertex-workshop/locations/global/builds/e5bc2a38-238d-45dc-9164-ead6c44f5f1e].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/e5bc2a38-238d-45dc-9164-ead6c44f5f1e?project=910094146258].\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/4225330826566434816 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"e5bc2a38-238d-45dc-9164-ead6c44f5f1e\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://jk-vertex-workshop_cloudbuild/source/1624405871.596184-f548804c5cf947478bc6f00aba038711.tgz#1624405872227487\n",
      "Copying gs://jk-vertex-workshop_cloudbuild/source/1624405871.596184-f548804c5cf947478bc6f00aba038711.tgz#1624405872227487...\n",
      "/ [1 files][  3.4 KiB/  3.4 KiB]                                                \n",
      "Operation completed over 1 objects/3.4 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  11.78kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-4\n",
      "latest: Pulling from deeplearning-platform-release/tf2-cpu.2-4\n",
      "01bf7da0a88c: Pulling fs layer\n",
      "f3b4a5f15c7a: Pulling fs layer\n",
      "57ffbe87baa1: Pulling fs layer\n",
      "424e7c9d5d89: Pulling fs layer\n",
      "9b397537aef0: Pulling fs layer\n",
      "2bd5028f4b85: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "b2ed56b85d3a: Pulling fs layer\n",
      "8bfb788e9874: Pulling fs layer\n",
      "0618fb353339: Pulling fs layer\n",
      "42045a665612: Pulling fs layer\n",
      "031d8d7b75f7: Pulling fs layer\n",
      "5780cc9addac: Pulling fs layer\n",
      "8fbe78107b3d: Pulling fs layer\n",
      "eee173fc570a: Pulling fs layer\n",
      "9334ecc802d5: Pulling fs layer\n",
      "c631c38965fd: Pulling fs layer\n",
      "803ecb627365: Pulling fs layer\n",
      "4466b5c2ac37: Pulling fs layer\n",
      "16a6777d4439: Pulling fs layer\n",
      "7a84944cecd7: Pulling fs layer\n",
      "031d8d7b75f7: Waiting\n",
      "5780cc9addac: Waiting\n",
      "8fbe78107b3d: Waiting\n",
      "eee173fc570a: Waiting\n",
      "9334ecc802d5: Waiting\n",
      "c631c38965fd: Waiting\n",
      "803ecb627365: Waiting\n",
      "4466b5c2ac37: Waiting\n",
      "16a6777d4439: Waiting\n",
      "424e7c9d5d89: Waiting\n",
      "7a84944cecd7: Waiting\n",
      "9b397537aef0: Waiting\n",
      "2bd5028f4b85: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "b2ed56b85d3a: Waiting\n",
      "8bfb788e9874: Waiting\n",
      "0618fb353339: Waiting\n",
      "42045a665612: Waiting\n",
      "f3b4a5f15c7a: Verifying Checksum\n",
      "f3b4a5f15c7a: Download complete\n",
      "57ffbe87baa1: Verifying Checksum\n",
      "57ffbe87baa1: Download complete\n",
      "424e7c9d5d89: Verifying Checksum\n",
      "424e7c9d5d89: Download complete\n",
      "01bf7da0a88c: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "b2ed56b85d3a: Verifying Checksum\n",
      "b2ed56b85d3a: Download complete\n",
      "2bd5028f4b85: Verifying Checksum\n",
      "2bd5028f4b85: Download complete\n",
      "0618fb353339: Verifying Checksum\n",
      "0618fb353339: Download complete\n",
      "42045a665612: Verifying Checksum\n",
      "42045a665612: Download complete\n",
      "031d8d7b75f7: Verifying Checksum\n",
      "031d8d7b75f7: Download complete\n",
      "5780cc9addac: Download complete\n",
      "8fbe78107b3d: Verifying Checksum\n",
      "8fbe78107b3d: Download complete\n",
      "eee173fc570a: Download complete\n",
      "9334ecc802d5: Download complete\n",
      "c631c38965fd: Download complete\n",
      "8bfb788e9874: Verifying Checksum\n",
      "8bfb788e9874: Download complete\n",
      "4466b5c2ac37: Download complete\n",
      "9b397537aef0: Verifying Checksum\n",
      "9b397537aef0: Download complete\n",
      "7a84944cecd7: Verifying Checksum\n",
      "7a84944cecd7: Download complete\n",
      "16a6777d4439: Verifying Checksum\n",
      "16a6777d4439: Download complete\n",
      "01bf7da0a88c: Pull complete\n",
      "f3b4a5f15c7a: Pull complete\n",
      "57ffbe87baa1: Pull complete\n",
      "424e7c9d5d89: Pull complete\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/4225330826566434816 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "803ecb627365: Verifying Checksum\n",
      "803ecb627365: Download complete\n",
      "9b397537aef0: Pull complete\n",
      "2bd5028f4b85: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "b2ed56b85d3a: Pull complete\n",
      "8bfb788e9874: Pull complete\n",
      "0618fb353339: Pull complete\n",
      "42045a665612: Pull complete\n",
      "031d8d7b75f7: Pull complete\n",
      "5780cc9addac: Pull complete\n",
      "8fbe78107b3d: Pull complete\n",
      "eee173fc570a: Pull complete\n",
      "9334ecc802d5: Pull complete\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/4225330826566434816 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "c631c38965fd: Pull complete\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/4225330826566434816 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "803ecb627365: Pull complete\n",
      "4466b5c2ac37: Pull complete\n",
      "16a6777d4439: Pull complete\n",
      "7a84944cecd7: Pull complete\n",
      "Digest: sha256:56489bdb00b3f5c342d8cbcdeea68fd63b982858199c4ed7edac15614954b110\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/tf2-cpu.2-4:latest\n",
      " ---> fbf8f454dc05\n",
      "Step 2/5 : WORKDIR /trainer\n",
      " ---> Running in 59c3bbe2b192\n",
      "Removing intermediate container 59c3bbe2b192\n",
      " ---> 554a1d7839cf\n",
      "Step 3/5 : RUN pip install cloudml-hypertune\n",
      " ---> Running in 4fd6798cbf72\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Building wheels for collected packages: cloudml-hypertune\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3988 sha256=607591cf1d482ef86f37d860b5514734641143e8b3696a1fae66e88e078fd7ff\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "Successfully built cloudml-hypertune\n",
      "Installing collected packages: cloudml-hypertune\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6\n",
      "\u001b[91mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 4fd6798cbf72\n",
      " ---> 7df97475111d\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 583142566e33\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 42080ead1195\n",
      "Removing intermediate container 42080ead1195\n",
      " ---> f474b96f6ec3\n",
      "Successfully built f474b96f6ec3\n",
      "Successfully tagged gcr.io/jk-vertex-workshop/jk1_taxi_trainer:latest\n",
      "PUSH\n",
      "Pushing gcr.io/jk-vertex-workshop/jk1_taxi_trainer\n",
      "The push refers to repository [gcr.io/jk-vertex-workshop/jk1_taxi_trainer]\n",
      "5503c3e9e3c6: Preparing\n",
      "f53df32421b1: Preparing\n",
      "76a052fe32aa: Preparing\n",
      "d39fae2e5e4e: Preparing\n",
      "33302bd81efa: Preparing\n",
      "0c5f899e9bc7: Preparing\n",
      "5e593cdd4a12: Preparing\n",
      "06a5bf49b163: Preparing\n",
      "b34dae69fc5d: Preparing\n",
      "0ffb7465dde9: Preparing\n",
      "e2563d1ada9a: Preparing\n",
      "42b027d1e826: Preparing\n",
      "636a7c2e7d03: Preparing\n",
      "1ba1158adf89: Preparing\n",
      "96e46d1341e8: Preparing\n",
      "954f6dc3f7f5: Preparing\n",
      "8760a171b659: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "a0710233fd2d: Preparing\n",
      "05449afa4be9: Preparing\n",
      "5b9e34b5cf74: Preparing\n",
      "8cafc6d2db45: Preparing\n",
      "a5d4bacb0351: Preparing\n",
      "5153e1acaabc: Preparing\n",
      "0c5f899e9bc7: Waiting\n",
      "5e593cdd4a12: Waiting\n",
      "06a5bf49b163: Waiting\n",
      "b34dae69fc5d: Waiting\n",
      "e2563d1ada9a: Waiting\n",
      "42b027d1e826: Waiting\n",
      "636a7c2e7d03: Waiting\n",
      "1ba1158adf89: Waiting\n",
      "96e46d1341e8: Waiting\n",
      "954f6dc3f7f5: Waiting\n",
      "8760a171b659: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "a0710233fd2d: Waiting\n",
      "05449afa4be9: Waiting\n",
      "5b9e34b5cf74: Waiting\n",
      "8cafc6d2db45: Waiting\n",
      "a5d4bacb0351: Waiting\n",
      "5153e1acaabc: Waiting\n",
      "0ffb7465dde9: Waiting\n",
      "d39fae2e5e4e: Layer already exists\n",
      "33302bd81efa: Layer already exists\n",
      "0c5f899e9bc7: Layer already exists\n",
      "5e593cdd4a12: Layer already exists\n",
      "06a5bf49b163: Layer already exists\n",
      "b34dae69fc5d: Layer already exists\n",
      "0ffb7465dde9: Layer already exists\n",
      "e2563d1ada9a: Layer already exists\n",
      "636a7c2e7d03: Layer already exists\n",
      "42b027d1e826: Layer already exists\n",
      "1ba1158adf89: Layer already exists\n",
      "96e46d1341e8: Layer already exists\n",
      "954f6dc3f7f5: Layer already exists\n",
      "8760a171b659: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "a0710233fd2d: Layer already exists\n",
      "05449afa4be9: Layer already exists\n",
      "5b9e34b5cf74: Layer already exists\n",
      "8cafc6d2db45: Layer already exists\n",
      "a5d4bacb0351: Layer already exists\n",
      "5153e1acaabc: Layer already exists\n",
      "76a052fe32aa: Pushed\n",
      "f53df32421b1: Pushed\n",
      "5503c3e9e3c6: Pushed\n",
      "latest: digest: sha256:61e27b52180af6cb32131e9ee5656deb24b363a5f2e97a3f3d7885bce1bb9e90 size: 5335\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                            IMAGES                                                STATUS\n",
      "e5bc2a38-238d-45dc-9164-ead6c44f5f1e  2021-06-22T23:51:12+00:00  2M47S     gs://jk-vertex-workshop_cloudbuild/source/1624405871.596184-f548804c5cf947478bc6f00aba038711.tgz  gcr.io/jk-vertex-workshop/jk1_taxi_trainer (+1 more)  SUCCESS\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/4225330826566434816 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag {TRAIN_IMAGE} {SCRIPT_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62f55ff",
   "metadata": {},
   "source": [
    "#### Prepare worker pool specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57eff29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "#            \"accelerator_type\": \"NVIDIA_TESLA_T4\",\n",
    "#            \"accelerator_count\": 1,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE,\n",
    "#            \"command\": [\"python\", \"train.py\"],\n",
    "            \"args\": [\n",
    "                '--epochs=2', \n",
    "                '--per_replica_batch_size=128',\n",
    "                '--training_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "                '--validation_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_VALID_SPLIT_NAME}',\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4650731",
   "metadata": {},
   "source": [
    "#### Submit and monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65478e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating CustomJob\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob created. Resource name: projects/910094146258/locations/us-central1/customJobs/1111654634193158144\n",
      "INFO:google.cloud.aiplatform.jobs:To use this CustomJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:custom_job = aiplatform.CustomJob.get('projects/910094146258/locations/us-central1/customJobs/1111654634193158144')\n",
      "INFO:google.cloud.aiplatform.jobs:View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/1111654634193158144?project=910094146258\n",
      "INFO:google.cloud.aiplatform.jobs:View Tensorboard:\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+910094146258+locations+us-central1+tensorboards+1647226748082847744+experiments+1111654634193158144\n"
     ]
    }
   ],
   "source": [
    "job_name = f'{PREFIX}_CUSTOM_CONTAINER'\n",
    "base_output_dir = '{}/jobs/{}/{}'.format(STAGING_BUCKET, job_name, time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "job = vertex_ai.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=base_output_dir\n",
    ")\n",
    "\n",
    "job.run(sync=False, \n",
    "        service_account=VERTEX_SA,\n",
    "        tensorboard=TENSORBOARD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7e198a",
   "metadata": {},
   "source": [
    "### Configure and submit a hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5513e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f'{PREFIX}_HYPERPARAMETER'\n",
    "base_output_dir = '{}/jobs/{}/{}'.format(STAGING_BUCKET, job_name, time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "hyperparameter_tuning_job_spec = {\n",
    "        \"display_name\": job_name,\n",
    "        \"max_trial_count\": 3,\n",
    "        \"parallel_trial_count\": 3,\n",
    "        \"max_failed_trial_count\": 1,\n",
    "        \"study_spec\": {\n",
    "            \"metrics\": [\n",
    "                {\n",
    "                    \"metric_id\": \"val_accuracy\",\n",
    "                    \"goal\": vertex_ai.gapic.StudySpec.MetricSpec.GoalType.MAXIMIZE,\n",
    "                }\n",
    "            ],\n",
    "            \"parameters\": [\n",
    "                {\n",
    "                    \"parameter_id\": \"units\",\n",
    "                    \"discrete_value_spec\": {\"values\": [32, 64]},\n",
    "                },\n",
    "                {\n",
    "                    \"parameter_id\": \"dropout_ratio\",\n",
    "                    \"double_value_spec\": {\"min_value\": 0.4, \"max_value\": 0.6},\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        \"trial_job_spec\": {\n",
    "            \"base_output_directory\": {\n",
    "                \"output_uri_prefix\": base_output_dir\n",
    "            },\n",
    "            \"service_account\": VERTEX_SA,\n",
    "            \"tensorboard\": TENSORBOARD,\n",
    "            \"worker_pool_specs\": [\n",
    "                {\n",
    "                    \"machine_spec\": {\n",
    "                        \"machine_type\": \"n1-standard-4\",\n",
    "                      #  \"accelerator_type\": vertex_ai.gapic.AcceleratorType.NVIDIA_TESLA_T4,\n",
    "                      #  \"accelerator_count\": 1,\n",
    "                    },\n",
    "                    \"replica_count\": 1,\n",
    "                    \"container_spec\": {\n",
    "                        \"image_uri\": TRAIN_IMAGE,\n",
    "                        \"args\": [\n",
    "                            '--epochs=5', \n",
    "                            '--per_replica_batch_size=128',\n",
    "                            '--training_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "                            '--validation_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_VALID_SPLIT_NAME}',\n",
    "                        ],\n",
    "                    },\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9547dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_client = vertex_ai.initializer.global_config.create_client(\n",
    "        client_class=JobClientWithOverride, location_override=REGION\n",
    ")\n",
    "\n",
    "parent = f'projects/{PROJECT}/locations/{REGION}'\n",
    "version = 'v1beta1'\n",
    "\n",
    "response = job_client.select_version(version).create_hyperparameter_tuning_job(\n",
    "    parent=parent, hyperparameter_tuning_job=hyperparameter_tuning_job_spec\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d10ba26",
   "metadata": {},
   "source": [
    "#### Monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6d48454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobState.JOB_STATE_QUEUED\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/1111654634193158144 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "job_spec = job_client.get_hyperparameter_tuning_job(\n",
    "    name = response.name\n",
    ")\n",
    "print(job_spec.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6083f240",
   "metadata": {},
   "source": [
    "#### Wait for tuning to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf29fd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/1111654634193158144 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/1111654634193158144 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/910094146258/locations/us-central1/customJobs/1111654634193158144 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    job_response = job_client.get_hyperparameter_tuning_job(\n",
    "        name = response.name)\n",
    "    if job_response.state != vertex_ai.gapic.JobState.JOB_STATE_SUCCEEDED:\n",
    "        print(\"Study trials have not completed:\", job_response.state)\n",
    "        if (job_response.state == vertex_ai.gapic.JobState.JOB_STATE_FAILED or \n",
    "           job_response.state == vertex_ai.gapic.JobState.JOB_STATE_CANCELLED):\n",
    "            break\n",
    "    else:\n",
    "        print(\"Study trials have completed\")\n",
    "        break\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be122831",
   "metadata": {},
   "source": [
    "#### Review the results of the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a5e9bd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: \"2\"\n",
      "state: SUCCEEDED\n",
      "parameters {\n",
      "  parameter_id: \"dropout_ratio\"\n",
      "  value {\n",
      "    number_value: 0.5437410579977221\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameter_id: \"units\"\n",
      "  value {\n",
      "    number_value: 32.0\n",
      "  }\n",
      "}\n",
      "final_measurement {\n",
      "  step_count: 4\n",
      "  metrics {\n",
      "    metric_id: \"val_accuracy\"\n",
      "    value: 0.8744137287139893\n",
      "  }\n",
      "}\n",
      "start_time {\n",
      "  seconds: 1624331578\n",
      "  nanos: 786919773\n",
      "}\n",
      "end_time {\n",
      "  seconds: 1624333927\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0.0\n",
    "best_trial = None\n",
    "for trial in job_response.trials:\n",
    "    accuracy = float(trial.final_measurement.metrics[0].value)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_trial = trial\n",
    "        best_accuracy = accuracy\n",
    "print(best_trial)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac188a4",
   "metadata": {},
   "source": [
    "#### Get the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1505034e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://jk1-bucket/jobs/jk1_HYPERPARAMETER/20210622_031249/\n",
      "gs://jk1-bucket/jobs/jk1_HYPERPARAMETER/20210622_031249/1/\n",
      "gs://jk1-bucket/jobs/jk1_HYPERPARAMETER/20210622_031249/2/\n",
      "gs://jk1-bucket/jobs/jk1_HYPERPARAMETER/20210622_031249/3/\n"
     ]
    }
   ],
   "source": [
    "model_dir = job_response.trial_job_spec.base_output_directory.output_uri_prefix\n",
    "\n",
    "!gsutil ls {model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1968d43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://jk1-bucket/jobs/jk1_HYPERPARAMETER/20210622_031249/2/\n",
      "gs://jk1-bucket/jobs/jk1_HYPERPARAMETER/20210622_031249/2/checkpoints/\n",
      "gs://jk1-bucket/jobs/jk1_HYPERPARAMETER/20210622_031249/2/logs/\n",
      "gs://jk1-bucket/jobs/jk1_HYPERPARAMETER/20210622_031249/2/model/\n"
     ]
    }
   ],
   "source": [
    "best_model_dir = '{}/{}'.format(model_dir, best_trial.id)\n",
    "\n",
    "!gsutil ls {best_model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fa8a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m73",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m73"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
