{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5415cb65",
   "metadata": {},
   "source": [
    "# Training and deploying a tabular model using Vertex custom training job - Part 1\n",
    "\n",
    "![Training pipeline](../images/custom-tabular.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a305a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from google.cloud.aiplatform_v1beta1 import types\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import exceptions\n",
    "\n",
    "from google.cloud.aiplatform.utils import JobClientWithOverride\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from tensorflow_io import bigquery as tfio_bq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f643c4e2",
   "metadata": {},
   "source": [
    "## Configure GCP settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50a779d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  jk-wst1\n"
     ]
    }
   ],
   "source": [
    "PREFIX = 'jkwst1'\n",
    "\n",
    "shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "PROJECT = shell_output[0]\n",
    "print(\"Project ID: \", PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39c42c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORBOARD = 'projects/630263135640/locations/us-central1/tensorboards/5276565097790046208'\n",
    "\n",
    "REGION = 'us-central1'\n",
    "BQ_DATASET_NAME = f'{PREFIX}_dataset' \n",
    "BQ_TRAIN_SPLIT_NAME = 'training'\n",
    "BQ_VALID_SPLIT_NAME = 'validation'\n",
    "BQ_TEST_SPLIT_NAME = 'testing'\n",
    "STAGING_BUCKET = f'gs://{PREFIX}-bucket'\n",
    "VERTEX_SA = f'training-sa@{PROJECT}.iam.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b0550a",
   "metadata": {},
   "source": [
    "## Submitting Vertex training jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2397fba2",
   "metadata": {},
   "source": [
    "### Display the model\n",
    "\n",
    "`tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")`\n",
    "\n",
    "![Model](model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74f163a",
   "metadata": {},
   "source": [
    "### Prepare a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ca041ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_FOLDER = 'trainer'\n",
    "if tf.io.gfile.exists(SCRIPT_FOLDER):\n",
    "    tf.io.gfile.rmtree(SCRIPT_FOLDER)\n",
    "tf.io.gfile.mkdir(SCRIPT_FOLDER)\n",
    "file_path = os.path.join(SCRIPT_FOLDER, 'train.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fae9fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {file_path}\n",
    "\n",
    "\n",
    "# Copyright 2021 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "\n",
    "import hypertune\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow_io import bigquery as tfio_bq\n",
    "\n",
    "from tensorboard.plugins.hparams import api as tb_hp\n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('epochs', 3, 'Nubmer of epochs')\n",
    "flags.DEFINE_integer('units', 32, 'Number units in a hidden layer')\n",
    "flags.DEFINE_integer('per_replica_batch_size', 128, 'Per replica batch size')\n",
    "flags.DEFINE_float('dropout_ratio', 0.5, 'Dropout ratio')\n",
    "flags.DEFINE_string('training_table', None, 'Training table name')\n",
    "flags.DEFINE_string('validation_table', None, 'Validationa table name')\n",
    "flags.mark_flag_as_required('training_table')\n",
    "flags.mark_flag_as_required('validation_table')\n",
    "\n",
    "LOCAL_MODEL_DIR = '/tmp/saved_model'\n",
    "LOCAL_TB_DIR = '/tmp/logs'\n",
    "LOCAL_CHECKPOINT_DIR = '/tmp/checkpoints'\n",
    "EVALUATION_FILE_NAME = 'evaluations.json'\n",
    "\n",
    "# Define features\n",
    "FEATURES = {\n",
    "    \"tip_bin\": (\"categorical\", tf.int64),\n",
    "    \"trip_month\": (\"categorical\", tf.int64),\n",
    "    \"trip_day\": (\"categorical\", tf.int64),\n",
    "    \"trip_day_of_week\": (\"categorical\", tf.int64),\n",
    "    \"trip_hour\": (\"categorical\", tf.int64),\n",
    "    \"payment_type\": (\"categorical\", tf.string),\n",
    "    \"pickup_grid\": (\"categorical\", tf.string),\n",
    "    \"dropoff_grid\": (\"categorical\", tf.string),\n",
    "    \"euclidean\": (\"numeric\", tf.double),\n",
    "    \"trip_seconds\": (\"numeric\", tf.int64),\n",
    "    \"trip_miles\": (\"numeric\", tf.double),\n",
    "}\n",
    "TARGET_FEATURE_NAME = 'tip_bin'\n",
    "\n",
    " # Set hparams for Tensorboard and Vertex hp tuner\n",
    "HP_DROPOUT = tb_hp.HParam(\"dropout\")\n",
    "HP_UNITS = tb_hp.HParam(\"units\")\n",
    "HPARAMS = [\n",
    "    HP_UNITS,\n",
    "    HP_DROPOUT,\n",
    "]\n",
    "METRICS = [\n",
    "    tb_hp.Metric(\n",
    "        \"epoch_accuracy\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"epoch accuracy\"),\n",
    "]\n",
    "HPTUNE_METRIC = 'val_accuracy'\n",
    "    \n",
    "\n",
    "def set_job_dirs():\n",
    "    \"\"\"Sets job directories and hyperparameter tuning trial id\n",
    "    based on env variables set by Vertex AI.\"\"\"\n",
    "    \n",
    "    model_dir = os.getenv('AIP_MODEL_DIR', LOCAL_MODEL_DIR)\n",
    "    tb_dir = os.getenv('AIP_TENSORBOARD_LOG_DIR', LOCAL_TB_DIR)\n",
    "    checkpoint_dir = os.getenv('AIP_CHECKPOINT_DIR', LOCAL_CHECKPOINT_DIR)\n",
    "    \n",
    "    path = os.path.normpath(tb_dir)\n",
    "    trial_id = re.match('^[0-9]+$', path.split(os.sep)[-2])\n",
    "    if not trial_id:\n",
    "        trial_id = '0'\n",
    "    else:\n",
    "        trial_id = trial_id[0]\n",
    "    logging.info(trial_id)\n",
    "    \n",
    "    return model_dir, tb_dir, checkpoint_dir, trial_id\n",
    "\n",
    "\n",
    "def get_bq_dataset(table_name, selected_fields, target_feature='tip_bin', batch_size=32):\n",
    "    \n",
    "    def _transform_row(row_dict):\n",
    "        trimmed_dict = {column:\n",
    "                       (tf.strings.strip(tensor) if tensor.dtype == 'string' else tensor) \n",
    "                       for (column,tensor) in row_dict.items()\n",
    "                       }\n",
    "        target = trimmed_dict.pop(target_feature)\n",
    "        return (trimmed_dict, target)\n",
    "\n",
    "    project_id, dataset_id, table_id = table_name.split('.')\n",
    "    \n",
    "    client = tfio_bq.BigQueryClient()\n",
    "    parent = f'projects/{project_id}'\n",
    "\n",
    "    read_session = client.read_session(\n",
    "        parent=parent,\n",
    "        project_id=project_id,\n",
    "        table_id=table_id,\n",
    "        dataset_id=dataset_id,\n",
    "        selected_fields=selected_fields,\n",
    "    )\n",
    "\n",
    "    dataset = read_session.parallel_read_rows().map(_transform_row).batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_category_encoding_layer(name, dataset, dtype):\n",
    "    \"\"\"Creates a CategoryEncoding layer for a given feature.\"\"\"\n",
    "\n",
    "    if dtype == tf.string:\n",
    "      index = preprocessing.StringLookup()\n",
    "    else:\n",
    "      index = preprocessing.IntegerLookup()\n",
    "\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    index.adapt(feature_ds)\n",
    "    encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
    "\n",
    "    return lambda feature: encoder(index(feature))\n",
    "\n",
    "\n",
    "def get_normalization_layer(name, dataset):\n",
    "  \"\"\"\"Creates a Normalization layer for a given feature.\"\"\"\n",
    "  normalizer = preprocessing.Normalization()\n",
    "\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "  normalizer.adapt(feature_ds)\n",
    "\n",
    "  return normalizer\n",
    "\n",
    "\n",
    "def create_model(dataset, input_features, units, dropout_ratio):\n",
    "    \"\"\"Creates a binary classifier for Chicago Taxi tip prediction task.\"\"\"\n",
    "    \n",
    "    all_inputs = []\n",
    "    encoded_features = []\n",
    "    for feature_name, feature_info in input_features.items():\n",
    "        col = tf.keras.Input(shape=(1,), name=feature_name, dtype=feature_info[1])\n",
    "        if feature_info[0] == 'categorical':\n",
    "            \n",
    "            encoding_layer = get_category_encoding_layer(feature_name, \n",
    "                                                         dataset,\n",
    "                                                         feature_info[1])\n",
    "        else:\n",
    "            encoding_layer = get_normalization_layer(feature_name,\n",
    "                                                     dataset) \n",
    "        encoded_col = encoding_layer(col)\n",
    "        all_inputs.append(col)\n",
    "        encoded_features.append(encoded_col)\n",
    "        \n",
    "    all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(units, activation=\"relu\")(all_features)\n",
    "    x = tf.keras.layers.Dropout(dropout_ratio)(x)\n",
    "    output = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.Model(all_inputs, output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "class HptuneCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    A custom Keras callback class that reports a metric to hypertuner\n",
    "    at the end of each epoch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, metric_tag, metric_value):\n",
    "        super(HptuneCallback, self).__init__()\n",
    "        self.metric_tag = metric_tag\n",
    "        self.metric_value = metric_value\n",
    "        self.hpt = hypertune.HyperTune()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.hpt.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag=self.metric_tag,\n",
    "            metric_value=logs[self.metric_value],\n",
    "            global_step=epoch)\n",
    "        \n",
    "\n",
    "def main(argv):\n",
    "    del argv\n",
    "    \n",
    "    # Set distribution strategy\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "    global_batch_size = (strategy.num_replicas_in_sync *\n",
    "                         FLAGS.per_replica_batch_size)\n",
    "    \n",
    "    # Prepare datasets\n",
    "    selected_fields = {key: {'output_type': value[1]} for key, value in FEATURES.items()}\n",
    "    validation_ds = get_bq_dataset(FLAGS.validation_table, \n",
    "                                   selected_fields, \n",
    "                                   batch_size=global_batch_size)\n",
    "    training_ds = get_bq_dataset(FLAGS.training_table,\n",
    "                                 selected_fields,\n",
    "                                 batch_size=global_batch_size)\n",
    "    \n",
    "    # Configure Tensorboard hparams\n",
    "    model_dir, tb_dir, checkpoint_dir, trial_id = set_job_dirs()\n",
    "    with tf.summary.create_file_writer(tb_dir).as_default():\n",
    "        tb_hp.hparams_config(hparams=HPARAMS, metrics=METRICS)\n",
    "        \n",
    "    hparams = {\n",
    "        HP_UNITS: FLAGS.units,\n",
    "        HP_DROPOUT: FLAGS.dropout_ratio\n",
    "    }\n",
    "    \n",
    "    # Create the model\n",
    "    input_features = {key: value for key, value in FEATURES.items() if key != TARGET_FEATURE_NAME}\n",
    "    logging.info('Creating the model ...')\n",
    "    with strategy.scope():\n",
    "        model = create_model(training_ds, input_features, hparams[HP_UNITS], hparams[HP_DROPOUT])\n",
    "        model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Configure training regimen\n",
    "    callbacks = [tf.keras.callbacks.experimental.BackupAndRestore(backup_dir=checkpoint_dir)]\n",
    "    callbacks.append(tf.keras.callbacks.TensorBoard(log_dir=tb_dir, \n",
    "                                                    update_freq='batch',\n",
    "                                                    profile_batch=0))\n",
    "    callbacks.append(tb_hp.KerasCallback(writer=tb_dir, \n",
    "                                         hparams=hparams,\n",
    "                                         trial_id=trial_id))\n",
    "    callbacks.append(HptuneCallback(HPTUNE_METRIC, HPTUNE_METRIC))\n",
    "    \n",
    "    # Start training\n",
    "    logging.info('Starting training ...')\n",
    "    model.fit(training_ds, \n",
    "              epochs=FLAGS.epochs, \n",
    "              validation_data=validation_ds,\n",
    "              callbacks=callbacks)\n",
    "    \n",
    "    # Save trained model\n",
    "    logging.info('Training completed. Saving the trained model to: {}'.format(model_dir))\n",
    "    model.save(model_dir)  \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "    app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0636de01",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22f2e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccab3b5",
   "metadata": {},
   "source": [
    "### Configure and submit a custom Vertex job using a script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "689699da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.utils.source_utils:Training script copied to:\n",
      "gs://jkwst1-bucket/jobs/jkwst1_CUSTOM_SCRIPT/20210628_171831/aiplatform-2021-06-28-17:18:31.372-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
      "INFO:google.cloud.aiplatform.jobs:Creating CustomJob\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob created. Resource name: projects/630263135640/locations/us-central1/customJobs/4758497209014550528\n",
      "INFO:google.cloud.aiplatform.jobs:To use this CustomJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:custom_job = aiplatform.CustomJob.get('projects/630263135640/locations/us-central1/customJobs/4758497209014550528')\n",
      "INFO:google.cloud.aiplatform.jobs:View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/4758497209014550528?project=630263135640\n",
      "INFO:google.cloud.aiplatform.jobs:View Tensorboard:\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+630263135640+locations+us-central1+tensorboards+5276565097790046208+experiments+4758497209014550528\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/4758497209014550528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/4758497209014550528 current state:\n",
      "JobState.JOB_STATE_QUEUED\n"
     ]
    }
   ],
   "source": [
    "job_name = f'{PREFIX}_CUSTOM_SCRIPT'\n",
    "base_output_dir = '{}/jobs/{}/{}'.format(STAGING_BUCKET, job_name, time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "#container_uri = 'us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-4:latest'\n",
    "container_uri = 'us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-4:latest'\n",
    "requirements = ['tensorflow-datasets==4.3.0']\n",
    "args = [\n",
    "    '--epochs=2', \n",
    "    '--per_replica_batch_size=128',\n",
    "    '--training_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "    '--validation_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "]\n",
    "\n",
    "machine_type = 'n1-standard-4'\n",
    "#accelerator_type = 'NVIDIA_TESLA_T4'\n",
    "#accelerator_count = 1\n",
    "\n",
    "job = vertex_ai.CustomJob.from_local_script(\n",
    "    display_name=job_name,\n",
    "    machine_type=machine_type,\n",
    "#    accelerator_type=accelerator_type,\n",
    "#    accelerator_count=accelerator_count,\n",
    "    script_path='trainer/train.py',\n",
    "    container_uri=container_uri,\n",
    "    requirements=requirements,\n",
    "    args=args,\n",
    "    staging_bucket=base_output_dir\n",
    ")\n",
    "\n",
    "job.run(sync=False, \n",
    "        service_account=VERTEX_SA,\n",
    "        tensorboard=TENSORBOARD,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1261d7e9",
   "metadata": {},
   "source": [
    "### Configure and submit a Vertex job using a custom container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adee5d7",
   "metadata": {},
   "source": [
    "#### Create a docker file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79383491",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = 'gcr.io/deeplearning-platform-release/tf2-cpu.2-4'\n",
    "TRAIN_IMAGE = f'gcr.io/{PROJECT}/{PREFIX}_taxi_trainer'\n",
    "\n",
    "dockerfile = f'''\n",
    "FROM {BASE_IMAGE}\n",
    "\n",
    "WORKDIR /trainer\n",
    "RUN pip install cloudml-hypertune\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n",
    "'''\n",
    "\n",
    "with open(os.path.join(SCRIPT_FOLDER, 'Dockerfile'), 'w') as f:\n",
    "    f.write(dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bb161c",
   "metadata": {},
   "source": [
    "#### Build a container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ad55ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 9.0 KiB before compression.\n",
      "Uploading tarball of [trainer] to [gs://jk-wst1_cloudbuild/source/1624900738.281183-e25ad1a5405041cea901455a44f7268b.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/jk-wst1/locations/global/builds/74e39738-dc48-4dc0-a253-a5df8afc2200].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/74e39738-dc48-4dc0-a253-a5df8afc2200?project=630263135640].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"74e39738-dc48-4dc0-a253-a5df8afc2200\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://jk-wst1_cloudbuild/source/1624900738.281183-e25ad1a5405041cea901455a44f7268b.tgz#1624900739030774\n",
      "Copying gs://jk-wst1_cloudbuild/source/1624900738.281183-e25ad1a5405041cea901455a44f7268b.tgz#1624900739030774...\n",
      "/ [1 files][  3.4 KiB/  3.4 KiB]                                                \n",
      "Operation completed over 1 objects/3.4 KiB.\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/4758497209014550528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  11.78kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-4\n",
      "latest: Pulling from deeplearning-platform-release/tf2-cpu.2-4\n",
      "01bf7da0a88c: Pulling fs layer\n",
      "f3b4a5f15c7a: Pulling fs layer\n",
      "57ffbe87baa1: Pulling fs layer\n",
      "424e7c9d5d89: Pulling fs layer\n",
      "9b397537aef0: Pulling fs layer\n",
      "2bd5028f4b85: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "b2ed56b85d3a: Pulling fs layer\n",
      "8bfb788e9874: Pulling fs layer\n",
      "0618fb353339: Pulling fs layer\n",
      "42045a665612: Pulling fs layer\n",
      "031d8d7b75f7: Pulling fs layer\n",
      "5780cc9addac: Pulling fs layer\n",
      "8fbe78107b3d: Pulling fs layer\n",
      "eee173fc570a: Pulling fs layer\n",
      "9334ecc802d5: Pulling fs layer\n",
      "c631c38965fd: Pulling fs layer\n",
      "803ecb627365: Pulling fs layer\n",
      "4466b5c2ac37: Pulling fs layer\n",
      "16a6777d4439: Pulling fs layer\n",
      "7a84944cecd7: Pulling fs layer\n",
      "424e7c9d5d89: Waiting\n",
      "9b397537aef0: Waiting\n",
      "2bd5028f4b85: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "b2ed56b85d3a: Waiting\n",
      "8bfb788e9874: Waiting\n",
      "0618fb353339: Waiting\n",
      "42045a665612: Waiting\n",
      "031d8d7b75f7: Waiting\n",
      "5780cc9addac: Waiting\n",
      "8fbe78107b3d: Waiting\n",
      "eee173fc570a: Waiting\n",
      "9334ecc802d5: Waiting\n",
      "c631c38965fd: Waiting\n",
      "803ecb627365: Waiting\n",
      "4466b5c2ac37: Waiting\n",
      "16a6777d4439: Waiting\n",
      "7a84944cecd7: Waiting\n",
      "57ffbe87baa1: Verifying Checksum\n",
      "57ffbe87baa1: Download complete\n",
      "f3b4a5f15c7a: Verifying Checksum\n",
      "f3b4a5f15c7a: Download complete\n",
      "424e7c9d5d89: Verifying Checksum\n",
      "424e7c9d5d89: Download complete\n",
      "01bf7da0a88c: Verifying Checksum\n",
      "01bf7da0a88c: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "b2ed56b85d3a: Verifying Checksum\n",
      "b2ed56b85d3a: Download complete\n",
      "2bd5028f4b85: Verifying Checksum\n",
      "2bd5028f4b85: Download complete\n",
      "0618fb353339: Verifying Checksum\n",
      "0618fb353339: Download complete\n",
      "42045a665612: Verifying Checksum\n",
      "42045a665612: Download complete\n",
      "031d8d7b75f7: Verifying Checksum\n",
      "031d8d7b75f7: Download complete\n",
      "5780cc9addac: Verifying Checksum\n",
      "5780cc9addac: Download complete\n",
      "8fbe78107b3d: Verifying Checksum\n",
      "8fbe78107b3d: Download complete\n",
      "eee173fc570a: Verifying Checksum\n",
      "eee173fc570a: Download complete\n",
      "9334ecc802d5: Verifying Checksum\n",
      "9334ecc802d5: Download complete\n",
      "c631c38965fd: Verifying Checksum\n",
      "c631c38965fd: Download complete\n",
      "8bfb788e9874: Verifying Checksum\n",
      "8bfb788e9874: Download complete\n",
      "4466b5c2ac37: Verifying Checksum\n",
      "4466b5c2ac37: Download complete\n",
      "16a6777d4439: Verifying Checksum\n",
      "16a6777d4439: Download complete\n",
      "9b397537aef0: Verifying Checksum\n",
      "9b397537aef0: Download complete\n",
      "7a84944cecd7: Verifying Checksum\n",
      "7a84944cecd7: Download complete\n",
      "01bf7da0a88c: Pull complete\n",
      "f3b4a5f15c7a: Pull complete\n",
      "57ffbe87baa1: Pull complete\n",
      "424e7c9d5d89: Pull complete\n",
      "803ecb627365: Verifying Checksum\n",
      "803ecb627365: Download complete\n",
      "9b397537aef0: Pull complete\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/4758497209014550528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "2bd5028f4b85: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "b2ed56b85d3a: Pull complete\n",
      "8bfb788e9874: Pull complete\n",
      "0618fb353339: Pull complete\n",
      "42045a665612: Pull complete\n",
      "031d8d7b75f7: Pull complete\n",
      "5780cc9addac: Pull complete\n",
      "8fbe78107b3d: Pull complete\n",
      "eee173fc570a: Pull complete\n",
      "9334ecc802d5: Pull complete\n",
      "c631c38965fd: Pull complete\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/4758497209014550528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "803ecb627365: Pull complete\n",
      "4466b5c2ac37: Pull complete\n",
      "16a6777d4439: Pull complete\n",
      "7a84944cecd7: Pull complete\n",
      "Digest: sha256:56489bdb00b3f5c342d8cbcdeea68fd63b982858199c4ed7edac15614954b110\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/tf2-cpu.2-4:latest\n",
      " ---> fbf8f454dc05\n",
      "Step 2/5 : WORKDIR /trainer\n",
      " ---> Running in f27827977edb\n",
      "Removing intermediate container f27827977edb\n",
      " ---> b5e4f688dab8\n",
      "Step 3/5 : RUN pip install cloudml-hypertune\n",
      " ---> Running in 2ca5fda0cceb\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Building wheels for collected packages: cloudml-hypertune\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=3988 sha256=c10641ce3d30ab8a9287bf01a1a967f7ce92ca10a369237dcee5f4089c37e264\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/ff/87/e7bed0c2741fe219b3d6da67c2431d7f7fedb183032e00f81e\n",
      "Successfully built cloudml-hypertune\n",
      "Installing collected packages: cloudml-hypertune\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6\n",
      "\u001b[91mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 2ca5fda0cceb\n",
      " ---> c944d84c7edd\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> 3439638679ab\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 75e65d334486\n",
      "Removing intermediate container 75e65d334486\n",
      " ---> cacf02e143ed\n",
      "Successfully built cacf02e143ed\n",
      "Successfully tagged gcr.io/jk-wst1/jkwst1_taxi_trainer:latest\n",
      "PUSH\n",
      "Pushing gcr.io/jk-wst1/jkwst1_taxi_trainer\n",
      "The push refers to repository [gcr.io/jk-wst1/jkwst1_taxi_trainer]\n",
      "f38c4e6d371e: Preparing\n",
      "5df2ef96fdbd: Preparing\n",
      "e953b7fda400: Preparing\n",
      "d39fae2e5e4e: Preparing\n",
      "33302bd81efa: Preparing\n",
      "0c5f899e9bc7: Preparing\n",
      "5e593cdd4a12: Preparing\n",
      "06a5bf49b163: Preparing\n",
      "b34dae69fc5d: Preparing\n",
      "0ffb7465dde9: Preparing\n",
      "e2563d1ada9a: Preparing\n",
      "42b027d1e826: Preparing\n",
      "636a7c2e7d03: Preparing\n",
      "1ba1158adf89: Preparing\n",
      "96e46d1341e8: Preparing\n",
      "954f6dc3f7f5: Preparing\n",
      "8760a171b659: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "a0710233fd2d: Preparing\n",
      "05449afa4be9: Preparing\n",
      "5b9e34b5cf74: Preparing\n",
      "8cafc6d2db45: Preparing\n",
      "a5d4bacb0351: Preparing\n",
      "5153e1acaabc: Preparing\n",
      "1ba1158adf89: Waiting\n",
      "96e46d1341e8: Waiting\n",
      "954f6dc3f7f5: Waiting\n",
      "8760a171b659: Waiting\n",
      "0c5f899e9bc7: Waiting\n",
      "5e593cdd4a12: Waiting\n",
      "06a5bf49b163: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "a0710233fd2d: Waiting\n",
      "b34dae69fc5d: Waiting\n",
      "05449afa4be9: Waiting\n",
      "0ffb7465dde9: Waiting\n",
      "5b9e34b5cf74: Waiting\n",
      "8cafc6d2db45: Waiting\n",
      "e2563d1ada9a: Waiting\n",
      "42b027d1e826: Waiting\n",
      "a5d4bacb0351: Waiting\n",
      "636a7c2e7d03: Waiting\n",
      "5153e1acaabc: Waiting\n",
      "33302bd81efa: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "d39fae2e5e4e: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "0c5f899e9bc7: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "5e593cdd4a12: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "f38c4e6d371e: Pushed\n",
      "5df2ef96fdbd: Pushed\n",
      "e953b7fda400: Pushed\n",
      "06a5bf49b163: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "b34dae69fc5d: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "0ffb7465dde9: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "42b027d1e826: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "e2563d1ada9a: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "636a7c2e7d03: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "1ba1158adf89: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "96e46d1341e8: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "5f70bf18a086: Layer already exists\n",
      "8760a171b659: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "954f6dc3f7f5: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "a0710233fd2d: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "8cafc6d2db45: Layer already exists\n",
      "05449afa4be9: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "a5d4bacb0351: Layer already exists\n",
      "5153e1acaabc: Layer already exists\n",
      "5b9e34b5cf74: Mounted from deeplearning-platform-release/tf2-cpu.2-4\n",
      "latest: digest: sha256:09bd8d7817c00b2e6afe074e240dea17123b89242deed38aa3b22d8813b31adb size: 5335\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                 IMAGES                                        STATUS\n",
      "74e39738-dc48-4dc0-a253-a5df8afc2200  2021-06-28T17:18:59+00:00  2M45S     gs://jk-wst1_cloudbuild/source/1624900738.281183-e25ad1a5405041cea901455a44f7268b.tgz  gcr.io/jk-wst1/jkwst1_taxi_trainer (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag {TRAIN_IMAGE} {SCRIPT_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304bdb95",
   "metadata": {},
   "source": [
    "#### Prepare worker pool specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "752b9ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/7945919845286019072 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "worker_pool_specs =  [\n",
    "    {\n",
    "        \"machine_spec\": {\n",
    "            \"machine_type\": \"n1-standard-4\",\n",
    "#            \"accelerator_type\": \"NVIDIA_TESLA_T4\",\n",
    "#            \"accelerator_count\": 1,\n",
    "        },\n",
    "        \"replica_count\": 1,\n",
    "        \"container_spec\": {\n",
    "            \"image_uri\": TRAIN_IMAGE,\n",
    "#            \"command\": [\"python\", \"train.py\"],\n",
    "            \"args\": [\n",
    "                '--epochs=2', \n",
    "                '--per_replica_batch_size=128',\n",
    "                '--training_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "                '--validation_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_VALID_SPLIT_NAME}',\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89468a42",
   "metadata": {},
   "source": [
    "#### Submit and monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c7b4622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating CustomJob\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob created. Resource name: projects/630263135640/locations/us-central1/customJobs/5306810463646908416\n",
      "INFO:google.cloud.aiplatform.jobs:To use this CustomJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:custom_job = aiplatform.CustomJob.get('projects/630263135640/locations/us-central1/customJobs/5306810463646908416')\n",
      "INFO:google.cloud.aiplatform.jobs:View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/5306810463646908416?project=630263135640\n",
      "INFO:google.cloud.aiplatform.jobs:View Tensorboard:\n",
      "https://us-central1.tensorboard.googleusercontent.com/experiment/projects+630263135640+locations+us-central1+tensorboards+5276565097790046208+experiments+5306810463646908416\n"
     ]
    }
   ],
   "source": [
    "job_name = f'{PREFIX}_CUSTOM_CONTAINER'\n",
    "base_output_dir = '{}/jobs/{}/{}'.format(STAGING_BUCKET, job_name, time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "job = vertex_ai.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    "    staging_bucket=base_output_dir\n",
    ")\n",
    "\n",
    "job.run(sync=False, \n",
    "        service_account=VERTEX_SA,\n",
    "        tensorboard=TENSORBOARD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f49731",
   "metadata": {},
   "source": [
    "### Configure and submit a hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32fc949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f'{PREFIX}_HYPERPARAMETER'\n",
    "base_output_dir = '{}/jobs/{}/{}'.format(STAGING_BUCKET, job_name, time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "hyperparameter_tuning_job_spec = {\n",
    "        \"display_name\": job_name,\n",
    "        \"max_trial_count\": 3,\n",
    "        \"parallel_trial_count\": 3,\n",
    "        \"max_failed_trial_count\": 1,\n",
    "        \"study_spec\": {\n",
    "            \"metrics\": [\n",
    "                {\n",
    "                    \"metric_id\": \"val_accuracy\",\n",
    "                    \"goal\": vertex_ai.gapic.StudySpec.MetricSpec.GoalType.MAXIMIZE,\n",
    "                }\n",
    "            ],\n",
    "            \"parameters\": [\n",
    "                {\n",
    "                    \"parameter_id\": \"units\",\n",
    "                    \"discrete_value_spec\": {\"values\": [32, 64]},\n",
    "                },\n",
    "                {\n",
    "                    \"parameter_id\": \"dropout_ratio\",\n",
    "                    \"double_value_spec\": {\"min_value\": 0.4, \"max_value\": 0.6},\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        \"trial_job_spec\": {\n",
    "            \"base_output_directory\": {\n",
    "                \"output_uri_prefix\": base_output_dir\n",
    "            },\n",
    "            \"service_account\": VERTEX_SA,\n",
    "            \"tensorboard\": TENSORBOARD,\n",
    "            \"worker_pool_specs\": [\n",
    "                {\n",
    "                    \"machine_spec\": {\n",
    "                        \"machine_type\": \"n1-standard-4\",\n",
    "                      #  \"accelerator_type\": vertex_ai.gapic.AcceleratorType.NVIDIA_TESLA_T4,\n",
    "                      #  \"accelerator_count\": 1,\n",
    "                    },\n",
    "                    \"replica_count\": 1,\n",
    "                    \"container_spec\": {\n",
    "                        \"image_uri\": TRAIN_IMAGE,\n",
    "                        \"args\": [\n",
    "                            '--epochs=5', \n",
    "                            '--per_replica_batch_size=128',\n",
    "                            '--training_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_TRAIN_SPLIT_NAME}',\n",
    "                            '--validation_table=' + f'{PROJECT}.{BQ_DATASET_NAME}.{BQ_VALID_SPLIT_NAME}',\n",
    "                        ],\n",
    "                    },\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4febf175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/5306810463646908416 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "job_client = vertex_ai.initializer.global_config.create_client(\n",
    "        client_class=JobClientWithOverride, location_override=REGION\n",
    ")\n",
    "\n",
    "parent = f'projects/{PROJECT}/locations/{REGION}'\n",
    "version = 'v1beta1'\n",
    "\n",
    "response = job_client.select_version(version).create_hyperparameter_tuning_job(\n",
    "    parent=parent, hyperparameter_tuning_job=hyperparameter_tuning_job_spec\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f80d9b",
   "metadata": {},
   "source": [
    "#### Monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "119cf42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/7945919845286019072 current state:\n",
      "JobState.JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "job_spec = job_client.get_hyperparameter_tuning_job(\n",
    "    name = response.name\n",
    ")\n",
    "print(job_spec.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea36b5",
   "metadata": {},
   "source": [
    "#### Wait for tuning to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d40a7be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/5306810463646908416 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/7945919845286019072 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/5306810463646908416 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/7945919845286019072 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/5306810463646908416 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/4758497209014550528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/7945919845286019072 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/5306810463646908416 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/7945919845286019072 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/5306810463646908416 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/4758497209014550528 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/7945919845286019072 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/5306810463646908416 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/4758497209014550528 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/7945919845286019072 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/5306810463646908416 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/5306810463646908416 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob run completed. Resource name: projects/630263135640/locations/us-central1/customJobs/5306810463646908416\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/7945919845286019072 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob run completed. Resource name: projects/630263135640/locations/us-central1/customJobs/7945919845286019072\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/4758497209014550528 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/630263135640/locations/us-central1/customJobs/4758497209014550528 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob run completed. Resource name: projects/630263135640/locations/us-central1/customJobs/4758497209014550528\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have not completed: JobState.JOB_STATE_RUNNING\n",
      "Study trials have completed\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    job_response = job_client.get_hyperparameter_tuning_job(\n",
    "        name = response.name)\n",
    "    if job_response.state != vertex_ai.gapic.JobState.JOB_STATE_SUCCEEDED:\n",
    "        print(\"Study trials have not completed:\", job_response.state)\n",
    "        if (job_response.state == vertex_ai.gapic.JobState.JOB_STATE_FAILED or \n",
    "           job_response.state == vertex_ai.gapic.JobState.JOB_STATE_CANCELLED):\n",
    "            break\n",
    "    else:\n",
    "        print(\"Study trials have completed\")\n",
    "        break\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077ad1cb",
   "metadata": {},
   "source": [
    "#### Review the results of the study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09c115e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: \"1\"\n",
      "state: SUCCEEDED\n",
      "parameters {\n",
      "  parameter_id: \"dropout_ratio\"\n",
      "  value {\n",
      "    number_value: 0.5\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameter_id: \"units\"\n",
      "  value {\n",
      "    number_value: 64.0\n",
      "  }\n",
      "}\n",
      "final_measurement {\n",
      "  step_count: 4\n",
      "  metrics {\n",
      "    metric_id: \"val_accuracy\"\n",
      "    value: 0.8821922540664673\n",
      "  }\n",
      "}\n",
      "start_time {\n",
      "  seconds: 1624900926\n",
      "  nanos: 86728844\n",
      "}\n",
      "end_time {\n",
      "  seconds: 1624902475\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0.0\n",
    "best_trial = None\n",
    "for trial in job_response.trials:\n",
    "    accuracy = float(trial.final_measurement.metrics[0].value)\n",
    "    if accuracy > best_accuracy:\n",
    "        best_trial = trial\n",
    "        best_accuracy = accuracy\n",
    "print(best_trial)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f17aa40",
   "metadata": {},
   "source": [
    "#### Get the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fc498a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://jk1-bucket/jobs/jk1_HYPERPARAMETER/20210622_031249/\n",
      "gs://jk1-bucket/jobs/jk1_HYPERPARAMETER/20210622_031249/1/\n",
      "gs://jk1-bucket/jobs/jk1_HYPERPARAMETER/20210622_031249/2/\n",
      "gs://jk1-bucket/jobs/jk1_HYPERPARAMETER/20210622_031249/3/\n"
     ]
    }
   ],
   "source": [
    "model_dir = job_response.trial_job_spec.base_output_directory.output_uri_prefix\n",
    "\n",
    "!gsutil ls {model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9dc3f5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://jk1-bucket/jobs/jk1_HYPERPARAMETER/20210622_031249/2/\n",
      "gs://jk1-bucket/jobs/jk1_HYPERPARAMETER/20210622_031249/2/checkpoints/\n",
      "gs://jk1-bucket/jobs/jk1_HYPERPARAMETER/20210622_031249/2/logs/\n",
      "gs://jk1-bucket/jobs/jk1_HYPERPARAMETER/20210622_031249/2/model/\n"
     ]
    }
   ],
   "source": [
    "best_model_dir = '{}/{}'.format(model_dir, best_trial.id)\n",
    "\n",
    "!gsutil ls {best_model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc1adf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
