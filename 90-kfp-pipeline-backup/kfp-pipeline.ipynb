{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing Vertex AI pipelines with the KFP v2 SDK\n",
    "\n",
    "![Vertex pipeline](../images/pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import kfp\n",
    "import kfp.v2.dsl as dsl\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, Metrics, ClassificationMetrics)\n",
    "from typing import NamedTuple, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure lab settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'jk-vertexai-ws'\n",
    "REGION = 'us-central1'\n",
    "PREFIX = 'jkwst1'\n",
    "\n",
    "STAGING_BUCKET = f'gs://{PREFIX}-bucket'\n",
    "VERTEX_SA = f'training-sa@{PROJECT}.iam.gserviceaccount.com'\n",
    "PIPELINES_SA = f'pipelines-sa@{PROJECT}.iam.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy training data schema to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://schema.pbtxt [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  3.1 KiB/  3.1 KiB]                                                \n",
      "Operation completed over 1 objects/3.1 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "SCHEMA_LOCATION = f'{STAGING_BUCKET}/schema'\n",
    "\n",
    "!gsutil cp schema.pbtxt {SCHEMA_LOCATION}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://jkwst1-bucket/schema/schema.pbtxt\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls {SCHEMA_LOCATION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a training container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_FOLDER = 'trainer'\n",
    "if tf.io.gfile.exists(SCRIPT_FOLDER):\n",
    "    tf.io.gfile.rmtree(SCRIPT_FOLDER)\n",
    "tf.io.gfile.mkdir(SCRIPT_FOLDER)\n",
    "file_path = os.path.join(SCRIPT_FOLDER, 'train.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {file_path}\n",
    "\n",
    "\n",
    "# Copyright 2021 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow_io import bigquery as tfio_bq\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('epochs', 3, 'Nubmer of epochs')\n",
    "flags.DEFINE_integer('units', 32, 'Number units in a hidden layer')\n",
    "flags.DEFINE_integer('per_replica_batch_size', 128, 'Per replica batch size')\n",
    "flags.DEFINE_float('dropout_ratio', 0.5, 'Dropout ratio')\n",
    "flags.DEFINE_string('training_table', None, 'Training table name')\n",
    "flags.DEFINE_string('validation_table', None, 'Validationa table name')\n",
    "flags.DEFINE_string('schema_file', None, 'Location of the data schema file')\n",
    "flags.mark_flag_as_required('training_table')\n",
    "flags.mark_flag_as_required('validation_table')\n",
    "flags.mark_flag_as_required('schema_file')\n",
    "\n",
    "LOCAL_MODEL_DIR = '/tmp/saved_model'\n",
    "LOCAL_TB_DIR = '/tmp/logs'\n",
    "LOCAL_CHECKPOINT_DIR = '/tmp/checkpoints'\n",
    "TARGET_TAG = 'target'\n",
    "\n",
    "\n",
    "def schema_to_features(schema):\n",
    "    \"\"\"Converts a schema_pb2 protobuf to feature dictionary.\"\"\"\n",
    "    \n",
    "    features = {}\n",
    "    for feature in schema.feature:\n",
    "        if feature.type == 2:\n",
    "            if feature.int_domain.is_categorical:\n",
    "                features[feature.name] = ('categorical', tf.int64)\n",
    "            else:\n",
    "                features[feature.name] = ('numeric', tf.int64)\n",
    "        elif feature.type == 1:\n",
    "            features[feature.name] = ('categorical', tf.string)\n",
    "        elif feature.type == 3:\n",
    "            features[feature.name] = ('numeric', tf.double)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def get_target_feature(schema):\n",
    "    \"\"\"Returns the name of the target feature in schema.\"\"\"\n",
    "    \n",
    "    target_feature = None\n",
    "    for feature in schema.feature:\n",
    "        if feature.HasField('annotation'):\n",
    "            if TARGET_TAG in feature.annotation.tag:\n",
    "                target_feature = feature.name\n",
    "    return target_feature\n",
    "\n",
    "\n",
    "def set_job_dirs():\n",
    "    \"\"\"Sets job directories based on env variables set by Vertex AI.\"\"\"\n",
    "    \n",
    "    model_dir = os.getenv('AIP_MODEL_DIR', LOCAL_MODEL_DIR)\n",
    "    tb_dir = os.getenv('AIP_TENSORBOARD_LOG_DIR', LOCAL_TB_DIR)\n",
    "    checkpoint_dir = os.getenv('AIP_CHECKPOINT_DIR', LOCAL_CHECKPOINT_DIR)\n",
    "    \n",
    "    return model_dir, tb_dir, checkpoint_dir\n",
    "\n",
    "\n",
    "def get_bq_dataset(table_name, features, target_feature, batch_size=32):\n",
    "    \"\"\"Creates a tf.data dataset for direct access to BQ table.\"\"\"\n",
    "    \n",
    "    def _transform_row(row_dict):\n",
    "        trimmed_dict = {column:\n",
    "                       (tf.strings.strip(tensor) if tensor.dtype == 'string' else tensor) \n",
    "                       for (column,tensor) in row_dict.items()\n",
    "                       }\n",
    "        target = trimmed_dict.pop(target_feature)\n",
    "        return (trimmed_dict, target)\n",
    "    \n",
    "    selected_fields = {key: {'output_type': value[1]} \n",
    "                       for key, value in features.items()}\n",
    "    project_id, dataset_id, table_id = table_name.split('.')\n",
    "    client = tfio_bq.BigQueryClient()\n",
    "    parent = f'projects/{project_id}'\n",
    "\n",
    "    read_session = client.read_session(\n",
    "        parent=parent,\n",
    "        project_id=project_id,\n",
    "        table_id=table_id,\n",
    "        dataset_id=dataset_id,\n",
    "        selected_fields=selected_fields,\n",
    "    )\n",
    "\n",
    "    dataset = read_session.parallel_read_rows().map(_transform_row).batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_category_encoding_layer(name, dataset, dtype):\n",
    "    \"\"\"Creates a CategoryEncoding layer for a given feature.\"\"\"\n",
    "\n",
    "    if dtype == tf.string:\n",
    "      index = preprocessing.StringLookup()\n",
    "    else:\n",
    "      index = preprocessing.IntegerLookup()\n",
    "\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    index.adapt(feature_ds)\n",
    "    encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
    "\n",
    "    return lambda feature: encoder(index(feature))\n",
    "\n",
    "\n",
    "def get_normalization_layer(name, dataset):\n",
    "    \"\"\"\"Creates a Normalization layer for a given feature.\"\"\"\n",
    "    normalizer = preprocessing.Normalization()\n",
    "\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    return normalizer\n",
    "\n",
    "\n",
    "def create_model(dataset, input_features, units, dropout_ratio):\n",
    "    \"\"\"Creates a binary classifier for Chicago Taxi tip prediction task.\"\"\"\n",
    "    \n",
    "    all_inputs = []\n",
    "    encoded_features = []\n",
    "    for feature_name, feature_info in input_features.items():\n",
    "        col = tf.keras.Input(shape=(1,), name=feature_name, dtype=feature_info[1])\n",
    "        if feature_info[0] == 'categorical':\n",
    "            \n",
    "            encoding_layer = get_category_encoding_layer(feature_name, \n",
    "                                                         dataset,\n",
    "                                                         feature_info[1])\n",
    "        else:\n",
    "            encoding_layer = get_normalization_layer(feature_name,\n",
    "                                                     dataset) \n",
    "        encoded_col = encoding_layer(col)\n",
    "        all_inputs.append(col)\n",
    "        encoded_features.append(encoded_col)\n",
    "        \n",
    "    all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(units, activation=\"relu\")(all_features)\n",
    "    x = tf.keras.layers.Dropout(dropout_ratio)(x)\n",
    "    output = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.Model(all_inputs, output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    del argv\n",
    "    \n",
    "    # Set distribution strategy\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "    global_batch_size = (strategy.num_replicas_in_sync *\n",
    "                         FLAGS.per_replica_batch_size)\n",
    "    \n",
    "    # Extract features from schema_pb2\n",
    "    schema = tfdv.load_schema_text(FLAGS.schema_file)\n",
    "    features = schema_to_features(schema)\n",
    "    target_feature = get_target_feature(schema)\n",
    "\n",
    "    if not target_feature:\n",
    "        raise RuntimeError('Schema does not have a target feature')\n",
    "    \n",
    "    # Prepare datasets\n",
    "    validation_ds = get_bq_dataset(FLAGS.validation_table, \n",
    "                                   features,\n",
    "                                   target_feature,\n",
    "                                   batch_size=global_batch_size)\n",
    "    training_ds = get_bq_dataset(FLAGS.training_table,\n",
    "                                 features,\n",
    "                                 target_feature,\n",
    "                                 batch_size=global_batch_size)\n",
    "    \n",
    "    # Prepare the model\n",
    "    logging.info('Creating the model ...')\n",
    "    input_features = {key: value for key, value in features.items() if key != target_feature}\n",
    "    \n",
    "    with strategy.scope():\n",
    "        model = create_model(training_ds, input_features, FLAGS.units, FLAGS.dropout_ratio)\n",
    "        model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Configure Keras callbacks\n",
    "    model_dir, tb_dir, checkpoint_dir = set_job_dirs()\n",
    "    callbacks = [tf.keras.callbacks.experimental.BackupAndRestore(backup_dir=checkpoint_dir)]\n",
    "    callbacks.append(tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=tb_dir, update_freq='batch'))\n",
    "    \n",
    "    logging.info('Starting training ...')\n",
    "    model.fit(training_ds, \n",
    "              epochs=FLAGS.epochs, \n",
    "              validation_data=validation_ds,\n",
    "              callbacks=callbacks)\n",
    "    \n",
    "    # Save trained model\n",
    "    logging.info('Training completed. Saving the trained model to: {}'.format(model_dir))\n",
    "    model.save(model_dir)  \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "    app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = 'gcr.io/deeplearning-platform-release/tf2-cpu.2-4'\n",
    "TRAIN_IMAGE = f'gcr.io/{PROJECT}/taxi_classifier_trainer_v2'\n",
    "\n",
    "dockerfile = f'''\n",
    "FROM {BASE_IMAGE}\n",
    "\n",
    "WORKDIR /trainer\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n",
    "'''\n",
    "\n",
    "with open(os.path.join(SCRIPT_FOLDER, 'Dockerfile'), 'w') as f:\n",
    "    f.write(dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 8.0 KiB before compression.\n",
      "Uploading tarball of [trainer] to [gs://jk-wst1_cloudbuild/source/1624910431.434771-e3a22503cfff4662b529d66022f3525d.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/jk-wst1/locations/global/builds/75cf6e6f-ab4d-431a-874f-b6f49e9bbc02].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/75cf6e6f-ab4d-431a-874f-b6f49e9bbc02?project=630263135640].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"75cf6e6f-ab4d-431a-874f-b6f49e9bbc02\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://jk-wst1_cloudbuild/source/1624910431.434771-e3a22503cfff4662b529d66022f3525d.tgz#1624910431778319\n",
      "Copying gs://jk-wst1_cloudbuild/source/1624910431.434771-e3a22503cfff4662b529d66022f3525d.tgz#1624910431778319...\n",
      "/ [1 files][  2.9 KiB/  2.9 KiB]                                                \n",
      "Operation completed over 1 objects/2.9 KiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  10.75kB\n",
      "Step 1/4 : FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-4\n",
      "latest: Pulling from deeplearning-platform-release/tf2-cpu.2-4\n",
      "01bf7da0a88c: Pulling fs layer\n",
      "f3b4a5f15c7a: Pulling fs layer\n",
      "57ffbe87baa1: Pulling fs layer\n",
      "424e7c9d5d89: Pulling fs layer\n",
      "9b397537aef0: Pulling fs layer\n",
      "2bd5028f4b85: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "b2ed56b85d3a: Pulling fs layer\n",
      "8bfb788e9874: Pulling fs layer\n",
      "0618fb353339: Pulling fs layer\n",
      "42045a665612: Pulling fs layer\n",
      "031d8d7b75f7: Pulling fs layer\n",
      "5780cc9addac: Pulling fs layer\n",
      "8fbe78107b3d: Pulling fs layer\n",
      "eee173fc570a: Pulling fs layer\n",
      "9334ecc802d5: Pulling fs layer\n",
      "c631c38965fd: Pulling fs layer\n",
      "803ecb627365: Pulling fs layer\n",
      "4466b5c2ac37: Pulling fs layer\n",
      "16a6777d4439: Pulling fs layer\n",
      "7a84944cecd7: Pulling fs layer\n",
      "424e7c9d5d89: Waiting\n",
      "9b397537aef0: Waiting\n",
      "2bd5028f4b85: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "8bfb788e9874: Waiting\n",
      "0618fb353339: Waiting\n",
      "42045a665612: Waiting\n",
      "031d8d7b75f7: Waiting\n",
      "5780cc9addac: Waiting\n",
      "8fbe78107b3d: Waiting\n",
      "eee173fc570a: Waiting\n",
      "b2ed56b85d3a: Waiting\n",
      "9334ecc802d5: Waiting\n",
      "c631c38965fd: Waiting\n",
      "803ecb627365: Waiting\n",
      "4466b5c2ac37: Waiting\n",
      "16a6777d4439: Waiting\n",
      "7a84944cecd7: Waiting\n",
      "57ffbe87baa1: Download complete\n",
      "f3b4a5f15c7a: Verifying Checksum\n",
      "f3b4a5f15c7a: Download complete\n",
      "424e7c9d5d89: Verifying Checksum\n",
      "424e7c9d5d89: Download complete\n",
      "01bf7da0a88c: Verifying Checksum\n",
      "01bf7da0a88c: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "b2ed56b85d3a: Verifying Checksum\n",
      "b2ed56b85d3a: Download complete\n",
      "2bd5028f4b85: Download complete\n",
      "0618fb353339: Download complete\n",
      "42045a665612: Verifying Checksum\n",
      "42045a665612: Download complete\n",
      "031d8d7b75f7: Verifying Checksum\n",
      "031d8d7b75f7: Download complete\n",
      "5780cc9addac: Verifying Checksum\n",
      "5780cc9addac: Download complete\n",
      "8fbe78107b3d: Verifying Checksum\n",
      "8fbe78107b3d: Download complete\n",
      "eee173fc570a: Verifying Checksum\n",
      "eee173fc570a: Download complete\n",
      "9334ecc802d5: Verifying Checksum\n",
      "9334ecc802d5: Download complete\n",
      "c631c38965fd: Verifying Checksum\n",
      "c631c38965fd: Download complete\n",
      "8bfb788e9874: Verifying Checksum\n",
      "8bfb788e9874: Download complete\n",
      "4466b5c2ac37: Verifying Checksum\n",
      "4466b5c2ac37: Download complete\n",
      "9b397537aef0: Verifying Checksum\n",
      "9b397537aef0: Download complete\n",
      "7a84944cecd7: Verifying Checksum\n",
      "7a84944cecd7: Download complete\n",
      "16a6777d4439: Download complete\n",
      "01bf7da0a88c: Pull complete\n",
      "f3b4a5f15c7a: Pull complete\n",
      "57ffbe87baa1: Pull complete\n",
      "424e7c9d5d89: Pull complete\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag {TRAIN_IMAGE} {SCRIPT_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom KFP components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='gcr.io/ml-pipeline/google-cloud-pipeline-components:0.1.1')\n",
    "def ingest_data_op(\n",
    "    project: str,\n",
    "    bq_location: str,\n",
    "    sample_size: int,\n",
    "    year: int,\n",
    "    dataset_name: str,\n",
    "    train_split_name: str,\n",
    "    valid_split_name: str,\n",
    "    test_split_name: str,\n",
    "    dataset: Output[Dataset]\n",
    "):\n",
    "    \"\"\"Prepares training, validation, and testing data splits\n",
    "    from Chicago taxi public dataset.\"\"\"\n",
    "    \n",
    "    import logging\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import exceptions\n",
    "    \n",
    "    METADATA_TRAIN_SPLIT_KEY = 'train_split'\n",
    "    METADATA_VALID_SPLIT_KEY = 'valid_split'\n",
    "    METADATA_TEST_SPLIT_KEY = 'test_split'\n",
    "    \n",
    "    sql_script_template = '''\n",
    "    CREATE TEMP TABLE features \n",
    "    AS (\n",
    "        WITH\n",
    "        taxitrips AS (\n",
    "        SELECT\n",
    "            FORMAT_DATETIME('%Y-%d-%m', trip_start_timestamp) AS date,\n",
    "            trip_start_timestamp,\n",
    "            trip_seconds,\n",
    "            trip_miles,\n",
    "            payment_type,\n",
    "            pickup_longitude,\n",
    "            pickup_latitude,\n",
    "            dropoff_longitude,\n",
    "            dropoff_latitude,\n",
    "            tips,\n",
    "            fare\n",
    "        FROM\n",
    "            `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "        WHERE 1=1 \n",
    "        AND pickup_longitude IS NOT NULL\n",
    "        AND pickup_latitude IS NOT NULL\n",
    "        AND dropoff_longitude IS NOT NULL\n",
    "        AND dropoff_latitude IS NOT NULL\n",
    "        AND trip_miles > 0\n",
    "        AND trip_seconds > 0\n",
    "        AND fare > 0\n",
    "        AND EXTRACT(YEAR FROM trip_start_timestamp) = @YEAR\n",
    "        )\n",
    "\n",
    "        SELECT\n",
    "        trip_start_timestamp,\n",
    "        EXTRACT(MONTH from trip_start_timestamp) as trip_month,\n",
    "        EXTRACT(DAY from trip_start_timestamp) as trip_day,\n",
    "        EXTRACT(DAYOFWEEK from trip_start_timestamp) as trip_day_of_week,\n",
    "        EXTRACT(HOUR from trip_start_timestamp) as trip_hour,\n",
    "        trip_seconds,\n",
    "        trip_miles,\n",
    "        payment_type,\n",
    "        ST_AsText(\n",
    "            ST_SnapToGrid(ST_GeogPoint(pickup_longitude, pickup_latitude), 0.1)\n",
    "        ) AS pickup_grid,\n",
    "        ST_AsText(\n",
    "            ST_SnapToGrid(ST_GeogPoint(dropoff_longitude, dropoff_latitude), 0.1)\n",
    "        ) AS dropoff_grid,\n",
    "        ST_Distance(\n",
    "            ST_GeogPoint(pickup_longitude, pickup_latitude), \n",
    "            ST_GeogPoint(dropoff_longitude, dropoff_latitude)\n",
    "        ) AS euclidean,\n",
    "        IF((tips/fare >= 0.2), 1, 0) AS tip_bin,\n",
    "        CASE (ABS(MOD(FARM_FINGERPRINT(date),10))) \n",
    "            WHEN 9 THEN 'TEST'\n",
    "            WHEN 8 THEN 'VALIDATE'\n",
    "            ELSE 'TRAIN' END AS data_split\n",
    "        FROM\n",
    "        taxitrips\n",
    "        LIMIT @LIMIT\n",
    "    );\n",
    "\n",
    "    CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@TRAIN_SPLIT`\n",
    "    AS\n",
    "    SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "    FROM features\n",
    "    WHERE data_split='TRAIN';\n",
    "\n",
    "    CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@VALIDATE_SPLIT`\n",
    "    AS\n",
    "    SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "    FROM features\n",
    "    WHERE data_split='VALIDATE';\n",
    "\n",
    "    CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@TEST_SPLIT`\n",
    "    AS\n",
    "    SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "    FROM features\n",
    "    WHERE data_split='TEST';\n",
    "\n",
    "    DROP TABLE features;\n",
    "    '''\n",
    "    \n",
    "    client = bigquery.Client(project=project)\n",
    "    ds = bigquery.Dataset(f'{project}.{dataset_name}')\n",
    "    ds.location = bq_location\n",
    "    try:\n",
    "        ds = client.create_dataset(ds, timeout=30)\n",
    "        logging.info(f'Created dataset: {project}.{dataset_name}')\n",
    "    except exceptions.Conflict:\n",
    "        logging.info(f'Dataset {project}.{dataset_name} already exists')\n",
    "        \n",
    "    sql_script = sql_script_template.replace(\n",
    "        '@PROJECT', project).replace(\n",
    "        '@DATASET', dataset_name).replace(\n",
    "        '@TRAIN_SPLIT', train_split_name).replace(\n",
    "        '@VALIDATE_SPLIT', valid_split_name).replace(\n",
    "        '@TEST_SPLIT', test_split_name).replace(\n",
    "        '@YEAR', str(year)).replace(\n",
    "        '@LIMIT', str(sample_size))\n",
    "\n",
    "    job = client.query(sql_script)\n",
    "    job.result()\n",
    "    \n",
    "    dataset.metadata[METADATA_TRAIN_SPLIT_KEY] = f'{project}.{dataset_name}.{train_split_name}'\n",
    "    dataset.metadata[METADATA_VALID_SPLIT_KEY] = f'{project}.{dataset_name}.{valid_split_name}'\n",
    "    dataset.metadata[METADATA_TEST_SPLIT_KEY] = f'{project}.{dataset_name}.{test_split_name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='tensorflow/tfx:latest',\n",
    "               packages_to_install=['google-cloud-bigquery[bqstorage,pandas]'])\n",
    "def generate_stats_op(\n",
    "    project: str,\n",
    "    sample_percentage: int,\n",
    "    dataset: Input[Dataset],\n",
    "    stats: Output[Artifact],\n",
    "   \n",
    "):\n",
    "    \"\"\"Generates statistics for the data splits \n",
    "    referenced in the input Dataset artifact.\"\"\"\n",
    "    \n",
    "    import os\n",
    "    import tensorflow_data_validation as tfdv\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    METADATA_TRAIN_SPLIT_KEY = 'train_split'\n",
    "    METADATA_VALID_SPLIT_KEY = 'valid_split'\n",
    "    METADATA_TEST_SPLIT_KEY = 'test_split'\n",
    "    \n",
    "    STATS_FILE_NAME = 'stats.pbtxt'\n",
    "    \n",
    "    sql_script_template = '''\n",
    "    SELECT * \n",
    "    FROM @TABLE\n",
    "    TABLESAMPLE SYSTEM (@SAMPLE_PERC PERCENT)\n",
    "    '''\n",
    "    \n",
    "    client = bigquery.Client(project=project)\n",
    "    for key in [METADATA_TRAIN_SPLIT_KEY, METADATA_VALID_SPLIT_KEY, METADATA_TEST_SPLIT_KEY]:\n",
    "        if key in dataset.metadata.keys():\n",
    "            sql_script = sql_script_template.replace(\n",
    "                '@TABLE', dataset.metadata[key]).replace(\n",
    "                '@SAMPLE_PERC', str(sample_percentage))\n",
    "            \n",
    "            df = client.query(sql_script).result().to_dataframe()\n",
    "    \n",
    "            stats_proto = tfdv.generate_statistics_from_dataframe(\n",
    "                dataframe=df,\n",
    "                stats_options=tfdv.StatsOptions(\n",
    "                    num_top_values=50\n",
    "                )\n",
    "            )\n",
    "    \n",
    "            file_path = os.path.join(stats.path, key)\n",
    "            os.makedirs(file_path)\n",
    "            tfdv.write_stats_text(stats_proto, \n",
    "                                  os.path.join(file_path, STATS_FILE_NAME))\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='tensorflow/tfx:latest')\n",
    "def validate_stats_op(\n",
    "    project: str,\n",
    "    stats: Input[Artifact],\n",
    "    schema: Input[Artifact],\n",
    "    anomalies: Output[Artifact],  \n",
    ")-> NamedTuple(\n",
    "    'ValidOutputs',\n",
    "    [\n",
    "        ('anomalies_detected', str)\n",
    "    ]):\n",
    "    \"\"\"Validates statistices referenced in the input stats Artifact.\"\"\"\n",
    "    \n",
    "    STATS_FILE_NAME = 'stats.pbtxt'\n",
    "    ANOMALIES_FILE_NAME = 'anomalies.pbtxt'\n",
    "    TRUE = 'true'\n",
    "    FALSE = 'false'\n",
    "    \n",
    "    import os\n",
    "    import logging\n",
    "    import tensorflow_data_validation as tfdv\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    schema_proto = tfdv.load_schema_text(\n",
    "        input_path=schema.path\n",
    "    ) \n",
    "    \n",
    "    anomalies_detected = FALSE\n",
    "    for folder in os.listdir(stats.path):\n",
    "        stats_proto = tfdv.load_stats_text(\n",
    "            input_path=os.path.join(stats.path, folder, STATS_FILE_NAME)\n",
    "        )\n",
    "        \n",
    "        anomalies_proto = tfdv.validate_statistics(\n",
    "            statistics=stats_proto, \n",
    "            schema=schema_proto\n",
    "        )\n",
    "        \n",
    "        file_path = os.path.join(anomalies.path, folder)\n",
    "        os.makedirs(file_path)\n",
    "        file_path = os.path.join(file_path, ANOMALIES_FILE_NAME)\n",
    "        tfdv.write_anomalies_text(anomalies_proto, file_path)\n",
    "                                 \n",
    "        if anomalies_proto.anomaly_info:\n",
    "            anomalies_detected = TRUE\n",
    "            logging.info('Anomamlies detected: {}'.format(file_path))\n",
    "    \n",
    "    output = namedtuple('ValidOutputs', ['anomalies_detected'])\n",
    "    \n",
    "    return output(anomalies_detected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile time settings\n",
    "PIPELINE_NAME = f'{PREFIX}-continuous-training-pipeline'\n",
    "\n",
    "BQ_DATASET_NAME = f'{PREFIX}_dataset_pipeline'\n",
    "BQ_LOCATION = 'US'\n",
    "TRAINING_TABLE_NAME = 'training'\n",
    "VALIDATION_TABLE_NAME = 'validation'\n",
    "TESTING_TABLE_NAME = 'testing'\n",
    "SCHEMA = f'{SCHEMA_LOCATION}/schema.pbtxt'\n",
    "\n",
    "TRAINING_CONTAINER_IMAGE = TRAIN_IMAGE\n",
    "TRAINING_MACHINE_TYPE = 'n1-standard-4'\n",
    "REPLICA_COUNT = 1\n",
    "\n",
    "SERVING_CONTAINER_IMAGE = 'us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-4:latest'\n",
    "SERVING_MACHINE_TYPE = 'n1-standard-4'\n",
    "\n",
    "\n",
    "@dsl.pipeline(name=PIPELINE_NAME)\n",
    "def taxi_tip_predictor_training(\n",
    "    model_display_name: str,\n",
    "    epochs: int,\n",
    "    staging_location: str,\n",
    "    per_replica_batch_size: int,\n",
    "    sample_percentage: int = 100,\n",
    "    year: int = 2020,\n",
    "    sample_size: int = 1000000,\n",
    "):\n",
    "    \n",
    "    import_schema = kfp.dsl.importer(\n",
    "        artifact_uri=SCHEMA,\n",
    "        artifact_class=Artifact,\n",
    "        reimport=False,\n",
    "    )\n",
    "    \n",
    "    prepare_data = ingest_data_op(\n",
    "        project=PROJECT,\n",
    "        bq_location=BQ_LOCATION,\n",
    "        sample_size=sample_size,\n",
    "        year=year,\n",
    "        dataset_name=BQ_DATASET_NAME,\n",
    "        train_split_name=TRAINING_TABLE_NAME,\n",
    "        valid_split_name=VALIDATION_TABLE_NAME,\n",
    "        test_split_name=TESTING_TABLE_NAME,\n",
    "    )\n",
    "    \n",
    "    generate_stats = generate_stats_op(\n",
    "        project=PROJECT,\n",
    "        sample_percentage=sample_percentage,\n",
    "        dataset=prepare_data.outputs['dataset'],\n",
    "    )\n",
    "    \n",
    "    validate_stats = validate_stats_op(\n",
    "        project=PROJECT,\n",
    "        schema=import_schema.output,\n",
    "        stats=generate_stats.outputs['stats'],\n",
    "    )\n",
    "    \n",
    "    with dsl.Condition(validate_stats.outputs['anomalies_detected'] == 'false',\n",
    "                       name = 'Anomalies detected'):\n",
    "    \n",
    "        args = [\n",
    "            '--epochs', str(epochs),\n",
    "            '--per_replica_batch_size', str(per_replica_batch_size),\n",
    "            '--training_table', f'{PROJECT}.{BQ_DATASET_NAME}.{TRAINING_TABLE_NAME}',\n",
    "            '--validation_table',  f'{PROJECT}.{BQ_DATASET_NAME}.{VALIDATION_TABLE_NAME}',\n",
    "            '--schema_file', SCHEMA,\n",
    "        ]\n",
    "        \n",
    "        train = gcc_aip.CustomContainerTrainingJobRunOp(\n",
    "            project=PROJECT,\n",
    "            location=REGION,\n",
    "            display_name=model_display_name,\n",
    "            model_display_name=model_display_name,\n",
    "            container_uri=TRAINING_CONTAINER_IMAGE,\n",
    "            args=args,\n",
    "            replica_count=REPLICA_COUNT,\n",
    "            staging_bucket=staging_location,\n",
    "            model_serving_container_image_uri=SERVING_CONTAINER_IMAGE,\n",
    "        )\n",
    "    \n",
    "        create_endpoint = gcc_aip.EndpointCreateOp(\n",
    "            project=PROJECT,\n",
    "            display_name=model_display_name\n",
    "        )\n",
    "        create_endpoint.after(train)\n",
    "    \n",
    "        deploy_model = gcc_aip.ModelDeployOp(\n",
    "            project=PROJECT,\n",
    "            endpoint=create_endpoint.outputs['endpoint'],\n",
    "            model=train.outputs['model'],\n",
    "            deployed_model_display_name=model_display_name,\n",
    "            machine_type=SERVING_MACHINE_TYPE\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "package_path = 'taxi_tip_predictor_pipeline.json'\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=taxi_tip_predictor_training,\n",
    "    package_path=package_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT,\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/jk-continuous-training-pipeline-20210616005908?project=jk-vertexai-ws\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameter_values = {\n",
    "    'model_display_name': f'{PREFIX} Taxi tip predictor',\n",
    "    'epochs': 2,\n",
    "    'per_replica_batch_size': 128,\n",
    "    'staging_location': f'{STAGING_BUCKET}/vertex_staging',\n",
    "}\n",
    "\n",
    "response = api_client.create_run_from_job_spec(\n",
    "    package_path,\n",
    "    pipeline_root=f'{STAGING_BUCKET}/pipeline_runs',\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=False,\n",
    "    service_account=PIPELINES_SA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract pipeline run metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline_name</th>\n",
       "      <th>run_name</th>\n",
       "      <th>param.input:per_replica_batch_size</th>\n",
       "      <th>param.input:year</th>\n",
       "      <th>param.input:model_display_name</th>\n",
       "      <th>param.input:sample_percentage</th>\n",
       "      <th>param.input:staging_location</th>\n",
       "      <th>param.input:epochs</th>\n",
       "      <th>param.input:sample_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jk-continuous-training-pipeline</td>\n",
       "      <td>jk-continuous-training-pipeline-20210616005908</td>\n",
       "      <td>128</td>\n",
       "      <td>2020</td>\n",
       "      <td>jk Taxi tip predictor</td>\n",
       "      <td>100</td>\n",
       "      <td>gs://jk-bucket/vertex_staging</td>\n",
       "      <td>2</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jk-continuous-training-pipeline</td>\n",
       "      <td>jk-continuous-training-pipeline-20210615220234</td>\n",
       "      <td>128</td>\n",
       "      <td>2020</td>\n",
       "      <td>jk Taxi tip predictor</td>\n",
       "      <td>100</td>\n",
       "      <td>gs://jk-bucket/vertex_staging</td>\n",
       "      <td>2</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     pipeline_name  \\\n",
       "0  jk-continuous-training-pipeline   \n",
       "1  jk-continuous-training-pipeline   \n",
       "\n",
       "                                         run_name  \\\n",
       "0  jk-continuous-training-pipeline-20210616005908   \n",
       "1  jk-continuous-training-pipeline-20210615220234   \n",
       "\n",
       "  param.input:per_replica_batch_size param.input:year  \\\n",
       "0                                128             2020   \n",
       "1                                128             2020   \n",
       "\n",
       "  param.input:model_display_name param.input:sample_percentage  \\\n",
       "0          jk Taxi tip predictor                           100   \n",
       "1          jk Taxi tip predictor                           100   \n",
       "\n",
       "    param.input:staging_location param.input:epochs param.input:sample_size  \n",
       "0  gs://jk-bucket/vertex_staging                  2                 1000000  \n",
       "1  gs://jk-bucket/vertex_staging                  2                 1000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_name = PIPELINE_NAME\n",
    "\n",
    "pipeline_df = aiplatform.get_pipeline_df(pipeline=pipeline_name)\n",
    "pipeline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
