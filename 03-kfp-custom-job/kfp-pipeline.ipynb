{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing pipelines with the KFP SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import uuid\n",
    "import kfp\n",
    "import tensorflow as tf\n",
    "\n",
    "import kfp.v2.dsl as dsl\n",
    "\n",
    "from datetime import datetime\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, Metrics, ClassificationMetrics)\n",
    "\n",
    "\n",
    "from typing import NamedTuple, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a training container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRIPT_FOLDER = 'trainer'\n",
    "if tf.io.gfile.exists(SCRIPT_FOLDER):\n",
    "    tf.io.gfile.rmtree(SCRIPT_FOLDER)\n",
    "tf.io.gfile.mkdir(SCRIPT_FOLDER)\n",
    "file_path = os.path.join(SCRIPT_FOLDER, 'train.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {file_path}\n",
    "\n",
    "\n",
    "# Copyright 2021 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#            http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow_io import bigquery as tfio_bq\n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_integer('epochs', 3, 'Nubmer of epochs')\n",
    "flags.DEFINE_integer('units', 32, 'Number units in a hidden layer')\n",
    "flags.DEFINE_integer('per_replica_batch_size', 128, 'Per replica batch size')\n",
    "flags.DEFINE_float('dropout_ratio', 0.5, 'Dropout ratio')\n",
    "flags.DEFINE_string('training_table', None, 'Training table name')\n",
    "flags.DEFINE_string('validation_table', None, 'Validationa table name')\n",
    "flags.DEFINE_string('schema_file', None, 'Location of the data schema file')\n",
    "flags.mark_flag_as_required('training_table')\n",
    "flags.mark_flag_as_required('validation_table')\n",
    "flags.mark_flag_as_required('schema_file')\n",
    "\n",
    "LOCAL_MODEL_DIR = '/tmp/saved_model'\n",
    "LOCAL_TB_DIR = '/tmp/logs'\n",
    "LOCAL_CHECKPOINT_DIR = '/tmp/checkpoints'\n",
    "TARGET_TAG = 'target'\n",
    "\n",
    "\n",
    "def schema_to_features(schema):\n",
    "    \"\"\"Converts schema_pb2 protobuf to feature dictionary.\"\"\"\n",
    "    \n",
    "    features = {}\n",
    "    for feature in schema.feature:\n",
    "        if feature.type == 2:\n",
    "            if feature.int_domain.is_categorical:\n",
    "                features[feature.name] = ('categorical', tf.int64)\n",
    "            else:\n",
    "                features[feature.name] = ('numeric', tf.int64)\n",
    "        elif feature.type == 1:\n",
    "            features[feature.name] = ('categorical', tf.string)\n",
    "        elif feature.type == 3:\n",
    "            features[feature.name] = ('numeric', tf.double)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def get_target_feature(schema):\n",
    "    \"\"\"Returns the name of the target feature from schema.\"\"\"\n",
    "    \n",
    "    target_feature = None\n",
    "    for feature in schema.feature:\n",
    "        if feature.HasField('annotation'):\n",
    "            if TARGET_TAG in feature.annotation.tag:\n",
    "                target_feature = feature.name\n",
    "    return target_feature\n",
    "\n",
    "\n",
    "def set_job_dirs():\n",
    "    \"\"\"Sets job directories based on env variables set by Vertex AI.\"\"\"\n",
    "    \n",
    "    model_dir = os.getenv('AIP_MODEL_DIR', LOCAL_MODEL_DIR)\n",
    "    tb_dir = os.getenv('AIP_TENSORBOARD_LOG_DIR', LOCAL_TB_DIR)\n",
    "    checkpoint_dir = os.getenv('AIP_CHECKPOINT_DIR', LOCAL_CHECKPOINT_DIR)\n",
    "    \n",
    "    return model_dir, tb_dir, checkpoint_dir\n",
    "\n",
    "\n",
    "\n",
    "def get_bq_dataset(table_name, features, target_feature, batch_size=32):\n",
    "    \n",
    "    def _transform_row(row_dict):\n",
    "        trimmed_dict = {column:\n",
    "                       (tf.strings.strip(tensor) if tensor.dtype == 'string' else tensor) \n",
    "                       for (column,tensor) in row_dict.items()\n",
    "                       }\n",
    "        target = trimmed_dict.pop(target_feature)\n",
    "        return (trimmed_dict, target)\n",
    "    \n",
    "    selected_features = {key: {'output_type': value[1]} \n",
    "                         for key, value in features.items()}\n",
    "    project_id, dataset_id, table_id = table_name.split('.')\n",
    "    client = tfio_bq.BigQueryClient()\n",
    "    parent = f'projects/{project_id}'\n",
    "\n",
    "    read_session = client.read_session(\n",
    "        parent=parent,\n",
    "        project_id=project_id,\n",
    "        table_id=table_id,\n",
    "        dataset_id=dataset_id,\n",
    "        selected_fields=selected_fields,\n",
    "    )\n",
    "\n",
    "    dataset = read_session.parallel_read_rows().map(_transform_row).batch(batch_size)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_category_encoding_layer(name, dataset, dtype):\n",
    "    \"\"\"Creates a CategoryEncoding layer for a given feature.\"\"\"\n",
    "\n",
    "    if dtype == tf.string:\n",
    "      index = preprocessing.StringLookup()\n",
    "    else:\n",
    "      index = preprocessing.IntegerLookup()\n",
    "\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    index.adapt(feature_ds)\n",
    "    encoder = preprocessing.CategoryEncoding(max_tokens=index.vocab_size())\n",
    "\n",
    "    return lambda feature: encoder(index(feature))\n",
    "\n",
    "\n",
    "def get_normalization_layer(name, dataset):\n",
    "    \"\"\"\"Creates a Normalization layer for a given feature.\"\"\"\n",
    "    normalizer = preprocessing.Normalization()\n",
    "\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    return normalizer\n",
    "\n",
    "\n",
    "def create_model(dataset, input_features, units, dropout_ratio):\n",
    "    \"\"\"Creates a binary classifier for Chicago Taxi tip prediction task.\"\"\"\n",
    "    \n",
    "    all_inputs = []\n",
    "    encoded_features = []\n",
    "    for feature_name, feature_info in input_features.items():\n",
    "        col = tf.keras.Input(shape=(1,), name=feature_name, dtype=feature_info[1])\n",
    "        if feature_info[0] == 'categorical':\n",
    "            \n",
    "            encoding_layer = get_category_encoding_layer(feature_name, \n",
    "                                                         dataset,\n",
    "                                                         feature_info[1])\n",
    "        else:\n",
    "            encoding_layer = get_normalization_layer(feature_name,\n",
    "                                                     dataset) \n",
    "        encoded_col = encoding_layer(col)\n",
    "        all_inputs.append(col)\n",
    "        encoded_features.append(encoded_col)\n",
    "        \n",
    "    all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(units, activation=\"relu\")(all_features)\n",
    "    x = tf.keras.layers.Dropout(dropout_ratio)(x)\n",
    "    output = tf.keras.layers.Dense(1)(x)\n",
    "    model = tf.keras.Model(all_inputs, output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    del argv\n",
    "    \n",
    "    # Set distribution strategy\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "    global_batch_size = (strategy.num_replicas_in_sync *\n",
    "                         FLAGS.per_replica_batch_size)\n",
    "    \n",
    "    schema = tfdv.load_schema_text(FLAGS.schema_file)\n",
    "    \n",
    "    features = schema_to_features(schema)\n",
    "    target_feature = get_target_feature(schema)\n",
    "\n",
    "    if not target_feature:\n",
    "        raise RuntimeError('Schema does not have target feature annotation')\n",
    "    \n",
    "    # Prepare datasets\n",
    "    validation_ds = get_bq_dataset(FLAGS.validation_table, \n",
    "                                   selected_fields,\n",
    "                                   target_feature,\n",
    "                                   batch_size=global_batch_size)\n",
    "    training_ds = get_bq_dataset(FLAGS.training_table,\n",
    "                                 selected_fields,\n",
    "                                 target_feature,\n",
    "                                 batch_size=global_batch_size)\n",
    "    \n",
    "    # Prepare the model\n",
    "    logging.info('Creating the model ...')\n",
    "    input_features = {key: value for key, value in features.items() if key != target_feature}\n",
    "    \n",
    "    with strategy.scope():\n",
    "        model = create_model(training_ds, input_features, FLAGS.units, FLAGS.dropout_ratio)\n",
    "        model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Configure Keras callbacks\n",
    "    model_dir, tb_dir, checkpoint_dir = set_job_dirs()\n",
    "    callbacks = [tf.keras.callbacks.experimental.BackupAndRestore(backup_dir=checkpoint_dir)]\n",
    "    callbacks.append(tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=tb_dir, update_freq='batch'))\n",
    "    \n",
    "    logging.info('Starting training ...')\n",
    "    model.fit(training_ds, \n",
    "              epochs=FLAGS.epochs, \n",
    "              validation_data=validation_ds,\n",
    "              callbacks=callbacks)\n",
    "    \n",
    "    # Save trained model\n",
    "    logging.info('Training completed. Saving the trained model to: {}'.format(model_dir))\n",
    "    model.save(model_dir)  \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    logging.set_verbosity(logging.INFO)\n",
    "    app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = 'gcr.io/deeplearning-platform-release/tf2-gpu.2-4'\n",
    "TRAIN_IMAGE = f'gcr.io/{PROJECT}/taxi_classifier_trainer_v2'\n",
    "\n",
    "dockerfile = f'''\n",
    "FROM {BASE_IMAGE}\n",
    "\n",
    "WORKDIR /trainer\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY train.py .\n",
    "\n",
    "ENTRYPOINT [\"python\", \"train.py\"]\n",
    "'''\n",
    "\n",
    "with open(os.path.join(SCRIPT_FOLDER, 'Dockerfile'), 'w') as f:\n",
    "    f.write(dockerfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit --tag {TRAIN_IMAGE} {SCRIPT_FOLDER}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='gcr.io/ml-pipeline/google-cloud-pipeline-components:0.1.1')\n",
    "def ingest_data_op(\n",
    "    project: str,\n",
    "    bq_location: str,\n",
    "    sample_size: int,\n",
    "    year: int,\n",
    "    dataset_name: str,\n",
    "    train_split_name: str,\n",
    "    valid_split_name: str,\n",
    "    test_split_name: str,\n",
    "    dataset: Output[Dataset]\n",
    "):\n",
    "    \"\"\"Prepares training, validation, and testing data splits\n",
    "    from Chicago taxi public dataset.\"\"\"\n",
    "    \n",
    "    import logging\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import exceptions\n",
    "    \n",
    "    METADATA_TRAIN_SPLIT_KEY = 'train_split'\n",
    "    METADATA_VALID_SPLIT_KEY = 'valid_split'\n",
    "    METADATA_TEST_SPLIT_KEY = 'test_split'\n",
    "    \n",
    "    sql_script_template = '''\n",
    "    CREATE TEMP TABLE features \n",
    "    AS (\n",
    "        WITH\n",
    "        taxitrips AS (\n",
    "        SELECT\n",
    "            FORMAT_DATETIME('%Y-%d-%m', trip_start_timestamp) AS date,\n",
    "            trip_start_timestamp,\n",
    "            trip_seconds,\n",
    "            trip_miles,\n",
    "            payment_type,\n",
    "            pickup_longitude,\n",
    "            pickup_latitude,\n",
    "            dropoff_longitude,\n",
    "            dropoff_latitude,\n",
    "            tips,\n",
    "            fare\n",
    "        FROM\n",
    "            `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "        WHERE 1=1 \n",
    "        AND pickup_longitude IS NOT NULL\n",
    "        AND pickup_latitude IS NOT NULL\n",
    "        AND dropoff_longitude IS NOT NULL\n",
    "        AND dropoff_latitude IS NOT NULL\n",
    "        AND trip_miles > 0\n",
    "        AND trip_seconds > 0\n",
    "        AND fare > 0\n",
    "        AND EXTRACT(YEAR FROM trip_start_timestamp) = @YEAR\n",
    "        )\n",
    "\n",
    "        SELECT\n",
    "        trip_start_timestamp,\n",
    "        EXTRACT(MONTH from trip_start_timestamp) as trip_month,\n",
    "        EXTRACT(DAY from trip_start_timestamp) as trip_day,\n",
    "        EXTRACT(DAYOFWEEK from trip_start_timestamp) as trip_day_of_week,\n",
    "        EXTRACT(HOUR from trip_start_timestamp) as trip_hour,\n",
    "        trip_seconds,\n",
    "        trip_miles,\n",
    "        payment_type,\n",
    "        ST_AsText(\n",
    "            ST_SnapToGrid(ST_GeogPoint(pickup_longitude, pickup_latitude), 0.1)\n",
    "        ) AS pickup_grid,\n",
    "        ST_AsText(\n",
    "            ST_SnapToGrid(ST_GeogPoint(dropoff_longitude, dropoff_latitude), 0.1)\n",
    "        ) AS dropoff_grid,\n",
    "        ST_Distance(\n",
    "            ST_GeogPoint(pickup_longitude, pickup_latitude), \n",
    "            ST_GeogPoint(dropoff_longitude, dropoff_latitude)\n",
    "        ) AS euclidean,\n",
    "        IF((tips/fare >= 0.2), 1, 0) AS tip_bin,\n",
    "        CASE (ABS(MOD(FARM_FINGERPRINT(date),10))) \n",
    "            WHEN 9 THEN 'TEST'\n",
    "            WHEN 8 THEN 'VALIDATE'\n",
    "            ELSE 'TRAIN' END AS data_split\n",
    "        FROM\n",
    "        taxitrips\n",
    "        LIMIT @LIMIT\n",
    "    );\n",
    "\n",
    "    CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@TRAIN_SPLIT`\n",
    "    AS\n",
    "    SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "    FROM features\n",
    "    WHERE data_split='TRAIN';\n",
    "\n",
    "    CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@VALIDATE_SPLIT`\n",
    "    AS\n",
    "    SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "    FROM features\n",
    "    WHERE data_split='VALIDATE';\n",
    "\n",
    "    CREATE OR REPLACE TABLE `@PROJECT.@DATASET.@TEST_SPLIT`\n",
    "    AS\n",
    "    SELECT * EXCEPT (trip_start_timestamp, data_split)\n",
    "    FROM features\n",
    "    WHERE data_split='TEST';\n",
    "\n",
    "    DROP TABLE features;\n",
    "    '''\n",
    "    \n",
    "    client = bigquery.Client(project=project)\n",
    "    ds = bigquery.Dataset(f'{project}.{dataset_name}')\n",
    "    ds.location = bq_location\n",
    "    try:\n",
    "        ds = client.create_dataset(ds, timeout=30)\n",
    "        logging.info(f'Created dataset: {project}.{dataset_name}')\n",
    "    except exceptions.Conflict:\n",
    "        logging.info(f'Dataset {project}.{dataset_name} already exists')\n",
    "        \n",
    "    sql_script = sql_script_template.replace(\n",
    "        '@PROJECT', project).replace(\n",
    "        '@DATASET', dataset_name).replace(\n",
    "        '@TRAIN_SPLIT', train_split_name).replace(\n",
    "        '@VALIDATE_SPLIT', valid_split_name).replace(\n",
    "        '@TEST_SPLIT', test_split_name).replace(\n",
    "        '@YEAR', str(year)).replace(\n",
    "        '@LIMIT', str(sample_size))\n",
    "\n",
    "    job = client.query(sql_script)\n",
    "    job.result()\n",
    "    \n",
    "    dataset.metadata[METADATA_TRAIN_SPLIT_KEY] = f'{project}.{dataset_name}.{train_split_name}'\n",
    "    dataset.metadata[METADATA_VALID_SPLIT_KEY] = f'{project}.{dataset_name}.{valid_split_name}'\n",
    "    dataset.metadata[METADATA_TEST_SPLIT_KEY] = f'{project}.{dataset_name}.{test_split_name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='tensorflow/tfx:latest',\n",
    "               packages_to_install=['google-cloud-bigquery[bqstorage,pandas]'])\n",
    "def generate_stats_op(\n",
    "    project: str,\n",
    "    sample_percentage: int,\n",
    "    dataset: Input[Dataset],\n",
    "    stats: Output[Artifact],\n",
    "   \n",
    "):\n",
    "    \"\"\"Generates statistics for the data splits \n",
    "    referenced in the input Dataset artifact.\"\"\"\n",
    "    \n",
    "    import os\n",
    "    import tensorflow_data_validation as tfdv\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    METADATA_TRAIN_SPLIT_KEY = 'train_split'\n",
    "    METADATA_VALID_SPLIT_KEY = 'valid_split'\n",
    "    METADATA_TEST_SPLIT_KEY = 'test_split'\n",
    "    \n",
    "    STATS_FILE_NAME = 'stats.pbtxt'\n",
    "    \n",
    "    sql_script_template = '''\n",
    "    SELECT * \n",
    "    FROM @TABLE\n",
    "    TABLESAMPLE SYSTEM (@SAMPLE_PERC PERCENT)\n",
    "    '''\n",
    "    \n",
    "    client = bigquery.Client(project=project)\n",
    "    for key in [METADATA_TRAIN_SPLIT_KEY, METADATA_VALID_SPLIT_KEY, METADATA_TEST_SPLIT_KEY]:\n",
    "        if key in dataset.metadata.keys():\n",
    "            sql_script = sql_script_template.replace(\n",
    "                '@TABLE', dataset.metadata[key]).replace(\n",
    "                '@SAMPLE_PERC', str(sample_percentage))\n",
    "            \n",
    "            df = client.query(sql_script).result().to_dataframe()\n",
    "    \n",
    "            stats_proto = tfdv.generate_statistics_from_dataframe(\n",
    "                dataframe=df,\n",
    "                stats_options=tfdv.StatsOptions(\n",
    "                    num_top_values=50\n",
    "                )\n",
    "            )\n",
    "    \n",
    "            file_path = os.path.join(stats.path, key)\n",
    "            os.makedirs(file_path)\n",
    "            tfdv.write_stats_text(stats_proto, \n",
    "                                  os.path.join(file_path, STATS_FILE_NAME))\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(base_image='tensorflow/tfx:latest')\n",
    "def validate_stats_op(\n",
    "    project: str,\n",
    "    stats: Input[Artifact],\n",
    "    schema: Input[Artifact],\n",
    "    anomalies: Output[Artifact],  \n",
    ")-> NamedTuple(\n",
    "    'ValidOutputs',\n",
    "    [\n",
    "        ('anomalies_detected', str)\n",
    "    ]):\n",
    "    \"\"\"Validates statistices referenced in the input stats Artifact.\"\"\"\n",
    "    \n",
    "    STATS_FILE_NAME = 'stats.pbtxt'\n",
    "    ANOMALIES_FILE_NAME = 'anomalies.pbtxt'\n",
    "    TRUE = 'true'\n",
    "    FALSE = 'false'\n",
    "    \n",
    "    import os\n",
    "    import logging\n",
    "    import tensorflow_data_validation as tfdv\n",
    "    from collections import namedtuple\n",
    "    \n",
    "    schema_proto = tfdv.load_schema_text(\n",
    "        input_path=schema.path\n",
    "    ) \n",
    "    \n",
    "    anomalies_detected = FALSE\n",
    "    for folder in os.listdir(stats.path):\n",
    "        stats_proto = tfdv.load_stats_text(\n",
    "            input_path=os.path.join(stats.path, folder, STATS_FILE_NAME)\n",
    "        )\n",
    "        \n",
    "        anomalies_proto = tfdv.validate_statistics(\n",
    "            statistics=stats_proto, \n",
    "            schema=schema_proto\n",
    "        )\n",
    "        \n",
    "        file_path = os.path.join(anomalies.path, folder)\n",
    "        os.makedirs(file_path)\n",
    "        file_path = os.path.join(file_path, ANOMALIES_FILE_NAME)\n",
    "        tfdv.write_anomalies_text(anomalies_proto, file_path)\n",
    "                                 \n",
    "        if anomalies_proto.anomaly_info:\n",
    "            anomalies_detected = TRUE\n",
    "            logging.info('Anomamlies detected: {}'.format(file_path))\n",
    "    \n",
    "    output = namedtuple('ValidOutputs', ['anomalies_detected'])\n",
    "    \n",
    "    return output(anomalies_detected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile time settings\n",
    "PIPELINE_NAME = 'taxi-tip-continuous-training'\n",
    "\n",
    "PROJECT = 'jk-mlops-dev'\n",
    "REGION = 'us-central1'\n",
    "\n",
    "BQ_LOCATION = 'US'\n",
    "BQ_DATASET_NAME = 'chicago_taxi_ml'\n",
    "TRAINING_TABLE_NAME = 'training'\n",
    "VALIDATION_TABLE_NAME = 'validation'\n",
    "TESTING_TABLE_NAME = 'testing'\n",
    "SCHEMA = 'gs://jk-vertex-workshop-bucket/schema/schema.pbtxt'\n",
    "\n",
    "TRAINING_CONTAINER_IMAGE = 'gcr.io/jk-mlops-dev/taxi_classifier_trainer'\n",
    "TRAINING_MACHINE_TYPE = 'n1-standard-4'\n",
    "ACCELERATOR_TYPE = 'NVIDIA_TESLA_T4'\n",
    "ACCELERATOR_COUNT = 1\n",
    "REPLICA_COUNT = 1\n",
    "\n",
    "SERVING_CONTAINER_IMAGE = 'us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-4:latest'\n",
    "SERVING_MACHINE_TYPE = 'n1-standard-4'\n",
    "\n",
    "\n",
    "@dsl.pipeline(name=PIPELINE_NAME)\n",
    "def taxi_tip_predictor_training(\n",
    "    model_display_name: str,\n",
    "    epochs: int,\n",
    "    staging_bucket: str,\n",
    "    per_replica_batch_size: int,\n",
    "    sample_percentage: int = 100,\n",
    "    year: int = 2020,\n",
    "    sample_size: int = 1000000,\n",
    "):\n",
    "    \n",
    "    import_schema = kfp.dsl.importer(\n",
    "        artifact_uri=SCHEMA,\n",
    "        artifact_class=Artifact,\n",
    "        reimport=False,\n",
    "    )\n",
    "    \n",
    "    prepare_data = ingest_data_op(\n",
    "        project=PROJECT,\n",
    "        bq_location=BQ_LOCATION,\n",
    "        sample_size=sample_size,\n",
    "        year=year,\n",
    "        dataset_name=BQ_DATASET_NAME,\n",
    "        train_split_name=TRAINING_TABLE_NAME,\n",
    "        valid_split_name=VALIDATION_TABLE_NAME,\n",
    "        test_split_name=TESTING_TABLE_NAME,\n",
    "    )\n",
    "    \n",
    "    generate_stats = generate_stats_op(\n",
    "        project=PROJECT,\n",
    "        sample_percentage=sample_percentage,\n",
    "        dataset=prepare_data.outputs['dataset'],\n",
    "    )\n",
    "    \n",
    "    validate_stats = validate_stats_op(\n",
    "        project=PROJECT,\n",
    "        schema=import_schema.output,\n",
    "        stats=generate_stats.outputs['stats'],\n",
    "    )\n",
    "    \n",
    "    with dsl.Condition(validate_stats.outputs['anomalies_detected'] == 'false',\n",
    "                       name = 'Anomalies detected'):\n",
    "    \n",
    "        args = [\n",
    "            '--epochs', str(epochs),\n",
    "            '--per_replica_batch_size', str(per_replica_batch_size),\n",
    "            '--training_table', f'{PROJECT}.{BQ_DATASET_NAME}.{TRAINING_TABLE_NAME}',\n",
    "            '--validation_table',  f'{PROJECT}.{BQ_DATASET_NAME}.{VALIDATION_TABLE_NAME}',\n",
    "        ]\n",
    "        \n",
    "        train = gcc_aip.CustomContainerTrainingJobRunOp(\n",
    "            project=PROJECT,\n",
    "            location=REGION,\n",
    "            display_name=model_display_name,\n",
    "            model_display_name=model_display_name,\n",
    "            container_uri=TRAINING_CONTAINER_IMAGE,\n",
    "            args=args,\n",
    "            replica_count=REPLICA_COUNT,\n",
    "            accelerator_type=ACCELERATOR_TYPE,\n",
    "            accelerator_count=ACCELERATOR_COUNT,\n",
    "            staging_bucket=staging_bucket,\n",
    "            model_serving_container_image_uri=SERVING_CONTAINER_IMAGE,\n",
    "        )\n",
    "    \n",
    "        create_endpoint = gcc_aip.EndpointCreateOp(\n",
    "            project=PROJECT,\n",
    "            display_name=model_display_name\n",
    "        )\n",
    "        create_endpoint.after(train)\n",
    "    \n",
    "        deploy_model = gcc_aip.ModelDeployOp(\n",
    "            project=PROJECT,\n",
    "            endpoint=create_endpoint.outputs['endpoint'],\n",
    "            model=train.outputs['model'],\n",
    "            deployed_model_display_name=model_display_name,\n",
    "            machine_type=SERVING_MACHINE_TYPE\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "package_path = 'taxi_tip_predictor_pipeline.json'\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=taxi_tip_predictor_training,\n",
    "    package_path=package_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT,\n",
    "    region=REGION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/taxi-tip-continuous-training-20210608234432?project=jk-mlops-dev\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "STAGING_BUCKET = 'gs://jk-vertex-workshop-bucket/pipeline_runs'\n",
    "PIPELINES_SA = 'pipelines-sa@jk-mlops-dev.iam.gserviceaccount.com'\n",
    "        \n",
    "parameter_values = {\n",
    "    'model_display_name': 'Taxi tip predictor',\n",
    "    'epochs': 2,\n",
    "    'per_replica_batch_size': 128,\n",
    "    'staging_bucket': STAGING_BUCKET,\n",
    "}\n",
    "\n",
    "response = api_client.create_run_from_job_spec(\n",
    "    package_path,\n",
    "    pipeline_root=STAGING_BUCKET,\n",
    "    parameter_values=parameter_values,\n",
    "    enable_caching=True,\n",
    "    service_account=PIPELINES_SA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m69"
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
